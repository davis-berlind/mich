\section{Introduction}
\label{sec:intro}

Change-point detection (CPD) has been a perennial topic of interest in statistical inference since the introduction of the CUSUM algorithm by \cite{Page54}. Broadly, CPD involves identifying the locations of structural breaks in an ordered sequence of data $\mathbf{y}_{1:T}:= \{\mathbf{y}_t\}_{t=1}^T$ where $\mathbf{y}_t \in \mathbb{R}^d$. Suppose there are $L$ unique times $\mathbf{t}^*:=\{t^*_\ell\}_{\ell=1}^{L} \subset \{1,\ldots,T\}$, with $t^*_0:=0 < t_1^* < \ldots< t_L^* <t^*_{L+1} := T+1$, and a collection of $L+1$ distributions $\mathbf{F} := \{F_\ell\}_{\ell=0}^L$ such that:
\begin{align}\label{eq:cdp-def}
    \mathbf{y}_t \sim F_\ell, \;\sforall t \in [t^*_\ell,t^*_{\ell+1}).
\end{align}
The goal of any CPD method is to consistently estimate $\mathbf{t}^*$ using $\mathbf{y}_{1:T}$. Due to the generality of this construction, CPD problems appear in a variety of scientific fields, including: detecting increases in the real interest rate (\citealp{Bai03}), the location of changes in the structure of a genome (\citealp{Muggeo11}), radiological anomaly detection \citep{madrid2019sequential}, the occurrence of deforestation events (\citealp{Wendelberger21}), or analysis of neurological data \citep{padilla2022change}. If we define $\boldsymbol{\mu}_t := \E[\mathbf{y}_t]$ and $\boldsymbol{\Sigma}_t := \Var(\mathbf{y}_t)$, then the structure specified in (\ref{eq:cdp-def}) induces a piece-wise constant structure on $\boldsymbol{\mu}_{1:T} := \{\boldsymbol{\mu}_t\}_{t=1}^T$ and $\boldsymbol{\Sigma}_{1:T} := \{\boldsymbol{\Sigma}_t\}_{t=1}^T$ such that for a collection of (potentially) distinct and unknown parameters $\{\boldsymbol{\mu}^*_\ell\}_{\ell=0}^{L}$ and $\{\boldsymbol{\Sigma}^*_\ell\}_{\ell=0}^{L}$, we have $\boldsymbol{\mu}_t = \boldsymbol{\mu}^*_\ell$ and $\boldsymbol{\Sigma}_t = \boldsymbol{\Sigma}^*_\ell$ for $t \in [t^*_\ell,t^*_{\ell+1})$. 

In many applications, we require a way to quantify the reliability of the point estimates for $\mathbf{t}^*$. As an example, \cite{Gao19} study a variance CPD problem that determines the viability of a human liver for transplant. In this case, knowing whether the estimated change-point is likely to be off by minutes or hours will materially affect the performance of a medical procedure. Unfortunately, the vast majority of existing CPD methods do not provide any form of uncertainty quantification around the estimates of $\mathbf{t}^*$, and the few that do tend to return confidence intervals that are either difficult to interpret or are overly conservative. 

%MICH is a generalization of earlier work by \cite{Wang20} and \cite{Cappello22}, which introduced VB methods for detecting changes in the piece-wise constant mean and variance of a univariate time-series respectively. 

We attempt to address this uncertainty quantification gap in the CPD literature by introducing the Multiple Independent CHange-point (MICH) model, a novel variational Bayes (VB) method that both estimates $\mathbf{t}^*$ and returns credible sets around those estimates. In addition to mean and variance changes, MICH can detect: i) simultaneous changes in the mean and variance of a sequence, and ii) multivariate mean changes. MICH is implemented as an efficient backfitting procedure (\citealp{Friedman81, Breiman85}), allowing it to process large amounts of data much faster than existing bayesian CPD methods. Furthermore, MICH requires virtually no tuning of hyperparameters and returns easy-to-interpret $\alpha$-level credible sets, making the method readily accessible to analysts in a variety of fields. In simulations, MICH is competitive with other state-of-the-art models, often returning credible sets that are an order of magnitude smaller than competitors without sacrificing nominal coverage guarantees. 

In the multiple change-point setting, we construct MICH by modularly combining a collection of single change-point models. The promising empirical results we observe are in part explained by the optimal properties of the single change-point models we use as the building blocks of MICH. Bayesian methods for detecting variance changes having existed since at least \cite{Smith75}. Despite this long history, we are the first to show that a bayesian method for variance CPD is capable of achieving the optimal rate localization rate. In particular, for a sequence of independent sub-Gaussian observations with a single mean or variance change, we show that the estimator returned by MICH achieves a localization rate that is minimax optimal up to a $\log T$ factor. The proof of this result relies upon a novel analysis of the marginal log-likelihood of our model. Through careful consideration of the many non-linear terms in the log-likelihood, we are able to improve the $\mathcal{O}(\sqrt{T \log T})$ localization rate previously established by \cite{Cappello22} to a rate that is order $\mathcal{O}(\log T)$. Furthermore, under mild regularity conditions on the amount of serial correlation in $\mathbf{y}_{1:T}$, we show that MICH achieves a localization rate of approximately order $\mathcal{O}(\log^2 T)$ for dependent data.

For mean changes, we also consider the multivariate setting with $\mathbf{y}_t \in \mathbb{R}^d$ and show that the single change-point localization rate for our model is still optimal when $d > 1$. In another first for a bayesian CPD model, we consider the high-dimensional setting with $d \gtrsim T$ and show that exact recovery of the change-point is point is possible with a non-sparse mean signal, i.e. when the $\ell_2$-norm of $\{\boldsymbol{\mu}^*_\ell\}_{\ell=0}^{L}$ grows like $\sqrt{d}$. MICH is therefore the first bayesian method to recover the high-dimensional result of \cite{Bai10}.  

\subsection{Related Work}

Binary Segmentation (BS; \citealp{Scott74, Sen75, Vostrikova81}) is the classical method for detecting multiple changes in both the underlying mean and scale of $\mathbf{y}_{1:T}$. BS remains popular due to its $\mathcal{O}(T\log T)$ computational complexity. Many variants of BS have been introduced to overcome the greedy nature of the algorithm, including Circular Binary Segmentation (CBS; \citealp{Olshen04}), Wild Binary Segmentation (WBS; \citealp{Fryzlewicz14}), and Seeded Binary Segmentation (SBS; \citealp{Kovacs22}). Exact search methods can also improve on the approximate nature of BS. These include include: the segment neighborhood method (SN; \citealp{Auger89}), which has an $\mathcal{O}(LT^2)$ computational cost, where $L$ is the number of segments to be estimated; the pruned exact linear time method (PELT; \citealp{Killick12}), which improves to an $\mathcal{O}(T)$ computational cost when the number of change-points increases linearly in $T$; and the narrowest-over-threshold method (NOT; \citealp{Baranowski19}), which allows for detection of changes in more general features than just the mean and variance, e.g. changes in the derivatives of a signal function. There are also specialized methods for detecting changes in the just the variance, including: iterated cumulative sums of squares (\citealp{Inclan94}), penalized weighted least squares methods (\citealp{Chen97, Gao19}), and the fused lasso (\citealp{Padilla22}). 

The above methods and the majority of existing frequentist CPD methods do not provide uncertainty quantification for their point estimates of $\mathbf{t}^*$. A few early works sought to address this gap by generating confidence sets around change-point estimates at prescribed significance levels (\citealp{Worsley86, Siegmund86, Bai03}), but until recently the literature has mostly ignored the the issue. The past decade has seen renewed interest in providing uncertainty quantification with various theoretical guaranties. We now have methods that return confidence sets around change-points in the cases of: changes to a piece-wise constant mean (\citealp{Frick14, Fang20, Jewell22}), changes to a piece-wise linear mean (\citealp{Fryzlewicz23}),  simultaneous changes in a piece-wise constant mean and variance (\citealp{Bai10, Pein17, Eichinger18}), and nonparametric multivariate changes \cite{madrid2023change}.

Bayesian methods can offer parsimonious approaches to uncertainty quantification. \cite{Smith75} was the first to take a Bayesian approach to CPD but was limited to the case of a single change-point in the mean. \cite{Stephens94} used Gibbs sampling to generalize to the case of multiple change-points. \cite{Barry93} offered further computational improvements by reformulating CPD as an application of the product partition model (\citealp{Hartigan90, Barry92}), which allowed the authors to bypass the need for Gibbs sampling through the use of efficient recursions. Recent developments include: extensions of \cite{Barry93} to the case of online CPD (\citealp{Fearnhead06, Adams07}), detecting structural breaks in an auto-covariance function (\citealp{Preuss15}), empirical Bayes procedures (\citealp{Liu17}), and the use of approximate recursions (\citealp{Cappello21}). Despite these improvements, these methods all still require some degree of Markov chain Monte Carlo (MCMC) sampling. As a result, Bayesian CPD methods remain an order of magnitude slower than state-of-the-art methods like PELT and NOT.

A novel idea that circumvents the need for MCMC methods is to forgo exact inference in favor of finding a variational approximation to the posterior distribution of the change-point locations. Variational Bayesian methods seek to find a distribution $q$ that is the best approximation to the posterior within a specified family of distributions (\citealp{Jordan99, Bishop06, Wainwright08, Blei17}). \cite{Wang20} were the first to apply this idea to the case of a piece-wise constant mean using their variable selection method, the Sum of Single Effects (SuSiE) model. \cite{Cappello22} adapted the approach of \citeauthor{Wang20} to the case of a piece-wise constant variance with their Product of Individual Scale Parameters (PRISCA) model. Our work is a continuation of this branch of the literature.


\subsection{Outline of the Paper}

The structure of the rest of this article is as follows: In Section \ref{sec:scp}, we introduce and characterize the localization rates of three Bayesian models for identifying a single change-point in the mean, variance, and a simultaneous change in the mean and variance. In Section \ref{sec:mich}, we modularly combine the three models from Section \ref{sec:scp} to create MICH, introduce the backfitting algorithm to fit MICH, and show that this procedure returns a variational approximation to the true posterior distribution. In Section \ref{sec:simulations}, we repeat a simulation study first seen in \cite{Pein17} to compare the performance of MICH to competitors. In Section \ref{sec:data}, we apply MICH to oil well log data to show that it can effectively identify substantive changes in the mean and variance of a series. In Section \ref{sec:discussion}, we conclude and discuss future directions. An \texttt{R} package implementation of MICH is available for download at \url{https://github.com/davis-berlind/SUPR}. All the proofs are deferred to the Appendix. 

\subsection{Notation}

$[T] := \{1, \ldots, T\}$, for $\mathbf{x}\in\mathbb{R}^d$ and  $\lVert \mathbf{x} \rVert_p := \left(\sum_{i=1}^d |x_i|^p\right)^{1/p}$ for $p \geq 1$, $\mathbb{B}_{r}^{p}(\mathbf{x}_0) := \{\mathbf{x}\in\mathbb{R}^d \::\: \lVert \mathbf{x}_0 - \mathbf{x} \rVert_p \leq r\}$ with $\mathbb{B}_{r}^{p} \equiv \mathbb{B}_{r}^{p}(\mathbf{0})$, $\mathcal{S}^d$ is the $d$-dimensional probability simplex, i.e. $\mathcal{S}^d := \{\mathbf{x}\in\mathbb{R}^d_+ \::\: \lVert \mathbf{x} \rVert_1 =1\}$. Let $\E_{g} [\:\cdot\:]$ denote the expectation taken with respect to a generic distribution $g$.