\section{Introduction}
\label{sec:intro}

Change-point detection (CPD) has been a perennial topic of interest in statistical inference going back to at least \cite{Page54}. Broadly, CPD involves identifying the locations of structural breaks in an ordered sequence of data $\mathbf{y}_{1:T}:= \{\mathbf{y}_t\}_{t=1}^T$ where $\mathbf{y}_t \in \mathbb{R}^d$. Suppose there are $L$ unique times $\mathbf{t}^*:=\{t^*_\ell\}_{\ell=1}^{L} \subset \{1,\ldots,T\}$, with $t^*_0=0$ and $t^*_{L+1} = T+1$, and a collection of $L+1$ distributions $\mathbf{F} := \{F_\ell\}_{\ell=0}^L$ such that:
\begin{align}\label{eq:cdp-def}
    \mathbf{y}_t \sim F_\ell, \;\sforall t \in [t^*_\ell,t^*_{\ell+1}).
\end{align}
The goal of any CPD method is to consistently estimate $\mathbf{t}^*$ from $\mathbf{y}_{1:T}$.  CPD problems are found in a variety of scientific fields, including: detecting an increase in the real interest rate (\citealp{Bai03}), the location of a change in the structure of a genome (\citealp{Muggeo11}), or the occurrence of deforestation events (\citealp{Wendelberger21}). If we define $\boldsymbol{\mu}_t := \E[\mathbf{y}_t]$ and $\boldsymbol{\Sigma}_t := \Var(\mathbf{y}_t)$, then the structure specified in (\ref{eq:cdp-def}) induces a piece-wise constant structure on $\boldsymbol{\mu}_{1:T} := \{\boldsymbol{\mu}_t\}_{t=1}^T$ and $\boldsymbol{\Sigma}_{1:T} := \{\boldsymbol{\Sigma}_t\}_{t=1}^T$ such that for a collection of (potentially) distinct and unknown parameters $\{\boldsymbol{\mu}^*_\ell\}_{\ell=0}^{L}$ and $\{\boldsymbol{\Sigma}^*_\ell\}_{\ell=0}^{L}$, and for $t \in [t^*_\ell,t^*_{\ell+1})$ we have $\boldsymbol{\mu}_t = \boldsymbol{\mu}^*_\ell$ and $\boldsymbol{\Sigma}_t = \boldsymbol{\Sigma}^*_\ell$. In this article we introduce a novel variational Bayes algorithm that can both detect and provide uncertainty quantification for changes in the piece-wise constant structure of $\boldsymbol{\mu}_{1:T}$ and $\boldsymbol{\Sigma}_{1:T}$.

\subsection{Related Work}

Binary Segmentation (BS; \citealp{Scott74, Sen75, Vostrikova81}) is the classical method for detecting multiple changes in both the underlying mean and scale of $\mathbf{y}_{1:T}$. BS remains popular due to its $\mathcal{O}(T\log T)$ computational complexity. Many variants of BS have been introduced to overcome the greedy nature of the algorithm, including Wild Binary Segmentation (WBS; \citealp{Fryzlewicz14}) and Circular Binary Segmentation (CBS; \citealp{Olshen04}). Exact search methods can also improve on the approximate nature of BS. These include include: the segment neighborhood method (SN; \citealp{Auger89}), which has an $\mathcal{O}(LT^2)$ computational cost, where $L$ is the number of segments to be estimated; the pruned exact linear time method (PELT; \citealp{Killick12}), which improves to an $\mathcal{O}(T)$ computational cost when the number of change-points increases linearly in $T$; and the narrowest-over-threshold method (NOT; \citealp{Baranowski19}), which allows for detection of changes in more general features than just the mean and variance, e.g. changes in the derivatives of a signal function. There are also specialized methods for detecting changes in the just the variance, including: cumulative sum squares (\citealp{Inclan94}), penalized weighted least squares methods (\citealp{Chen97, Gao19}), and the fused lasso (\citealp{Padilla22}). 

The drawback of each of these and the majority of other existing frequentist CPD methods is that they do not provide uncertainty quantification for the point estimates of $\mathbf{t}^*$ that they return. A few early works sought to address this gap by generating confidence sets around change-point estimates at prescribed significance levels (\citealp{Worsley86, Siegmund86, Bai03}), but until recently the literature has mostly ignored this issue. In the past decade, there has been renewed interest in providing uncertainty quantification with various theoretical guaranties. We now have methods that return confidence sets around change-points in the cases of: changes to a piece-wise constant mean (\citealp{Frick14, Fang20, Jewell22}), changes to a piece-wise linear mean (\citealp{Fryzlewicz23}), and simultaneous changes in a piece-wise constant mean and variance (\citealp{Bai10, Pein17, Eichinger18}). 

Bayesian methods on the other hand can offer parsimonious approaches to uncertainty quantification. \cite{Smith75} was the first to take a Bayesian approach to CPD but was limited to the case of a single change-point in the mean. \cite{Stephens94} used Gibbs sampling to generalize to the case of multiple change-points. \cite{Barry93} offered further computational improvements by reformulating CPD as an application of the product partition model (\citealp{Hartigan90, Barry92}), which allowed the authors to bypass the need for Gibbs sampling through the use of efficient recursions. Recent developments include: extensions of \cite{Barry93} to the case of online CPD (\citealp{Fearnhead06, Adams07}), detecting structural breaks in an auto-covariance function (\citealp{Preuss15}), empirical Bayes procedures (\citealp{Liu17}), and the use of approximate recursions (\citealp{Cappello21}). Despite the improvements offered by each of these methods, they all still require some degree of Markov chain Monte Carlo (MCMC) sampling. As a result, these Bayesian CPD methods remain orders of magnitude slower than state-of-the-art methods like PELT and NOT.

A novel idea that circumvents the need for MCMC methods altogether involves foregoing exact inference in favor of finding a variational approximation to the posterior distribution of the change-points. Variational Bayesian methods seek to find a distribution $q$ that is the best approximation to the posterior within a specified family of distributions (\citealp{Jordan99, Bishop06, Wainwright08, Blei17}). \cite{Wang20} were the first to apply this idea to the case of a piece-wise constant mean using their variable selection method, the Sum of Single Effects (SuSiE) model. \cite{Cappello22} adapted \citeauthor{Wang20}'s idea to the case of series with a piece-wise constant variance with the Product of Individual Scale Parameters (PRISCA) model and were the first to show that a Bayesian procedure can achieve a localization rate of order $\mathcal{O}(\sqrt{T\log T})$ when identifying a variance change-point. Our work is a continuation of this branch of the literature.

In this article we introduce the Multiple Independent Change-Point (MICH) model which combines SuSiE and PRISCA's respective abilities to identify separate changes in a piece-wise constant mean and variance, with the new ability to identify simultaneous changes in the mean and variance of the sequence. We are able to show that like SuSiE and PRISCA, MICH can be efficiently implemented as a backfitting procedure (\citealp{Friedman81, Breiman85}). In simulations, our method performs competitively with state-of-the-art models for identifying joint changes in the mean and variance of a sequence, and often returns credible set that are an order of magnitude smaller than competitors. Furthermore, we build upon the theoretical results in \cite{Cappello22} to show that i) the single effect model of \cite{Wang20} has a localization rate that is minimax optimal up to a $\log(T)$ factor  when applied to the single change in the mean problem, and ii) our new Bayesian model for identifying a single simultaneous change in the mean and variance of a sequence achieves a localization rate that of order $\mathcal{O}(\sqrt{T\log T})$, which matches the rate achieved by the Binary Segmentation in Operator Norm (BSOP) method introduced in \cite{Wang21}.

The structure of the rest of this article is as follows: In Section \ref{sec:scp}, we introduce three Bayesian models for identifying a single change-point in the mean, variance, and a simultaneous change in the mean and variance respectively. We establish optimality results for the localization rate of each model. In Section \ref{sec:mich}, we modularly combine the three models from Section \ref{sec:scp} to create MICH, introduce the backfitting algorithm to fit MICH, and show that this procedure returns a variational approximation to the true posterior distribution. In Section \ref{sec:simulations}, we repeat a simulation study first seen in \cite{Pein17} to compare the performance of MICH to competitors. In Section \ref{sec:data}, we apply MICH to oil well log data to show that it can effectively identify substantive changes in the mean and variance of a series. In Section \ref{sec:discussion}, we conclude and discuss future directions. An \texttt{R} package implementation of MICH is available for download at \url{https://github.com/davis-berlind/SUPR}.
