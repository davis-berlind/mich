\section{Introduction}
\label{sec:intro}

Change-point detection (CPD) has been a perennial topic of interest in statistical inference since the introduction of the CUSUM algorithm by \cite{Page54}. Broadly, CPD involves identifying the locations of structural breaks in an ordered sequence of data $\mathbf{y}_{1:T}:= \{\mathbf{y}_t\}_{t=1}^T$ where $\mathbf{y}_t \in \mathbb{R}^d$. Suppose there are $L$ indices $\boldsymbol{\tau}_{1:L}:=\{\tau_\ell\}_{\ell=1}^{L} \subset \{1,\ldots,T\}$, with $\tau_0:=1 < \tau_1 < \ldots< \tau_L < \tau_{L+1} := T+1$, and a collection of $L+1$ distributions $\{F_\ell\}_{\ell=0}^L$ such that:
\begin{align}\label{eq:cdp-def}
    \mathbf{y}_t \sim F_\ell, \;\sforall t \in [\tau_\ell,\tau_{\ell+1}).
\end{align}
The aim of any CPD method is to consistently estimate the number of changes $L$ and their locations $\boldsymbol{\tau}_{1:L}$. Due to the generality of this construction, CPD problems appear in a variety of fields. Examples include: detecting adjustments to the real interest rate (\citealp{Bai03}), structural changes in a DNA sequence (\citealp{Muggeo11}), radiological anomalies \citep{madrid2019sequential}, and deforestation events (\citealp{Wendelberger21}). In addition to estimating the number and the locations of the changes underlying $\mathbf{y}_{1:T}$, a limited set of methods can return localized regions of $\{1,\ldots,T\}$ that contain change-points at a prescribed significance level. Such measures of uncertainty are essential in applications like the design of medical procedures. For example, \cite{Gao19} use a CPD method to determine whether a human liver is viable for transplant. In this case, knowing whether the estimated change-point is likely to be off by minutes versus hours will materially affect the outcome of the procedure. 

Early attempts to provide confidence sets for change-point estimates were either limited to the case of a single mean change (\citealp{Siegmund86, Worsley86,Jirak15,Horvath17}), required knowledge of $L$ (\citealp{Bai03}), or could only produce approximate sets based on the limiting distribution of the estimator (\citealp{Bai10}). Since the introduction of SMUCE by \cite{Frick14}, the state-of-the-art has advanced rapidly. There are now methods that can generate $\alpha$-level confidence sets in the presence of a variety of types of changes, data structures, and dependence settings (\citealp{Pein17, Eichinger18, Dette20, Fang20, fang2021detectionestimationlocalsignals, Chen22, Cho22, madrid2023change, Fryzlewicz24median, Fryzlewicz24}). Still, there remains room for improvement. \cite{chen2014discussion} note that the sets returned by SMUCE exhibit undesirable coverage properties as $\alpha$ decreases. \cite{Fryzlewicz24} attempted to address this issue, but like SMUCE, the large confidence sets returned by their method tend to be overly conservative. Additionally, methods capable of producing confidence sets in the case of multivariate data or changes in the variance of $\mathbf{y}_{1:T}$ remain underdeveloped.

Bayesian CPD (BCPD) can offer a parsimonious framework for uncertainty quantification by fully characterizing the posterior distribution of $\{\boldsymbol{\tau}_{1,L}, L\}$. BCPD traces its origin to \cite{chernoff1964estimating}. Important early developments came from \cite{Barry93}, who formulated (\ref{eq:cdp-def}) as a product partition model (\citealp{Hartigan90, Barry92}) and used closed-form recursions to efficiently draw from the posterior distribution. Another popular approach to BCPD involves introducing a latent change-point indicator $z_t$ and using techniques from the HMM literature to simulate the posterior distribution of $\mathbf{z}_{1:T}$ (\citealp{Chib98, Fearnhead06, Nam12, Harle16,Fan17}). Even though the posterior characterizes the joint distribution of all the change-point locations, condensing this information into meaningful summaries like credible sets is challenging. In practice, the vast majority of BCPD methods simply return the marginal posterior probability that any given index $t$ is a change-point. %We desire a method that inverts this logic, i.e. in place of the probability that $t$ is a change-point, we would like to know the probability that a given change-point is equal to $t$. 

\subsection{List of Contributions}

\textbf{MICH}: %We aim to design a method that is computationally efficient, can estimate and effectively localize a broad class of changes in the structure of $\mathbf{y}_{1:T}$, and has good theoretical guarantees. To this end, 
We introduce the Multiple Independent CHange-point (MICH) model, a Bayesian method that can identify changes in the mean and variance of $\mathbf{y}_{1:T}$. We construct MICH by modularly combining many single change-point models, a concept that was first introduced for mean changes by \cite{Wang20} and for variance changes by \cite{Cappello22}. This modular construction enables us to efficiently approximate the posterior distribution of the change-points using a variational algorithm (\citealp{Jordan99, Blei17}) that we implement as a deterministic backfitting procedure (\citealp{Breiman85}). Unlike other Bayesian methods, MICH requires virtually no hyperparameter tuning. In simulations, MICH is competitive with state-of-the-art models at little additional computational cost. In fact, MICH returns credible sets that are an order of magnitude smaller than the sets returned by competing methods without sacrificing nominal coverage guarantees.

%Each component of MICH maps to a single $\alpha$-level credible set, endowing the model with our desired localization behavior. Unlike other Bayesian methods, MICH requires virtually no hyperparameter tuning, making it readily accessible to analysts in a variety of fields. In simulations, MICH is competitive with state-of-the-art models at little additional computational cost. In fact, our method often returns credible sets that are an order of magnitude smaller than competitors without sacrificing nominal coverage guarantees.

\textbf{Minimax Optimal Localization}: We show that our promising empirical results are partially explained by the optimal consistency properties of the single change-point models that make up MICH. For univariate $\mathbf{y}_{1:T}$ with a single change-point, we show that for the cases of a mean-only change, a variance-only change, and a simultaneous mean and variance change, our model achieves the minimax localization rate up to a logarithmic factor. For the mean-only and variance-only cases, this result matches the optimal rates recently achieved the Bayesian models of \cite{Liu17}, \cite{Cappello21}, and \cite{Kim24}. For the simultaneous mean and variance change case, this optimal rate is a novel result for a Bayesian CPD method, the proof of which relies on a careful treatment of the many non-linear terms in the model likelihood.

\textbf{Non-Gaussian and Dependent Data}: Despite the assumption of i.i.d. Gaussian data in our model, we show that the localization rate remains unchanged when the elements of $\mathbf{y}_{1:T}$ are independent and sub-Gaussian, somewhat in-line with the robustness to misspecification observed for other Bayesian methods (\citealp{lai2011simple}). Beyond independence, we generalize our results to univariate $\alpha$-mixing sequences. Using novel concentration results from \cite{Padilla23}, we show that MICH achieves a localization rate of approximately order $\mathcal{O}(\log^2 T)$ for dependent data.

\textbf{High-Dimensional Mean Changes}: In the multivariate setting, MICH can localize a single mean change at the minimax optimal rate when $\lVert\E[\mathbf{y}_{\tau_1} - \mathbf{y}_{\tau_1 - 1}]\rVert_2 \gtrsim \sqrt{d}$. In high-dimensions, this condition is satisfied when we observe a ``dense" signal, i.e. many coordinates of $\E[\mathbf{y}_t]$ undergo small changes. To the best of our knowledge, MICH is the first Bayesian method to achieve the optimal rate in high-dimensions in the presence of dense mean changes. Furthermore, when $d \gtrsim \log T$, we recover the result of \cite{Bai10} by showing that our estimator converges to the true location of the change.

\subsection{Other Related Work}

Frequentist methods for detecting changes in the mean and variance of $\mathbf{y}_{1:T}$ broadly fall into two categories: i) greedy algorithms like Binary Segmentation and its variants that search for changes by recursively partitioning $\mathbf{y}_{1:T}$ (\citealp{Scott74, Sen75, Vostrikova81, Olshen04, Fryzlewicz14, Kovacs22}), and ii) exact search methods that jointly estimate the locations of the changes by solving an optimization problem, often using dynamic programming (\citealp{Auger89, Killick12, Baranowski19, Padilla22}). There is also a vast literature for the ``on-line" setting, where the task is to detect changes as each $\mathbf{y}_t$ is sequentially observed. We refer the reader to \cite{Namoano19} and \cite{Yu02102023} for recent surveys of on-line methods.

The high-dimensional CPD (HD-CPD) literature is still nascent and is mostly concerned with detecting sparse signals where $\tilde{d} \ll d$ coordinates of the mean vector jump at a given change-point (\citealp{Cho14, Jirak15, Wang17, Enikeeva19, Yu20, Chen22}). Most HD-CPD methods employ $\ell_\infty$-norm based aggregations of the data to leverage this sparsity, which differs from the $\ell_2$-norm aggregation used by our method (see \cite{Li23} for a recent survey of $\ell_2$ methods). Only \cite{Jirak15} and \cite{Chen22} characterize the limiting distribution of their estimators and provide confidence sets for the high-dimensional regime. \cite{Kim24} recently developed the first Bayesian HD-CPD method capable of localizing sparse mean and covariance changes at the minimax optimal rate; however, they approach the problem from a testing perspective and do not provide credible sets.

While our study focuses on localizing change-points with credible sets, we note that uncertainty quantification for CPD is often approached as a problem of hypothesis testing and post-selection inference (\citealp{Horváth12,Preuss15,Duy20,Hyun21,Jewell22}). This approach has received considerable attention in the literature and we direct the reader to \cite{Fryzlewicz24} for a recent survey and discussion of the limitations of testing and post-selection inference for CPD. 

\subsection{Outline of the Paper}

The structure of the rest of this article is as follows: In Section \ref{sec:scp}, we introduce three Bayesian models for localizing a single change in the mean and/or variance of $\mathbf{y}_{1:T}$. We also characterize the localization rates for each of these models. In Section \ref{sec:mich}, we introduce the MICH model for multiple change-points. We develop a backfitting procedure in Algorithm \ref{alg:mich} that facilitates inference and returns a variational approximation to the true posterior distribution under MICH. In Section \ref{sec:simulations}, we conduct an extensive simulation study to compare the performance of MICH with other state-of-the art competitors. In Section \ref{sec:data}, we apply MICH to an oil well-log data set and show that the model effectively identifies structural breaks in the lithology of the well. In Section \ref{sec:discussion}, we conclude and discuss future directions. An \texttt{R} package implementation of MICH is available for download at \url{https://github.com/davis-berlind/SUPR}. All the proofs are deferred to the Appendix. 

\subsection{Notation}

We denote the index set $[T] := \{1, \ldots, T\}$. For any sequence $\{x_t\}_{t\in\mathbb{Z}}$ $s,t\in[T]$ and $r,s \in \mathbb{Z}$ with $r<s$, we define $\mathbf{x}_{r:s} := \{x_t\}_{t=r}^s$. for $\mathbf{x}\in\mathbb{R}^d$ and  $\lVert \mathbf{x} \rVert_p := (\sum_{i=1}^d |x_i|^p)^{1/p}$ for $p \geq 1$, $\mathbb{B}_{r}^{p}(\mathbf{x}_0) := \{\mathbf{x}\in\mathbb{R}^d \::\: \lVert \mathbf{x}_0 - \mathbf{x} \rVert_p \leq r\}$ with $\mathbb{B}_{r}^{p} \equiv \mathbb{B}_{r}^{p}(\mathbf{0})$, $\mathcal{S}^d$ is the $d$-dimensional probability simplex, i.e. $\mathcal{S}^d := \{\mathbf{x}\in\mathbb{R}^d_+ \::\: \lVert \mathbf{x} \rVert_1 =1\}$. For some $\boldsymbol{\pi}_1:d \in \mathcal{S}^d$, we say $x \sim \text{Categorical}(\boldsymbol{\pi}_1:d)$ if $x \in [d]$ and $\Pr(x = i) = \pi_i$ for each $i \in [d]$. Let $\E_{g} [\:\cdot\:]$ denote the expectation taken with respect to a generic distribution $g$.