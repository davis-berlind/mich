\section{Introduction}
\label{sec:intro}

Broadly speaking, change-point detection (CPD) is the task of locating structural breaks in the signal underlying an ordered sequence of data. In this article, we consider a univariate data stream of the form:
\begin{align}
    y_t &= \mu_t + \sigma_t \varepsilon_t, \quad\forall\; t\in\{1,\ldots,T\} \\
    \varepsilon_t &\overset{\text{i.i.d.}}{\sim} \mathcal{N}(0,1)
\end{align}
where either $\mu_t$, $\sigma_t$, or both have a piece-wise constant structure. CPD problems with this structure are found in a variety of scientific fields. Examples include: detecting an increase in the real interest rate (\citealp{Bai03}), the location of a change in the structure of a genome (\citealp{Muggeo11}), or the occurrence of deforestation events (\citealp{Wendelberger21}). Due to this diversity of applications, CPD has remained a perennial topic of interest for statistical inference since it was first addressed by \cite{Page54}.

One of the classical methods for detecting multiple changes in both the underlying mean and scale of a univariate sequence is Binary Segmentation (BS; \citealp{Scott74, Sen75, Vostrikova81}), which has remained popular due to its $\mathcal{O}(T\log T)$ computational complexity. Variants of BS that attempt to overcome the greedy nature of the algorithm include Wild Binary Segmentation (WBS; \citealp{Fryzlewicz14}) and Circular Binary Segmentation (CBS; \citealp{Olshen04}). Other popular methods include: the segment neighborhood method (SN; \citealp{Auger89}), which has a computational cost of $\mathcal{O}(LT^2)$, where $L$ is the number of segments to be estimated; the pruned exact linear time method (PELT; \citealp{Killick12}), which improves to an $\mathcal{O}(T)$ computational cost when the number of change-points increases linearly in $T$; and the narrowest-over-threshold method (NOT; \citealp{Baranowski19}), which allows for detection of changes in more general features than just the mean and variance, e.g. changes in the derivatives of a signal function. There are also specialized methods for detecting changes in the just the variance, including: cumulative sum squares (\citealp{Inclan94}), penalized weighted least squares methods (\citealp{Chen97, Gao19}), and the fused lasso (\citealp{Padilla22}). 

The drawback of each of these and the majority of other existing frequentist CPD methods is that they do not provide uncertainty quantification around the point estimates they generate. A few early works sought to address this gap by generating confidence sets around change-point estimates at prescribed significance levels (\citealp{Worsley86, Siegmund86, Bai03}), but the literature has mostly ignored this issue until recently. The past decade has seen a renewed emphasis on providing uncertainty quantification with various theoretical guarantees. We now have methods that return confidence sets around change-points in the cases of: changes to a piece-wise constant mean (\citealp{Frick14, Fang20, Jewell22}), changes to a piece-wise linear mean (\citealp{Fryzlewicz23}), and simultaneous changes in a piece-wise constant mean and variance (\citealp{Bai10, Pein17, Eichinger18}). 

Alternatively, Bayesian methods can offer parsimonious approaches to uncertainty quantification. \cite{Smith75} was the first to take a Bayesian approach to CPD but was limited to the case of a single change-point in the mean. \cite{Stephens94} used Gibbs sampling to generalize to the case of multiple change-points. \cite{Barry93} offered further computational improvements by reformulating CPD as an application of the product partition model (\citealp{Hartigan90, Barry92}), which allowed the authors to bypass the need for Gibbs sampling through the use of efficient recursions. Recent developments include: extensions of \cite{Barry93} to the case of online CPD (\citealp{Fearnhead06, Adams07}), empirical Bayes procedures (\citealp{Liu17}), and the use of approximate recursions (\citealp{Cappello21}). Despite the improvements offered by each of these methods, they all still require some degree of Markov chain Monte Carlo (MCMC) sampling. As a result, these Bayesian CPD methods remain orders of magnitude slower than state-of-the-art methods like PELT and NOT.

A new idea that circumvents the need for MCMC methods altogether is to forego exact inference in favor of finding a variational approximation to the posterior distribution of the change-points. Variational Bayesian methods seek to find a distribution $q$ that is in some sense the ``closest" approximation to the posterior within a specified family of distributions (\citealp{Jordan99, Bishop06, Wainright08, Blei17}). \cite{Wang20} were the first to apply this idea to the case of a piece-wise constant mean using their variable selection method, the Sum of Single Effects (SuSiE) model. \cite{Cappello22} adapted \citeauthor{Wang20}'s idea to the case of series with a piece-wise constant variance with the Product of Individual Scale Parameters (PRISCA) model and were the first to show that a Bayesian procedure can achieve a minimax optimal localization rate for the problem of identifying a variance change-point. Our work is a continuation of this branch of the literature.

In this article we introduce the Multiple Independent Change-Point (MICH) model which combines SuSiE and PRISCA's respective abilities to identify separate changes in a piece-wise constant mean and variance, with the new ability to identify simultaneous changes in the mean and variance of the sequence. We are able to show that like SuSiE and PRISCA, MICH can be efficiently implemented as a backfitting procedure (\citealp{Friedman81, Breiman85}). In simulations, our method performs competitively with state-of-the-art models for identifying joint changes in the mean and variance of a sequence, and often returns credible set that are an order of magnitude smaller than competitors. Furthermore, we build upon the theoretical results in \cite{Cappello22} to show that i) the single effect model of \cite{Wang20} when applied to identifying a single change-point has a localization rate that is minimax optimal up to a $\log(T)$ factor, and ii) our new Bayesian model for identifying a single simultaneous change in the mean and variance of a sequence achieves a localization rate that is minimax optimal regardless of whether we are in the regime of an identifiable change in the mean or an identifiable change in the variance.  

The structure of the rest of this article is as follows: In Section \ref{sec:scp}, we introduce three Bayesian models for identifying a single change-point in the mean, variance, and a simultaneous change in the mean and variance respectively. We establish optimality results for the localization rate of each model. In Section \ref{sec:mich}, we modularly combine the three models from Section \ref{sec:scp} to create MICH, introduce the backfitting algorithm to fit MICH, and show that this procedure returns a variational approximation to the true posterior distribution. In Section \ref{sec:simulations}, we repeat a simulation study first seen in \cite{Pein17} to compare the performance of MICH to competitors. In Section \ref{sec:data}, we apply MICH to oil well log data to show that it can effectively identify substantive changes in the mean and variance of a series. In Section \ref{sec:discussion}, we conclude and discuss future directions. An \texttt{R} package implementation of MICH is available for download at \url{https://github.com/davis-berlind/SUPR}.
