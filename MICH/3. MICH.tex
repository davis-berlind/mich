\section{Multiple Independent Change-Point (MICH) Model}
\label{sec:mich}

We now introduce the Multiple Independent CHange-point (MICH) model. We continue to assume that $\mathbf{y}_{1:T}$ is a sequence of independent Gaussian observations as in (\ref{eq:dgp}). We focus on the univariate case for now and assume that there are: i) $L \geq 0$ changes to just the mean of $\mathbf{y}_{1:T}$, ii) $K \geq 0$ changes to just the variance of $\mathbf{y}_{1:T}$, and iii) $J \geq 0$ joint changes to the mean and variance. Although it is possible to fit the model when $L,K,J>0$, we stress that the generality of the preceding assumption is simply meant to facilitate our presentation of the model. In practice, the analyst will typically be interested in just simultaneous changes to $\boldsymbol{\mu}_{1:T}$ and $\boldsymbol{\lambda}_{1:T}$ ($J>0$ and $L=K=0$), or changes that are allowed to occur independently ($L,K>0$, and $J=0$). We discuss the choice of $L$, $K$, and $J$ in greater detail in Section \ref{sec:LKJ}. To actually model the changes to $\boldsymbol{\mu}_{1:T}$ and $\boldsymbol{\lambda}_{1:T}$, we employ the following composite structure:
\begin{align}
    \mu_{t} &:= \mu_0 + \sum_{\ell = 1}^L \mu_{\ell t} + \sum_{j = 1}^J \mu_{jt} := \mu_0 + \sum_{\ell = 1}^L b_\ell\mathbbm{1}\{t\geq\tau_\ell\} + \sum_{j = 1}^J b_j\mathbbm{1}\{t\geq\tau_j\}, \label{eq:mu_t}\\
    \lambda_t &:= \lambda_0\prod_{k=1}^K \lambda_{kt}\prod_{j=1}^J \lambda_{jt} := \lambda_0\prod_{k=1}^K s_k^{\mathbbm{1}\{t\geq\tau_k\}}\prod_{j=1}^J s_j^{\mathbbm{1}\{t\geq\tau_j\}}. \label{eq:lambda_t}
\end{align}
\textcolor{red}{should we use a different notation for the $\tau$'s and parameters in different sums?} 
Here $\mu_0 \in\mathbb{R}$ and $\lambda_0 > 0$ are the known intercept and initial precision (the case of unknown $\mu_0$ and $\lambda_0$ is addressed in Appendix \ref{app:empirical-bayes}). The parameter blocks $\{\boldsymbol{\theta}_\ell\}_{\ell=1}^L := \{b_\ell, \tau_\ell\}_{\ell=1}^L$, $\{\boldsymbol{\theta}_k\}_{k=1}^K := \{s_k, \tau_k\}_{k=1}^K$, and $\{\boldsymbol{\theta}_j\}_{j=1}^J := \{b_j, s_j, \tau_j\}_{j=1}^J$ are modeled as independent random vectors with $\boldsymbol{\theta}_\ell$, $\boldsymbol{\theta}_k$, and $\boldsymbol{\theta}_j$ following the same respective prior distributions as specified for the mean-scp (\ref{eq:smcp-end}), var-scp (\ref{eq:sscp-end}), and meanvar-scp (\ref{eq:smscp-end}) models.\footnote{Note the slight abuse of notation in these definitions. In addition to indexing the sum and product in (\ref{eq:mu_t}) and (\ref{eq:lambda_t}), the symbols $\ell$, $k$, and $j$ index the mean-scp, var-scp, and meanvar-scp models.} Figure \ref{fig:plate-diagram} shows a plate diagram depicting MICH graphically. We can also adapt MICH to the multivariate setting so long as the sequence of precision matrices $\boldsymbol{\Lambda}_{1:T}$ is  known or can be estimated. We simply restrict $J=K=0$ and replace (\ref{eq:mu_t}) with its $d$-dimensional analogue $\boldsymbol{\mu}_{t} :=\boldsymbol{\mu}_0+ \sum_{\ell = 1}^L \mathbf{b}_\ell\mathbbm{1}\{t \geq\tau_\ell\}$ (see Appendix \ref{app:multi-mich}). 

Equations (\ref{eq:mu_t}) and (\ref{eq:lambda_t}) describe $\boldsymbol{\mu}_{1:T}$ and $\boldsymbol{\lambda}_{1:T}$ as the sum and product of a collection of SCP models. While this modular construction may initially appear convoluted, it allows for computationally efficient inference as shown in Algorithm \ref{alg:mich}. Before detailing how to perform inference under MICH, we briefly point out MICH's connection to preceding Bayesian CPD models. For univariate data, if we set $L > 0$ and $K=J=0$, then MICH recovers CPD version SuSiE with $L$ change-points (see Section 6 of \cite{Wang20}). If $K > 0$ and $L=J=0$, then MICH reduces to the $K$ component PRISCA model (\citealp{Cappello22}). 


\subsection{Fitting MICH}
\label{sec:fit-mich}

Implementing a Gibbs sampler (\citealp{Geman84,Gelfand90}) to perform inference on $\boldsymbol{\Theta}:= \{\boldsymbol{\theta}_{1:L},\boldsymbol{\theta}_{1:K},\boldsymbol{\theta}_{1:J}\}$ is conceptually straightforward. % seeing as the prior distributions for each component $\boldsymbol{\theta}_\ell$, $\boldsymbol{\theta}_k$, $\boldsymbol{\theta}_j$ are conditionally conjugate. 
However, because the change-point locations $\tau_{1:L}$, $\tau_{1:K}$, and $\tau_{1:J}$ are discrete, highly correlated, and live in a high-dimensional space as $L$, $K$, $J$, and $T$ increase, the sampler may suffer from poor mixing and even fail to converge (\citealp{Smith93, Cappello21}). With Algorithm \ref{alg:mich}, we circumvent the challenges of MCMC based inference by implementing a deterministic backfitting procedure (\citealp{Friedman81, Breiman85}) that returns an approximation to the posterior distribution $p(\boldsymbol{\Theta}\:|\:\mathbf{y}_{1:T})$. Algorithm \ref{alg:mich-multi} in Appendix \ref{app:multi-mich} shows how to modify Algorithm \ref{alg:mich} for the case of multivariate mean changes.

\input{MICH/Figures/MICH Algorithm}

To motivate Algorithm \ref{alg:mich}, pretend for the moment that we have access to all of the model parameters in (\ref{eq:mu_t}) and (\ref{eq:lambda_t}) except for those in block $\boldsymbol{\theta}_j$. We can use these known parameters to calculate partial residual terms $r_{-j t} := y_t - \mu_t + \mu_{j t}$ and $\lambda_{-jt} := \lambda_{jt}^{-1}\lambda_t$. Then, $p(\boldsymbol{\theta}_j\:|\:\mathbf{r}_{-j,1:T}, \boldsymbol{\lambda}_{-j,1:T})$ is simply be the meanvar-scp posterior distribution from (\ref{eq:gamma-post-cat1}) and (\ref{eq:bs-smscp}) with parameters defined by the following call of (\ref{eq:meanvar-scp-fn}):
\begin{align}
    \overline{\boldsymbol{\theta}}_j := \{\overline{b}_{jt}, \overline{\omega}_{jt}, \overline{u}_{jt}, \overline{v}_{jt}, \overline{\pi}_{jt}\}_{t=1}^T = \texttt{meanvar-scp}(\mathbf{r}_{-j,1:T}\:;\: \boldsymbol{\lambda}_{-j,1:T}, \omega_j, u_j, v_j, \boldsymbol{\pi}_{j}).
\end{align}
In reality, we do not have access to any of the model parameters, so we cannot calculate $r_{-j t}$ or $\lambda_{-jt}$. However, we can pick some initial values for $\overline{\boldsymbol{\theta}}_j$ as well as for $\overline{\boldsymbol{\theta}}_\ell := \{\overline{b}_{\ell t}, \overline{\omega}_{\ell t}, \overline{\pi}_{\ell t}\}_{t=1}^T$ and $\overline{\boldsymbol{\theta}}_k := \{ \overline{u}_{kt}, \overline{v}_{kt}, \overline{\pi}_{kt}\}_{t=1}^T$, and iterate between: i) calculating partial residuals using $\overline{\boldsymbol{\Theta}} := \{\overline{\boldsymbol{\theta}}_{1:L}, \overline{\boldsymbol{\theta}}_{1:K},\overline{\boldsymbol{\theta}}_{1:J}\}$, and ii) updating $\overline{\boldsymbol{\Theta}}$ by fitting the appropriate SCP model to the partial residuals. At a high level, Algorithm \ref{alg:mich} repeats these two steps until a convergence criterion is met (see Appendix \ref{app:convergence} for details on the stopping rule). Within each iteration of Algorithm \ref{alg:mich}, we can use the current values of $\{\overline{\boldsymbol{\theta}}_{\ell}, \overline{\boldsymbol{\theta}}_{k}, \overline{\boldsymbol{\theta}}_{j}\}$ to define distribution $\{q_\ell, q_k, q_j\}$ over the blocks $\{\boldsymbol{\theta}_\ell, \boldsymbol{\theta}_k, \boldsymbol{\theta}_j\}$. Specifically, we assume that $q_\ell$, $q_k$, and $q_j$ are the respective SCP posteriors (\ref{eq:b-smcp}), (\ref{eq:s-sscp}), and (\ref{eq:bs-smscp}) with parameters $\overline{\boldsymbol{\theta}}_{\ell}$, $\overline{\boldsymbol{\theta}}_{k}$, and $\overline{\boldsymbol{\theta}}_{j}$. Thus, we can use (\ref{eq:mu-post-mean}), (\ref{eq:lambda-post-mean}), and (\ref{eq:mu-lambda-post-mean}) to calculate the respective terms $\E_{q_\ell}[\boldsymbol{\mu}_{\ell t}]$, $\E_{q_k}[\boldsymbol{\lambda}_{k t}]$, and $\E_{q_j}[\boldsymbol{\lambda}_{j t}\boldsymbol{\mu}_{j t}]$ in Algorithm \ref{alg:mich}.

\begin{remark}[Complexity of Algorithm \ref{alg:mich}] \label{rmk:computational-complexity}
    Calculating (\ref{eq:mu-post-mean}), (\ref{eq:lambda-post-mean}), (\ref{eq:mu-lambda-post-mean}) and each term in $\overline{\boldsymbol{\Theta}}$ requires calcuating a cumulative sum of $T$ terms (see Appendix \ref{app:posterior-parameters}), which is an $\mathcal{O}(T)$ operation. Therefore, each outer loop of Algorithm \ref{alg:mich} is $\mathcal{O}((L+K+J)T)$. The actual number of iterations required for Algorithm \ref{alg:mich} to converge is indeterminate and will depend on the chosen stopping criterion. 
\end{remark}
\vspace{-10pt}

For a given block index $i \in \{\ell, k, j\}$, Algorithm \ref{alg:mich} returns change-point location probabilities $\overline{\boldsymbol{\pi}}_{i,1:T}$, which we can use to construct estimators $\hat{t}_{\text{MAP}, i}$ and $\alpha$-level credible sets $\mathcal{CS}(\alpha, \overline{\boldsymbol{\pi}}_i)$ as per (\ref{eq:map}) and (\ref{eq:cs}). We adopt the same detection rule as in Section \ref{sec:cred-sets}, which suggests the following estimates for $L$, $K$, and $J$: 
\begin{align}
    \{\hat{L},\hat{K},\hat{J}\}  := \bigcup_{I \in \{L,K,J\}}|\{ i \in [I] : |\mathcal{CS}(\alpha, \overline{\boldsymbol{\pi}}_i)| \leq \log^{2+\delta} T\}|. \label{eq:LKJ-estimator}
\end{align}

\subsection{Mean-Field Variational Approximation}
\label{sec:variational-bayes}
%The independence assumption in (\ref{eq:mean-field}) is strong, but still allows $q$ to capture arbitrary dependencies within each parameter block $\boldsymbol{\theta}_i$. This is key to the success of MICH and in Section \ref{sec:simulations} we show that $q$ effectively localizes changes in simulations by characterizing the relationship between $\tau_i$ and the rest of $\boldsymbol{\theta}_i$. 

Once Algorithm \ref{alg:mich} converges, we can use $\{q_\ell\}_{\ell=1}^L$, $\{q_k\}_{k=1}^K$, and $\{q_j\}_{j=1}^J$ to define a joint distribution over $\boldsymbol{\Theta}$:
\begin{align}\label{eq:mean-field}
    q_{\text{mich}}(\boldsymbol{\Theta}) := \prod_{\ell=1}^L q_\ell(\boldsymbol{\theta}_\ell)\prod_{k=1}^K q_k(\boldsymbol{\theta}_k) \prod_{j=1}^J q_j(\boldsymbol{\theta}_j).
\end{align}
The distribution $q_{\text{mich}}$ belongs to the family of distributions $\mathcal{Q}_{\text{MF}}$ that satisfy a mean-field assumption over the blocks of $\boldsymbol{\Theta}$, i.e. $q \in \mathcal{Q}_{\text{MF}}$ implies that $q(\boldsymbol{\Theta})$ has the same dependence structure as in (\ref{eq:mean-field}). We now show that within $\mathcal{Q}_{\text{MF}}$, $q_{\text{mich}}$ is the best variational approximation to the true posterior $p:=p(\boldsymbol{\Theta}\:|\:\mathbf{y}_{1:T})$ (see \cite{Blei17} for a recent overview of variational inference). In other words, $ q_{\text{mich}}$ solves $\min_{q \in  \mathcal{Q}_{\text{MF}}}  \; \text{KL}( q \:\lVert\: p)$, where $\text{KL}( \cdot\:\lVert\: \cdot )$ is the Kullbackâ€“Leibler divergence. We begin by defining the evidence lower bound (ELBO):
\begin{align}
    \text{ELBO}(q\:;\:\mu_0,\lambda_0) &:= \int q(\boldsymbol{\Theta}) \log \frac{ p(\mathbf{y}_{1:T},\boldsymbol{\Theta};\mu_0,\lambda_0)}{q(\boldsymbol{\Theta})} \; d\boldsymbol{\Theta} \label{eq:elbo}\\
    &\;= \log p(\mathbf{y}_{1:T};\mu_0,\lambda_0) - \text{KL}( q \:\lVert\: p)\label{eq:kl-decomp}.
\end{align}
The marginal log-likelihood in (\ref{eq:kl-decomp}) does not depend on our choice of $q$, so the $q$ that minimizes the KL-divergence is equivalent to the $q$ that maximizes the ELBO. In other words, our goal is to show that:
\begin{align} \label{eq:restriced-elbo}
    q_{\text{mich}} = \argmax{\{q_\ell\}_{\ell=1}^L,\; \{q_k\}_{k=1}^K,\; \{q_j\}_{j=1}^J}  \; \text{ELBO}\left(\prod_{\ell=1}^L q_\ell \prod_{k=1}^K q_k \prod_{j=1}^J q_j;\mu_0,\lambda_0\right).
\end{align}
The joint optimization task in (\ref{eq:restriced-elbo}) is generally intractable. Instead, we can use a coordinate ascent procedure to iteratively solve for each optimal $q_i$ conditional on the remaining distributions. Under mild regularity conditions on the priors of each block of $\boldsymbol{\Theta}$, the distribution $q_{\text{mich}}$ returned by Algorithm \ref{alg:mich} is converging to a stationary point of $\text{\normalfont ELBO}(q\:;\lambda_0, \mu_0)$.

\begin{proposition}
\label{prop:coord-ascent}
Assume that the prior distributions for each block of $\boldsymbol{\Theta}$ are non-degenerate. Then Algorithm \ref{alg:mich} is a coordinate ascent procedure for maximizing the ELBO (\ref{eq:elbo}) on $ \mathcal{Q}_{\text{MF}}$. Equivalently, Algorithm \ref{alg:mich} is a coordinate descent procedure for solving $\min_{q \in  \mathcal{Q}_{\text{MF}}}  \; \text{KL}( q \:\lVert\: p)$. 
For the $n^{\text{th}}$ iteration of Algorithm \ref{alg:mich}, define $q^{(n)}$ as in (\ref{eq:mean-field}). Then $q^{(n)}$ converges to a stationary point of $\text{\normalfont ELBO}(q\:;\lambda_0, \mu_0)$.
\end{proposition}
\vspace{-10pt}

%The proof of Proposition \ref{prop:stationary-point} is given in Appendix \ref{app:prop2-proof}.

\subsection{Computational Details}

\subsubsection{Choice of \texorpdfstring{$L$}{L}, \texorpdfstring{$K$}{K}, and \texorpdfstring{$J$}{J}}
\label{sec:LKJ}

Suppose that $L^*$, $K^*$, and $J^*$ are the true numbers of mean-only, variance-only, and joint mean and variance changes in $\mathbf{y}_{1:T}$. Since each model component $\boldsymbol{\theta}_\ell$, $\boldsymbol{\theta}_k$, and $\boldsymbol{\theta}_j$ corresponds to a distinct change-point, we must have $L \geq L^*$, $K \geq K^*$, and $J \geq J^*$ for MICH to correctly recover all the changes in $\boldsymbol{\mu}_{1:T}$ and $\boldsymbol{\lambda}_{1:T}$. As shown in both \cite{Wang21} and \cite{Cappello22}, the model is robust to overstating $L$, $K$, and $J$, as the extra $L - L^*$, $K- K^*$, and $J - J^*$ components tend to capture null effects with diffuse posteriors $\overline{\boldsymbol{\pi}}_{\ell,1:T}$, $\overline{\boldsymbol{\pi}}_{k,1:T}$, and $\overline{\boldsymbol{\pi}}_{j,1:T}$ that fail the detection criterion (see Appendix \ref{app:prior} for examples). Some prior knowledge is still required to ensure we do not underestimate the true number of components. In the absence of any information, we could set $L=K=J=\lceil T /\log T \rceil$, i.e. the number of observations divided by the worst case spacing condition from Assumption \ref{assumption:1}. This conservative default increases the complexity of Algorithm \ref{alg:mich} to $\mathcal{O}(T^2/\log T)$. To avoid including so many redundant components in the model, we propose using the value of $\text{ELBO}(q\:;\lambda_0, \mu_0)$ to automatically select $L$, $K$, and $J$ (See Appendix \ref{app:LKJ-choice} for detail). 

Ideally, we would like to select a model based on the \textit{evidence}, i.e. the marginal log-likelihood $\log p(\mathbf{y}_{1:T}; \mu_0,\lambda_0)$, which implicitly depends on  $L$, $K$, and $J$. When the evidence cannot be calculated, the BIC is often used as an approximation during selection. The ELBO is also an approximation to the evidence in the sense that $\text{KL}(q \:\lVert\: p)$ is nonnegative, so (\ref{eq:kl-decomp}) implies that the ELBO lower bounds the evidence, and this bound is tight when $q$ is ``near" the true posterior (\citealp{Blei17}). Using the ELBO in place of the evidence during model selection has recently gained some theoretical justification (\citealp{Cherief18,Cherief19}), and the experiments of Section \ref{sec:simulations} demonstrate the empirical success of using $\text{ELBO}(q\:;\lambda_0, \mu_0)$ to select $L$, $K$, and $J$.

\subsubsection{Prior Parameters}

Outside of choosing $L$, $K$, and $J$, MICH requires virtually no hyperparameter tuning. The sensitivity analysis in Appendix  indicates that our results do not depend on the values of the prior parameters for the $i^{\text{th}}$ model component. This result has theoretical support, as the finite sample results in the proofs of Theorems \ref{theorem:smcp}-\ref{theorem:alpha-mixing} each hold for any small value of $\omega_i$, $u_i$, and $v_i$, and the asymptotic localization results hold for any fixed values for these parameters. By default, we set $\omega_i = u_i = v_i = 0.001$ and choose $\pi_{it}$ so that $\overline{\pi}_{it} \approx T^{-1}$ under the null model (see Appendix \ref{app:prior} for more detail).

\subsubsection{Model Asymmetry}

By construction, the variable $\tau_i \in [T]$ is used in MICH to ``activate" the $i^{\text{th}}$ component of the model via the indicator $\mathbbm{1}\{t \geq \tau_i\}$. The direction of this inequality is arbitrary, and we could just as easily have decided to ``deactivate" components with  $\mathbbm{1}\{t < \tau_i\}$ instead. In Appendix \ref{app:prior}, we show how that the direction of the inequality builds an asymmetry into the model, which may artificially inflate the elements of $\overline{\boldsymbol{\pi}}_{i,1:T}$ closer to index $T$. The default prior we have chosen for MICH attempts to mitigate the effects of this asymmetry, but the model still occasionally misses change-points that it would have identified if the order of $\mathbf{y}_{1:T}$ were reversed (see for example ). To overcome this undesirable behavior, we fit MICH using $\mathbf{y}_{T:1}$, then reverse the fitted parameters and use them to restart Algorithm \ref{alg:mich}. We return the resulting fit if it improves the ELBO over directly inputting $\mathbf{y}_{1:T}$ into Algorithm \ref{alg:mich}. The exprimental results in Section \ref{sec:simulations} show that this procedure leads to a marked improvement in the performance of MICH. Since we can fit the model to $\mathbf{y}_{1:T}$ and $\mathbf{y}_{T:1}$ in parallel, this improvement comes at little additional cost. 

\subsubsection{Duplicate Components}
\label{sec:merge-procedure}

The mean field assumption (\ref{eq:mean-field}) is key to unlocking the computational simplicity of Algorithm \ref{alg:mich}, but because each block of $\boldsymbol{\Theta}$ is independent \textit{a priori}, there is a non-zero probability that $\tau_i = \tau_{i'}$ for some $i \neq i'$, i.e. two changes of the same class may be ``stacked" on top of each other. This construction differs from the product partition model of \cite{Barry93}, which rules out overlapping change-points by placing a joint prior on all possible fixed-sized partitions of $[T]$. Although Algorithm \ref{alg:mich} typically avoids assigning multiple components to a single change-point, we do observe instances of a single change splitting across multiple blocks of $\boldsymbol{\Theta}$ (see for example ). If $L$, $K$, or $J$ are not large enough to allow for some degree of redundancy in the model, then this behavior may result in an underestimation of the true number of changes. To avoid this scenario and eliminate duplicates, we propose merging components $i$ and $i'$ if $\langle\overline{\boldsymbol{\pi}}_{i'}, \overline{\boldsymbol{\pi}}_i \rangle$ exceeds some threshold $\beta > 0$. The details of this merge procedure are given in Appendix \ref{app:merge-procedure}.
