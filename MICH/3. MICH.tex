\section{Multiple Independent Change-Point (MICH) Model}
\label{sec:mich}

We now introduce the Multiple Independent CHange-point (MICH) model. By modularly combining the mean-scp, var-scp, and meanvar-scp models of Section \ref{sec:scp}, we are able to generate multiple changes in the piece-wise constant structures of both $\boldsymbol{\mu}_{1:T}$ and $\boldsymbol{\lambda}_{1:T}$. We independently generate $L$ additive changes to $\boldsymbol{\mu}_{1:T}$, $K$ multiplicative changes to $\boldsymbol{\lambda}_{1:T}$, and $J$ simultaneous changes to $\boldsymbol{\mu}_{1:T}$ and $\boldsymbol{\lambda}_{1:T}$: 
\begin{align}
    \boldsymbol{\theta}_\ell &:= \{b_\ell, \gamma_\ell\} \overset{\text{ind.}}{\sim} \text{mean-scp}(0, \tau_\ell, \boldsymbol{\pi}_{\ell}) \label{eq:mich-l-component} \\
    \boldsymbol{\theta}_k &:= \{s_k, \gamma_k\} \overset{\text{ind.}}{\sim} \text{var-scp}(u_k, v_k, \boldsymbol{\pi}_{k}) \label{eq:mich-k-component} \\
    \boldsymbol{\theta}_j &:= \{b_j, s_j, \gamma_j\} \overset{\text{ind.}}{\sim} \text{meanvar-scp}(0, \tau_j, u_j, v_j, \boldsymbol{\pi}_{j}). \label{eq:mich-j-component}
\end{align}
For each parameter block, we define the mean components $\mu_{\ell t}$ and $\mu_{jt}$ as in (\ref{eq:smcp-start}), and scale components $\lambda_{kt}$ and $\lambda_{jt}$ as in (\ref{eq:sscp-start}). Given a potentially unknown intercept $\mu_0$ and initial precision $\lambda_0$, we then aggregate these components into combined mean and scale signals:
\begin{align}
    \mu_{t} &:= \mu_0 + \sum_{\ell = 1}^L \mu_{\ell t} + \sum_{j = 1}^J \mu_{jt} \label{eq:mu_t}\\
    \lambda_t &:= \lambda_0\prod_{k=1}^K \lambda_{kt}\prod_{j=1}^J \lambda_{jt}. \label{eq:lambda_t}
\end{align}
Together with (\ref{eq:dgp}), (\ref{eq:mich-l-component})-(\ref{eq:lambda_t}) fully specify the MICH model.\footnote{Note the slight abuse of notation in these definitions. In addition to indexing the sum and product in (\ref{eq:mu_t}) and (\ref{eq:lambda_t}), the symbols $\ell$, $k$, and $j$ index which class of SCP model generated each mean and scale component.} A plate diagram depicting MICH graphically is available in Figure \ref{fig:plate-diagram}. Modeling $\boldsymbol{\mu}_{1:T}$ and $\boldsymbol{\lambda}_{1:T}$ as the sum and product of many single change-point components may seem convoluted, but this construction has computational advantages that will become apparent in Section \ref{sec:fit-mich}. We can also adapt MICH to the multivariate setting when the precision matrix $\boldsymbol{\Lambda}_t$ is known or can be estimated. We restrict $J=K=0$ and replace (\ref{eq:mich-l-component}) and (\ref{eq:mu_t}) respectively with:
\begin{align}
    \boldsymbol{\theta}_\ell &:= \{\mathbf{b}_\ell, \gamma_\ell\} \overset{\text{ind.}}{\sim} \text{mean-scp}(\mathbf{0}, \tau_\ell\mathbf{I}_d, \boldsymbol{\pi}_{\ell}) \label{eq:mich-multi} \\
    \boldsymbol{\mu}_{t} &:= \boldsymbol{\mu}_0 + \sum_{\ell = 1}^L \boldsymbol{\mu}_{\ell t}. \label{eq:mu_t-multi}
\end{align} 
MICH is a generalization of PRISCA (\citealp{Cappello22}) as well as the CPD version of SuSiE from Section 6 of \cite{Wang20}. If we set $L > 0$ and $K=J=0$, then MICH recovers the $L$ component SuSiE model applied to the same covariate matrix $\mathbf{X}$ described in Section \ref{sec:smcp}, and if $K > 0$ while $L=J=0$, then MICH reduces to the $K$ component PRISCA model. Unlike SuSiE and PRISCA, MICH can detect both multiple separate and simultaneous changes in $\boldsymbol{\mu}_{1:T}$ and $\boldsymbol{\lambda}_{1:T}$. Even though MICH has the ability to handle a combination of different types of changes, we stress that our presentation of a single unified model is primarily for notational convenience. In practice, the analyst will typically be interested in identifying a single category of change-points (i.e. only one of $L$, $K$, and $J$ will be positive).

\subsection{Fitting MICH}
\label{sec:fit-mich}

Standard MCMC based methods are ill-equipped to perform inference with the MICH model. The prior distributions for each of the SCP components (\ref{eq:mich-l-component})-(\ref{eq:mich-j-component}) are conditionally conjugate, so designing a Gibbs sampler (\citealp{Geman84}) is conceptually straightforward. However, the change-point locations $\{\gamma_\ell,\gamma_k, \gamma_j\}$ are discrete, highly correlated, and live in a high-dimensional space as $L$, $K$, $J$, and $T$ increase. Under these conditions, Gibbs sampling will suffer from poor mixing the sampler may fail to converge at all (\citealp{Smith93, Cappello21}). We circumvent the need for MCMC with Algorithm \ref{alg:1}, a computationally efficient backfitting procedure (\citealp{Friedman81, Breiman85}) that subsumes the IBSS and ProSCALE algorithms of \cite{Wang20} and \cite{Cappello22}. In Appendix , we show how to modify Algorithm \ref{alg:1} for the case of multivariate mean changes.

\input{MICH/Figures/Algorithm 1}

To motivate Algorithm \ref{alg:1}, let $\mu_0 = 0$ and $\lambda_0 = 1$ (the case of unknown $\mu_0$ and $\lambda_0$ is addressed in Appendix \ref{app:empirical-bayes}) and consider the residual term:
\begin{align}\label{eq:residual}
    r_t &:= y_t - \sum_{j = 1}^J \mu_{jt}  - \sum_{\ell = 1}^L \mu_{\ell t}.
\end{align}
If we had access to all of the MICH model parameters except for $\boldsymbol{\theta}_j$, we could calculate the corresponding mean and scale partial residuals: 
\begin{align}
    r_{-j t} &:= r_t + \mu_{j t}. \label{eq:mean-resid} \\
    \lambda_{-jt} &:= \lambda_{jt}^{-1}\lambda_t. \label{eq:scale-resid}
\end{align}
The conditional distribution for the single joint mean and variance change in $\{\lambda_{-jt}^\frac{1}{2}r_{-j t}\}_{t=1}^T$ would simply be the meanvar-scp posterior with parameters defined by the following call of (\ref{eq:meanvar-scp-fn}):
\begin{align}
    \overline{\boldsymbol{\theta}}_j := \{\overline{\mathbf{b}}_{j}, \overline{\boldsymbol{\tau}}_{j}, \overline{\mathbf{u}}_{j}, \overline{\mathbf{v}}_{j}, \overline{\boldsymbol{\pi}}_{j}\} = \texttt{meanvar-scp}(\mathbf{r}_{-j}\:;\: \boldsymbol{\lambda}_{-j}, \tau_j, u_j, v_j, \boldsymbol{\pi}_{j}).
\end{align}
For $\boldsymbol{\theta}_\ell$ and $\boldsymbol{\theta}_k$, we could define analogous partial residuals $\mathbf{r}_{-\ell}$ and $\boldsymbol{\lambda}_{-k}$ (conditional on the rest of the parameters) and determine the conditional posteriors with calls of \texttt{mean-scp} and \texttt{var-scp}. The challenge is that we do not have access to any of the model parameters, so  in reality we cannot calculate the partial residuals (\ref{eq:mean-resid}) and (\ref{eq:scale-resid}). We can however, i) choose initial approximations of $\overline{\boldsymbol{\Theta}} := \{\{\overline{\boldsymbol{\theta}}_\ell\}_{\ell=1}^L$, $\{\overline{\boldsymbol{\theta}}_k\}_{k=1}^K$, $\{\overline{\boldsymbol{\theta}}_j\}_{j=1}^J\}$, ii) calculate partial residuals using $\overline{\boldsymbol{\Theta}}$, and iii) update $\overline{\boldsymbol{\Theta}} $ by fitting the appropriate SCP model to the partial residuals. At a high level, Algorithm \ref{alg:1} iteratively repeats these three steps until some convergence criterion is met. Within each iteration of Algorithm \ref{alg:1}, $\overline{\boldsymbol{\Theta}} $ fully characterizes the marginal distributions of the blocks $\boldsymbol{\theta}_\ell$, $\boldsymbol{\theta}_k$, and $\boldsymbol{\theta}_j$:
\begin{align}
    q_\ell(\boldsymbol{\theta}_\ell) \sim \text{mean-scp}(\overline{\boldsymbol{\theta}}_\ell), \; q_k(\boldsymbol{\theta}_k) \sim \text{var-scp}(\overline{\boldsymbol{\theta}}_k) ,\; q_j(\boldsymbol{\theta}_j) \sim \text{meanvar-scp}(\overline{\boldsymbol{\theta}}_j). \label{eq:q-LKJ}
\end{align}
We can use these distributions to calculate the expected terms required in each update step (See Appendix for detailed derivations):
\small
\begin{align}
    \E_{q_\ell}[\mu_{\ell t}] = \sum_{t'=1}^t \overline{b}_{\ell t'} \overline{\pi}_{\ell t'},\quad \E_{q_k}[\lambda_{kt}] = \sum_{t'=1}^{t} \frac{\overline{u}_{kt'}\overline{\pi}_{kt'}}{\overline{v}_{kt'}}  + \sum_{t'=t+1}^T \overline{\pi}_{kt'},\quad \E_{q_j}[\lambda_{jt}\mu_{jt}] =  \sum_{t'=1}^t \frac{\overline{b}_{j t'}\overline{u}_{jt'}\overline{\pi}_{jt'} }{\overline{v}_{jt'}}. \label{eq:e-mu-lambda}
\end{align}
\normalsize

\begin{remark}[Computational Complexity of Algorithm \ref{alg:1}] \label{rmk:computational-complexity}
    Taking the cumulative sum of $T$ terms is $\mathcal{O}(T)$; therefore, so is calculating each term in (\ref{eq:e-mu-lambda}). Similarly, in Appendix \ref{app:posterior-parameters} we show that the posterior parameters in the SCP models depend on cumulative sums of $T$ terms, so each outer loop of Algorithm \ref{alg:1} is $\mathcal{O}((L+K+J)T)$. The actual number of iterations required for Algorithm \ref{alg:1} to converge is indeterminate and will depend on the chosen criterion. We discuss the stopping rule in greater detail in Appendix \ref{app:convergence}. 
\end{remark}

Upon convergence, Algorithm \ref{alg:1} returns estimates for the posterior probabilities of the change-point locations. For each component index $i \in \{\ell, k, j\}$, we can construct MAP estimators $\hat{t}_{\text{MAP}, i}$  as in (\ref{eq:map}) and $\alpha$-level credible sets $\mathcal{CS}(\alpha, \overline{\boldsymbol{\pi}}_i)$ as in (\ref{eq:cs}). We adopt the same detection rule from Section \ref{sec:cred-sets} with $\delta$ = 0.1 by default, which suggests the following estimates for $L$, $K$, and $J$: 
\begin{align}
    \{\hat{L},\hat{K},\hat{J}\}  := \bigcup_{I \in \{L,K,J\}}|\{ i \in [I] : |\mathcal{CS}(\alpha, \overline{\boldsymbol{\pi}}_i)| \leq \log^{2+\delta} T\}|. \label{eq:LKJ-estimator}
\end{align}

\subsection{Mean-Field Variational Approximation}
\label{sec:variational-bayes}

Let $p$ be the true posterior distribution for the MICH model parameters $\boldsymbol{\Theta}:= \{\{\boldsymbol{\theta}_\ell\}_{\ell=1}^L, \{\boldsymbol{\theta}_k\}_{k=1}^K, \{\boldsymbol{\theta}_j\}_{j=1}^J\}$. We can use the distributions in (\ref{eq:q-LKJ}) to approximate $p$ by defining a new distribution $q$ such that:
\begin{align}\label{eq:mean-field}
    q(\boldsymbol{\Theta}) :=  \prod_{\ell=1}^L q_\ell(\boldsymbol{\theta}_\ell)\prod_{k=1}^K q_k(\boldsymbol{\theta}_k) \prod_{j=1}^J q_j(\boldsymbol{\theta}_j).
\end{align}
The independence assumption in (\ref{eq:mean-field}) is strong, but still allows $q$ to capture arbitrary dependencies within each parameter block $\boldsymbol{\theta}_i$. This is key to the success of MICH and in Section \ref{sec:simulations} we show that $q$ effectively localizes changes in simulations by characterizing the relationship between $\gamma_i$ and the rest of $\boldsymbol{\theta}_i$. 

To provide formal justification for the success of $q$ as an approximation to $p$, we build upon the work of \cite{Wang20} and \cite{Cappello22}, showing that $q$ constitutes a variational approximation to $p$ (see \cite{Jordan99} for the classical treatment of variational Bayesian inference, and \cite{Blei17} for a recent survey of the literature). Given a family of distributions $\mathcal{Q}$, we call $q^* \in \mathcal{Q}$ a Variational Bayes (VB) approximation of the true posterior $p$ if $q^*$ is the ``closest" distribution to $p$ in terms of the KL-divergence (\citealp{Kullback51}):
\begin{align} \label{eq:restriced-kl}
    q^* = \argmin{q \in \mathcal{Q}}  \; \text{KL}( q \:\lVert\: p).
\end{align}
When no closed-form for $p$ is available, we can choose $\mathcal{Q}$ to make the optimization task in (\ref{eq:restriced-kl}) tractable. Restricting $\mathcal{Q}$ so that each $q \in \mathcal{Q}$ exhibits independence across the blocks of $\boldsymbol{\Theta}$ as in (\ref{eq:mean-field}) is called a mean-field assumption (\citealp{Wainwright08}). Our goal is to show that the distribution returned by Algorithm \ref{alg:1} is the optimal solution to (\ref{eq:restriced-kl}) among all distributions with the mean-field dependence structure specified in (\ref{eq:mean-field}). We begin by defining the evidence lower bound (ELBO) for a given $q$:
\begin{align}\label{eq:elbo}
    \text{ELBO}(q;\mu_0,\lambda_0) := \int q(\boldsymbol{\Theta}) \log \frac{ p(\mathbf{y}_{1:T},\boldsymbol{\Theta};\mu_0,\lambda_0)}{q(\boldsymbol{\Theta})} \; d\boldsymbol{\Theta}.
\end{align}
From (\ref{eq:elbo}) we get the following decomposition:
\begin{align}\label{eq:kl-decomp} 
     \text{KL}( q \:\lVert\: p) &= \log p(\mathbf{y}_{1:T};\mu_0,\lambda_0) - \text{ELBO}(q;\mu_0,\lambda_0). 
\end{align}
The marginal log-likelihood term in (\ref{eq:kl-decomp}) does not depend on our choice of $q$; therefore, under the factorization in (\ref{eq:mean-field}), the solution to (\ref{eq:restriced-kl}) is equivalent to:
\begin{align} \label{eq:restriced-elbo}
    q^* = \argmax{\{q_\ell\}_{\ell=1}^L,\; \{q_k\}_{k=1}^K,\; \{q_j\}_{j=1}^J}  \; \text{ELBO}\left(\prod_{\ell=1}^L q_\ell \prod_{k=1}^K q_k \prod_{j=1}^J q_j;\mu_0,\lambda_0\right).
\end{align}
The joint optimization task in (\ref{eq:restriced-elbo}) is generally intractable; however, the subproblem of finding the optimal $q_i$ for just the block $\boldsymbol{\theta}_i$ reduces to simply fitting one of the SCP models from Section \ref{sec:scp}.

\begin{proposition} 
\label{prop:1}
Given the distributions $\{q_\ell\}_{\ell=1}^L$, $\{q_k\}_{k=1}^K$, and $\{q_j\}_{j=1}^J$, define $q$ as in (\ref{eq:mean-field}) and define the terms: 
\small
\begin{align}
    \tilde{r}_t &:= y_t - \sum_{\ell = 1}^L \E_{q_\ell}[\mu_{\ell t}] - \sum_{j = 1}^J \frac{\E_{q_j}[\lambda_{jt} \mu_{jt}]}{\E_{q_j}[\lambda_{jt}]}, \label{eq:mod-resid} \\
    \overline{\lambda}_t &:= \prod_{k=1}^K \E_{q_k}[\lambda_{kt}] \prod_{j=1}^J \E_{q_j}[\lambda_{jt}], \label{eq:lambda-bar} \\
    \delta_t &:= \sum_{\ell=1}^L \Var_{q_\ell}\left(\mu_{\ell t} \right) +  \sum_{j=1}^J\left(\frac{\E_{q_j}[\lambda_{jt} \mu^2_{jt}]}{\E_{q_j}[\lambda_{jt}]} -\frac{\E_{q_j}[\lambda_{jt} \mu_{jt}]^2}{\E_{q_j}[\lambda_{jt}]^2} \right). \label{eq:delta}
\end{align}
\normalsize
\begin{enumerate}[label=\roman*.]
    \item Define the partial mean residual $\tilde{r}_{-\ell t} :=  \tilde{r}_{t} + \E_{q_{\ell}}[\mu_{\ell t}]$ and calculate the posterior parameters $\overline{\boldsymbol{\theta}}_\ell := \normalfont{\texttt{mean-scp}}(\tilde{\mathbf{r}}_{-\ell} \:;\: \overline{\boldsymbol{\lambda}}_{1:T}, \tau_{\ell}, \boldsymbol{\pi}_{\ell})$. Then, $q^*_\ell \equiv \text{\normalfont mean-scp}(\overline{\boldsymbol{\theta}}_\ell)$ solves $\max_{q_\ell} \; \text{\normalfont ELBO}\left(q;\mu_0,\lambda_0\right)$.
    \vspace{-5pt}
    
    \item Define the partial scale residual $\overline{\lambda}_{-kt} :=  \E_{q_k}[\lambda_{k t}]^{-1}\overline{\lambda}_t$. Calculate the variance corrected priors $\tilde{v}_{kt}$ and $\tilde{\pi}_{kt}$ as in () and () and the posterior parameters $\overline{\boldsymbol{\theta}}_k := \normalfont{\texttt{var-scp}}(\tilde{\mathbf{r}}_{1:T} \:;\:\overline{\boldsymbol{\lambda}}_{-k}, u_k, \tilde{\mathbf{v}}_{k}, \tilde{\boldsymbol{\pi}}_k)$.  Then, $q^*_k \equiv \text{\normalfont var-scp}(\overline{\boldsymbol{\theta}}_k)$ solves $\max_{q_k}  \; \text{\normalfont ELBO}\left(q;\mu_0,\lambda_0\right)$.
    \vspace{-5pt}

    \item Define the partial mean residuals $\tilde{r}_{-jt} := \tilde{r}_{t} +\E_{q_j}[\lambda_{jt}]^{-1} \E_{q_j}[\lambda_{jt} \mu_{j t}]$, the partial scale residual $\overline{\lambda}_{-jt} := \E_{q_j}[\lambda_{j t}]^{-1}\overline{\lambda}_t$, and the partial variance correction term:
    \begin{align}
        \delta_{-jt} &:= \delta_t - \frac{\E_{q_j}[\lambda_{jt} \mu^2_{jt}]}{\E_{q_j}[\lambda_{jt}]} + \frac{\E_{q_j}[\lambda_{jt} \mu_{jt}]^2}{\E_{q_j}[\lambda_{jt}]^2}. \label{eq:delta_j}
    \end{align}
    Calculate the variance corrected priors $\tilde{v}_{jt}$ and $\tilde{\pi}_{jt}$ as in () and () and the posterior parameters $\overline{\boldsymbol{\theta}}_j := \normalfont{\texttt{meanvar-scp}}(\tilde{\mathbf{r}}_{-j} \:;\:\overline{\boldsymbol{\lambda}}_{-j}, \tau_j, u_j, \tilde{\mathbf{v}}_{j}, \tilde{\boldsymbol{\pi}}_j)$. Then, $q^*_j \equiv \text{\normalfont meanvar-scp}(\overline{\boldsymbol{\theta}}_j)$ solves $\max_{q_j}  \; \text{\normalfont ELBO}\left(q;\mu_0,\lambda_0\right)$.
\end{enumerate}
\end{proposition}
%The proof of Proposition \ref{prop:1} is given in Appendix \ref{app:prop1-proof}. 
Backfitting procedures for models with additive effects typically work with the expected residual $\E_q[r_t]$ (\citealp{Breiman85,Hastie90,Friedman00,Wang20}). Algorithm \ref{alg:1} on the other hand uses the modified residual $\tilde{r}_t$ to fit the individual SCP models, which is necessitated by the inclusion of both mean and variance changes in the MICH model. Similarly, the variance correction term $\delta_t$ plays an important role in Algorithm \ref{alg:1} that did not previously appear in either the IBSS or ProSCALE algorithms. The variance of $\mathbf{y}_{1:T}$ is inflated by the uncertainty around each mean change, and Proposition \ref{prop:1} shows how to use $\delta_t$ to account for the added noise from $\mu_{\ell t}$ and $\mu_{j t}$ when fitting the var-scp and meanvar-scp components.

Upon re-examination, we see that Algorithm \ref{alg:1} begins by initializing some distribution $q$ that satisfies (\ref{eq:mean-field}). Then, by solving the appropriate maximization problem in Proposition \ref{prop:1} (i)-(iii), Algorithm \ref{alg:1} iteratively updates each $q_{i}(\boldsymbol{\theta}_{i})$ while holding all the other component distributions of $q$ fixed. Thus, Algorithm \ref{alg:1} is a coordinate ascent procedure for solving the maximization problem in (\ref{eq:restriced-elbo}).

\begin{corollary}
\label{cor:coord-ascent}
Algorithm \ref{alg:1} is a coordinate descent procedure for solving the minimization problem in (\ref{eq:restriced-kl}) subject to the mean-field restriction (\ref{eq:mean-field}). Equivalently, Algorithm \ref{alg:1} is a coordinate ascent procedure for maximizing the ELBO in (\ref{eq:restriced-elbo}).
\end{corollary}

Proposition \ref{prop:1} and Corollary \ref{cor:coord-ascent} establish that the distribution returned by Algorithm \ref{alg:1} is a VB approximation to the true posterior of MICH, but we must still show that Algorithm \ref{alg:1} is guarantied to converge. The parameters returned by the respective calls of \texttt{mean-scp}, \texttt{var-scp}, and \texttt{meanvar-scp} fully characterize he distributions $q_\ell$, $q_k$, and $q_j$ for each iteration of Algorithm \ref{alg:1}. Proposition \ref{prop:stationary-point} establishes that these parameters converge to a stationary point. Therefore, the distribution returned by Algorithm \ref{alg:1} also converges. 

\begin{proposition}
\label{prop:stationary-point}
Assume that each prior parameter for each component (\ref{eq:mich-l-component}), (\ref{eq:mich-k-component}), and (\ref{eq:mich-j-component}) of the MICH model is strictly positive. Then the sequence of parameters $\{\overline{\boldsymbol{\Theta}}^{(n)}\}_{n \geq 1} := \{\overline{\boldsymbol{\theta}}_\ell^{(n)}, \overline{\boldsymbol{\theta}}_k^{(n)}, \overline{\boldsymbol{\theta}}_j^{(n)}\}_{n\geq 1}$ generated by each iteration of Algorithm \ref{alg:1} converges to some limit point. Therefore, the sequence of distributions $\{q^{(n)}\}_{n\geq 1}$ generated by Algorithm \ref{alg:1} will converge to a stationary point of $\text{\normalfont ELBO}(q\:;\lambda_0, \mu_0)$.
\end{proposition}
%The proof of Proposition \ref{prop:stationary-point} is given in Appendix \ref{app:prop2-proof}.

\subsection{Computational Details}

\subsubsection{Prior Parameters}

Outside of choosing the number of components $L$, $K$, and $J$, MICH requires almost no hyperparameter tuning. The model is insensitive to the value of prior parameters $\tau_i$, $u_i$, and $v_i$ for each  $i \in \{\ell, k, j\}$, as demonstrated by the sensitivity analysis in Appendix . This result has theoretical support, as the finite sample results in the proofs of Theorems \ref{theorem:smcp}-\ref{theorem:alpha-mixing} all hold for any small value of $\tau_i$, $u_i$, and $v_i$, and the asymptotic localization results hold for any fixed values for these parameters. By default, we set $\tau_i = u_i = v_i = 0.001$. We also have intuitive defaults for $\boldsymbol{\pi}_i$, e.g. the uniform prior $\pi_{it} = T^{-1}$. By default, we choose $\boldsymbol{\pi}_i$ so that $\overline{\pi}_{it} \approx T^{-1}$ under the null model (see Appendix \ref{app:prior} for more detail).

\subsubsection{Choice of \texorpdfstring{$L$}{L}, \texorpdfstring{$K$}{K}, and \texorpdfstring{$J$}{J}}

Suppose that $L^*$, $K^*$, and $J^*$ are the true numbers of mean-only, variance-only, and joint mean and variance changes in $\mathbf{y}_{1:T}$. Since each model component in (\ref{eq:mich-l-component})-(\ref{eq:mich-j-component}) corresponds to a single change-point, we must have $L \geq L^*$, $K \geq K^*$, and $J \geq J^*$ in order for MICH to correctly recover all the changes in $\boldsymbol{\mu}_{1:T}$ and $\boldsymbol{\lambda}_{1:T}$. As shown in both \cite{Wang21} and \cite{Cappello22}, the model is robust to overstating $L$, $K$, and $J$. The extra $L - L^*$, $K- K^*$, and $J - J^*$ components tend to capture null effects with diffuse posteriors $\overline{\boldsymbol{\pi}}_{\ell}$, $\overline{\boldsymbol{\pi}}_{k}$, and $\overline{\boldsymbol{\pi}}_{j}$ that fail the detection criterion (see Appendix \ref{app:prior} for examples). 

Some prior knowledge of the magnitude of $L$, $K$, and $J$ is still required to ensure we don't underestimate the true number of components. In the true absence of any information on the number of changes, we could conservatively set $L=K=J=\lceil T /\log^{1+\varepsilon} T \rceil$, i.e. the number of observations divided by the minimum spacing condition from Assumption \ref{assumption:1}. However, this is an extremely conservative upper bound, and the inclusion of so many redundant components will cause a considerable slowdown in the convergence of Algorithm \ref{alg:1}. Instead, we propose using the value of $\text{ELBO}(q\:;\lambda_0, \mu_0)$ to automatically select $L$, $K$, and $J$. In the standard case where we only consider a single class of change-points, e.g. when $J^* > 0$ and $L^*=K^*=0$, we begin from the null model with $J=0$, then increment $J$ until the ELBO stops increasing.\footnote{For finite samples, the ELBO does not necessarily increase monotonically from $J = 1$ on $J = J^*$, so we continue the search for an additional $\log T$ steps after local maximum is found.} To increase the speed of the search, we can use the fitted parameters from the $J-1$ component model to initialize Algorithm \ref{alg:1} after incrementing $J$. To search over $L$, $K$, and $J$ simultaneously, we can individually increment $L$, $K$, and $J$ and pick the direction that leads to the largest increase in $\text{ELBO}(q\:;\lambda_0, \mu_0)$.

Using $\text{ELBO}(q\:;\lambda_0, \mu_0)$ to perform model selection has gained some recent theoretical justification in the VB literature (\citealp{Cherief18, Cherief19}). We ideally would like to use the marginal log-likelihood $\log p(\mathbf{y}_{1:T}; \mu_0,\lambda_0)$ for model selection. Within the VB literature this term is referred to as the model ``evidence" (hence ELBO), and in our case it implicitly depends on  $L$, $K$, and $J$. Unfortunately, the evidence is generally not calculable, but $\text{KL}(q \:\lVert\: p)$ is nonnegative, so from (\ref{eq:kl-decomp}) we get that $\text{ELBO}(q\:;\lambda_0, \mu_0)$ is a lower bound for the evidence. When this bound is tight, i.e. when $q$ is ``close" to the true posterior, then the ELBO is a good approximation to the marginal likelihood and can be used for model selection (\citealp{Blei17}). This is analogous selecting based on the BIC, which is just another approximation to the marginal log-likelihood (\citealp{Schwarz78}).

\subsubsection{Model Asymmetry}

By construction, the variable $\gamma_i \in [T]$ is used in MICH to ``turn-on" the $i^{\text{th}}$ component of the model via the indicator $\mathbbm{1}\{t \geq \gamma_i\}$. The direction of this inequality is arbitrary, and we could have just as easily decided to ``turn-off" components with  $\mathbbm{1}\{t < \gamma_i\}$ instead. In Appendix \ref{app:prior}, we show how that the direction of the inequality builds an asymmetry into the model, which may artificially inflate the elements of $\overline{\boldsymbol{\pi}}_{i}$ closer to index $T$. The default prior we have chosen for MICH attempts to mitigate the effects of this asymmetry, but the model does still occasionally miss change-points that it would have identified if the order of $\mathbf{y}_{1:T}$ were reversed (see for example ). To overcome this undesirable behavior, we also fit MICH using $\mathbf{y}_{T:1}$, then reverse the fitted parameters and use them to restart Algorithm \ref{alg:1}. We return the resulting fit if it improves the ELBO over inputting $\mathbf{y}_{1:T}$ directly into Algorithm \ref{alg:1}. The simulations in Section \ref{tab:hsmuce-sim} show that this procedure leads to a marked improvement in the performance of MICH, and since we can fit the model to $\mathbf{y}_{1:T}$ and $\mathbf{y}_{T:1}$ in parallel, this boost comes at little additional cost. 

\subsubsection{Duplicate Components}
\label{sec:merge-procedure}

The independence of the components (\ref{eq:mich-l-component})-(\ref{eq:mich-j-component}) is key to unlocking the computational simplicity of Algorithm \ref{alg:1}, but because each component is generated independently, there is a non-zero probability that $\gamma_i = \gamma_{i'}$ for some $i \neq i'$, i.e. two changes of the same class may be ``stacked" on top of each other. This construction differs from the product partition model of \cite{Barry93}, which rules out overlapping change-points by placing a joint prior on all possible fixed-sized partitions of $[T]$. Although Algorithm \ref{alg:1} typically avoids assigning multiple components to a single change-point, we do observe instances of a single change splitting across multiple $\gamma_i$'s (see for example ). In these cases, if $L$, $K$, or $J$ are not large enough to allow for some degree of redundancy, then MICH may underestimate of the true number of changes. To avoid this scenario and eliminate duplicates, we propose merging components $i$ and $i'$ if $\langle\overline{\boldsymbol{\pi}}_{i'}, \overline{\boldsymbol{\pi}}_i \rangle$ exceeds some threshold $\beta > 0$. The details of this merge procedure are given in Appendix \ref{app:merge-procedure}.
