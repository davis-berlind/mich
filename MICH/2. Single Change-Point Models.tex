\section{Single Change-Point (SCP) Models}
\label{sec:scp}

Suppose that we have $T$ observations of a $d$-dimensional time-series $\mathbf{y}_{1:T}$ where each $\mathbf{y}_t$ is generated according to:
\begin{align}\label{eq:dgp}
    \mathbf{y}_t \:|\: \boldsymbol{\mu}_t, \boldsymbol{\Lambda}_t \overset{\text{ind.}}{\sim} \mathcal{N}_d\left(\boldsymbol{\mu}_t, \boldsymbol{\Lambda}^{-1}_t\right), \;\sforall t \in [T].
\end{align}
We assume that either $\boldsymbol{\mu}_{1:T} := \{\boldsymbol{\mu}_t\}_{t=1}^{T}$, $\boldsymbol{\Lambda}_{1:T} := \{\boldsymbol{\Lambda}_t\}^{T}_{t=1}$, or both exhibit piece-wise constant structures with a single change-point occurring at some unknown time $\gamma \in [T]$.\footnote{In some cases it may make sense to restrict $\gamma$ to a subset of $[T]$, e.g. it is common in the CPD literature to assume that there is a buffer at the start and end of $\mathbf{y}$ within which no change $\boldsymbol{\mu}_{1:T}$ or $\boldsymbol{\Lambda}_{1:T}$ can occur. We show how to include such a buffer in our model in Appendix \ref{app:buffer}.} In this section we introduce three single change-point (SCP) models for the random processes that generate $\gamma$ along with the jumps in $\boldsymbol{\mu}_{1:T}$ and $\boldsymbol{\Lambda}_{1:T}$. In Section \ref{sec:smcp} and Section \ref{sec:sscp} we introduce models that handle the respective cases of either a single change-point in $\boldsymbol{\mu}_{1:T}$ or a single change-point in $\boldsymbol{\Lambda}_{1:T}$, while the model in Section \ref{sec:smscp} addresses the setting of a simultaneous change in both $\boldsymbol{\mu}_{1:T}$ and $\boldsymbol{\Lambda}_{1:T}$. In each case, by modeling $\gamma$ as a categorical random variable, and by choosing conditionally conjugate prior distributions for the jumps in $\boldsymbol{\mu}_{1:T}$ and $\boldsymbol{\Lambda}_{1:T}$, we arrive at simple closed form posterior distributions for the location of $\gamma$. In each model we parameterize $\boldsymbol{\Lambda}_t$ so that it can vary over $t$, even in the absence of a change-point. This general construction will prove to be useful in Section \ref{sec:mich} when we seek to model multiple change-points.

\subsection{Single Mean Change-Point (Mean-CP)}
\label{sec:smcp}

Suppose that we have $T$ observations from (\ref{eq:dgp}) where the sequence of positive definite precision matrices $\boldsymbol{\Lambda}_{1:T}$ is known and $\boldsymbol{\mu}_{1:T}$ is a random sequence with an underlying piece-wise constant structure generated by the following mean change-point (mean-cp) model:
\begin{align} \label{eq:smcp-start}
    \boldsymbol{\mu}_t &= \mathbf{b}\mathbbm{1}{\left\{t\geq \gamma \right\}} \\
    \mathbf{b} &\sim \mathcal{N}_d(\mathbf{0},\lambda_0^{-1} \mathbf{I}_d) \\
    \gamma &\sim \text{Categorical}(\boldsymbol{\pi}_{1:T})  \\
    \gamma &\indep \mathbf{b} 
    \label{eq:smcp-end}
\end{align}
where $\boldsymbol{\pi}_{1:T} \in \mathcal{S}^T $ and $\tau_0 > 0$ are known constants. In words, each of the coordinates of $\mathbf{y}_{t}$ start centered at zero and jump by $\mathbf{b}\in\mathbb{R}^d$ at some time $\gamma \in [T]$. For each $t \in [T]$, the posterior distribution $p(\mathbf{b}, \gamma \:|\: \mathbf{y}_{1:T})$ for the mean-cp model is given by:
\begin{align}
    \mathbf{b} \:|\: \gamma = t, \: \mathbf{y}_{1:T} &\sim \mathcal{N}_d\left(\overline{\mathbf{b}}_{t}, \overline{\boldsymbol{\Lambda}}_{t}^{-1}\right) \label{eq:b-smcp} \\
    \gamma \:|\: \mathbf{y}_{1:T} &\sim \text{Categorical}(\overline{\boldsymbol{\pi}}_{1:T}) \label{eq:gamma-smcp} \\
    \overline{\boldsymbol{\Lambda}}_t &= \lambda_0\mathbf{I}_d + \sum_{t'=t}^{T} \boldsymbol{\Lambda}_{t'} \\
    \overline{\mathbf{b}}_t &=  \overline{\boldsymbol{\Lambda}}_{t}^{-1}\sum_{t'=t}^{T} \boldsymbol{\Lambda}_{t'} \mathbf{y}_{t'} \\
    \overline{\pi}_t &\propto \pi_t|\overline{\boldsymbol{\Lambda}}_t|^{-
    \frac{1}{2}}\exp\left[\frac{1}{2}\lVert \overline{\boldsymbol{\Lambda}}^{\frac{1}{2}}_t \overline{\mathbf{b}}_t\rVert_2^2\right].
\end{align}
Even though $\mathbf{b}$ and $\gamma$ are generated independently, we see from the joint posterior that these parameters can exhibit an arbitrary degree of dependence after conditioning on $\mathbf{y}_{1:T}$. We also note that when $d = 1$, the model described in (\ref{eq:smcp-start})-(\ref{eq:smcp-end}) is identical to the SER model introduced in \cite{Wang20} when the covariate matrix $\mathbf{X}$ is lower-triangular with the non-zero entries equal to one. Motivated by this connection, we define a function \texttt{mean-cp} that is analogous to the \texttt{SER} function in \cite{Wang20}. \texttt{mean-cp} takes $\mathbf{y}_{1:T}$ and the model parameters as inputs and returns the posterior parameters of $p(\mathbf{b}, \gamma\:|\:\mathbf{y}_{1:T})$ as its output: 
\begin{align}\label{eq:smcp}
    \texttt{mean-cp}\left(\mathbf{y}_{1:T} \:;\: \boldsymbol{\Lambda}_{1:T}, \lambda_0, \boldsymbol{\pi}_{1:T}\right) := \{\overline{\mathbf{b}}_t, \overline{\boldsymbol{\Lambda}}_t, \overline{\pi}_t\}_{t=1}^T.
\end{align}
When appropriate, we will also use the notation $\{\mathbf{b},\gamma\} \sim \text{mean-cp}(\{\overline{\mathbf{b}}_t, \overline{\boldsymbol{\Lambda}}_t, \overline{\pi}_t\}_{t=1}^T)$ to mean that $\mathbf{b}$ and $\gamma$ follow the distribution specified in (\ref{eq:b-smcp})-(\ref{eq:gamma-smcp}).

\subsection{Single Covariance Change-Point (Cov-CP)}
\label{sec:sscp}

The covariance change-point model in this section is a multivariate generalization of the univariate model introduced in \cite{Cappello22}. Again, we have $T$ observations from (\ref{eq:dgp}), but now we assume that $\boldsymbol{\mu}_{1:T} \equiv \mathbf{0}$ and that the sequence of precision matrices $\boldsymbol{\Lambda}_{1:T}$ is random with an underlying piece-wise constant structure generated by the following covariance change-point (cov-cp) model:
\begin{align}\label{eq:sscp-start}
    \boldsymbol{\Lambda}_t &= 
    \begin{cases}
        \boldsymbol{\Psi}_{t}, & t < \gamma  \\
        \boldsymbol{\Psi}^{\frac{1}{2}}_{t}\mathbf{S}\boldsymbol{\Psi}^{\frac{1}{2}}_{t}, & t \geq \gamma
    \end{cases}
    \\
    \mathbf{S} &\sim \text{Wishart}(u_0, v_0^{-1}\mathbf{I}_d) \\
    \gamma &\sim \text{Categorical}(\boldsymbol{\pi}_{1:T}) \\
    \gamma &\indep \mathbf{S}
    \label{eq:sscp-end}
\end{align}
where again $\boldsymbol{\pi}_{1:T} \in \mathcal{S}^T$, $u_0 > d - 1$, $v_0 > 0$, and $\{\boldsymbol{\Psi}_{t}\}_{t=1}^T$ is a known sequence of positive definite matrices. In words, the model in (\ref{eq:sscp-start})-(\ref{eq:sscp-end}) independently draws the location of the change-point $\gamma$ and the precision matrix $\mathbf{S}$, then rescales the series $\mathbf{y}_t$ by $\mathbf{S}$ if $t \geq \gamma$. The posterior distribution $p(\mathbf{S}, \gamma\:|\:\mathbf{y}_{1:T})$ for this model is given by:
\begin{align}
    \mathbf{S} \:|\: \gamma = t, \: \mathbf{y}_{1:T} &\sim \text{Wishart}\left(\overline{u}_{t}, \overline{\mathbf{V}}_{t}\right) \label{eq:s-sscp} \\
    \gamma \:|\: \mathbf{y}_{1:T}  &\sim \text{Categorical}(\overline{\boldsymbol{\pi}}_{1:T}) \label{eq:gamma-sscp}\\
    \overline{u}_{t} &= u_0 + T - t + 1 \\
    \overline{\mathbf{V}}_{t} &= \left[v_0\mathbf{I}_d +  \sum_{t'=t}^{T} \boldsymbol{\Psi}^{\frac{1}{2}}_{t'}\mathbf{y}_{t'}\mathbf{y}^\intercal_{t'}\boldsymbol{\Psi}^{\frac{1}{2}}_{t'}\right]^{-1} \label{eq:v-sscp} \\
    \overline{\pi}_t &\propto \pi_t \left(2^d |\overline{\mathbf{V}}_{t}|\right)^{\frac{\overline{u}_{t}}{2}}\Gamma_d\left(\frac{\overline{u}_{t}}{2}\right)\exp\left(- \frac{1}{2}\sum_{t'=1}^{t-1} \lVert\boldsymbol{\Psi}_{t'}^{\frac{1}{2}}\mathbf{y}_{t'}\rVert_2^2\right) \label{eq:pi-sscp}
\end{align}
where $\Gamma_d$ denotes the $d$-dimensional multivariate gamma function. The summation in (\ref{eq:pi-sscp}) may be ill-defined if $t = 1$, so here and throughout the rest of this article we use the convention that a sum is equal to zero if the indexing set is empty. We again define a function \texttt{cov-cp} that takes the response $\mathbf{y}_{1:T}$ and the model parameters as inputs and returns the approximate posterior parameters of $p(\mathbf{S}, \gamma\:|\:\mathbf{y}_{1:T})$ as its output: 
\begin{align}
    \texttt{cov-cp}\left(\mathbf{y}_{1:T} \:;\: \boldsymbol{\Psi}_{1:T}, u_0, v_0, \boldsymbol{\pi}_{1:T}\right) := \{\overline{u}_t, \overline{\mathbf{V}}_t, \overline{\pi}_t\}_{t=1}^T.
\end{align}
As with the mean-cp model, we introduce a shorthand for the distribution (\ref{eq:s-sscp})-(\ref{eq:gamma-sscp}): 
\begin{align}
    \{\mathbf{S},\gamma\} \sim \text{cov-cp}(\{\overline{u}_t, \overline{\mathbf{V}}_t, \overline{\pi}_t\}_{t=1}^T).
\end{align}
While the model in (\ref{eq:sscp-start})-(\ref{eq:sscp-end}) gives rise to a closed form posterior, it is not very useful for our ultimate task of constructing multiple change-points in Section \ref{sec:mich}. We introduce an alternative construction for $\boldsymbol{\Lambda}_t$ that is better suited for fitting multiple covariance change-points by replacing (\ref{eq:sscp-start}) with: 
\begin{align}\label{eq:sscp-hadamard}
    \boldsymbol{\Lambda}_t &= 
    \begin{cases}
        \boldsymbol{\Psi}_{t}, & t < \gamma  \\
        \boldsymbol{\Psi}_{t} \odot \mathbf{S}, & t \geq \gamma
    \end{cases}
\end{align}
Because the Hadamard product of positive definite matrices is positive definite, the precision matrix in (\ref{eq:sscp-hadamard}) is well-defined, and when $d=1$, there is no difference between (\ref{eq:sscp-start}) and (\ref{eq:sscp-hadamard}). Unfortunately, for $d >1$ this new construction of the cov-cp model does not admit a closed form posterior, but we can construct a closed form approximation to $p(\mathbf{S}, \gamma\:|\:\mathbf{y}_{1:T})$. Consider the variational problem:
\begin{align}\label{eq:sscp-kl}
    \max_{q} -\text{KL}( q(\mathbf{S}, \gamma) \:\lVert\: p(\mathbf{S}, \gamma\:|\:\mathbf{y}_{1:T})).
\end{align}
If we knew the posterior distribution, then the solution to (\ref{eq:sscp-kl}) would be to set $q \equiv p(\mathbf{S}, \gamma\:|\:\mathbf{y}_{1:T})$. Without a known form for the posterior we can use the fact that:
\scriptsize
\begin{align}
    -\text{KL}( q(\mathbf{S}, \gamma) \:\lVert\: p(\mathbf{S}, \gamma\:|\:\mathbf{y}_{1:T})) &= \sum_{t=1}^T\int q(\mathbf{S},\gamma) \log \frac{p(\mathbf{y}_{1:T}\:|\: \mathbf{S},\gamma=t; \boldsymbol{\Psi_{1:T}}) p(\mathbf{S})\Pr(\gamma=t) }{q(\mathbf{S},\gamma) } \;d \mathbf{S} +C \\
    &=  \sum_{t=1}^T\int \frac{q(\mathbf{S},\gamma) }{2}\left[\sum_{t'=1}^{t-1}\log|\boldsymbol{\Psi}_t| + \sum_{t'=t}^T\log|\boldsymbol{\Psi}_t \odot \mathbf{S}| - \sum_{t'=1}^{t-1} \lVert\boldsymbol{\Psi}_t^{\frac{1}{2}}\mathbf{y}_{t'}\rVert_2^2 +\sum_{t'=t}^T \lVert(\mathcal{S}\odot\boldsymbol{\Psi}_t)^{\frac{1}{2}}\mathbf{y}_{t'}\rVert_2^2  \right] \notag \\
    &\quad + \sum_{t=1}^T\int q(\mathbf{S},\gamma) \log \frac{p(\mathbf{S})\Pr(\gamma=t) }{q(\mathbf{S},\gamma) } \;d \mathbf{S} +C \\
    &\geq  \sum_{t=1}^T\int \frac{q(\mathbf{S},\gamma) }{2}\left[\sum_{t'=1}^T\log|\boldsymbol{\Psi}_t| + (T-t+1)\log| \mathbf{S}| - \sum_{t'=1}^{t-1} \lVert\boldsymbol{\Psi}_t^{\frac{1}{2}}\mathbf{y}_{t'}\rVert_2^2 +\sum_{t'=t}^T \lVert(\mathcal{S}\odot\boldsymbol{\Psi}_t)^{\frac{1}{2}}\mathbf{y}_{t'}\rVert_2^2  \right] \notag \\
    &\quad + \sum_{t=1}^T\int q(\mathbf{S},\gamma) \log \frac{p(\mathbf{S})\Pr(\gamma=t) }{q(\mathbf{S},\gamma) } \;d \mathbf{S} +C \label{eq:sscp-approx}
\end{align}
\normalsize
The inequality in (\ref{eq:sscp-approx}) follows from the fact that $|A \odot B| \geq |A| |B|$ when $A$ and $B$ are positive definite. The lower bound in (\ref{eq:sscp-approx}) is maximized by picking $q$ so that $\{\mathbf{S},\gamma\} \sim \text{cov-cp}(\{\overline{u}_t, \overline{\mathbf{V}}_t, \overline{\pi}_t\}_{t=1}^T)$ with following the modification to (\ref{eq:v-sscp}):
\begin{align}
    \overline{\mathbf{V}}_t := \left[v_0\mathbf{I}_d +  \sum_{t'=t}^{T} \mathbf{y}_{t'}\mathbf{y}^\intercal_{t'} \odot \boldsymbol{\Psi}_{t'}\right]^{-1}
\end{align}
We can therefore treat this distribution as an approximation to the cov-cp posterior under the construction in (\ref{eq:sscp-hadamard}). If we make the additional restriction: 
\begin{align}
    \mathbf{S} &= \text{diag}(\{s_i\}_{i=1}^d) \\
    s_i &\overset{\text{ind.}}{\sim} \text{Gamma}\left(\frac{u_{i0}}{2}, \frac{v_{i0}}{2}\right)
\end{align}
then the constructions of $\boldsymbol{\Lambda}_t$ in (\ref{eq:sscp-start}) and (\ref{eq:sscp-hadamard}) agree and:
\begin{align}
    s_i \:|\: \gamma = t, \: \mathbf{y}_{1:T} &\overset{\text{ind.}}{\sim} \text{Gamma}\left(\frac{\overline{u}_{it}}{2}, \frac{\overline{v}_{it}}{2}\right) \label{eq:ind-s-start} \\
    \gamma \:|\: \mathbf{y}_{1:T}  &\sim \text{Categorical}(\overline{\boldsymbol{\pi}}_{1:T}) \\
    \overline{u}_{it} &= u_{i0} + T - t + 1 \\
    \overline{v}_{it} &= v_{i0} +  \sum_{t'=t}^{T} [\boldsymbol{\Psi}_t]_{ii} y^2_{it} \\
    \overline{\pi}_t &\propto \pi_t \prod_{i=1}^d\Gamma\left(\frac{\overline{u}_{it}}{2}\right)\left(\frac{\overline{v}_{it}}{2}\right)^{-\frac{\overline{u}_{it}}{2}}\exp\left(- \frac{1}{2}\sum_{t'=1}^{t-1} \lVert\boldsymbol{\Psi}_{t'}^{\frac{1}{2}}\mathbf{y}_{t'}\rVert_2^2\right) \label{eq:ind-s-end}
\end{align}
We refer to the construction in (\ref{eq:ind-s-start})-(\ref{eq:ind-s-end}) as $\{\mathbf{S},\gamma\} \sim \text{ind-cov-cp}(\{\overline{\mathbf{u}}_{t}, \overline{\mathbf{v}}_{t}, \overline{\pi}_t\}_{t=1}^T)$.

\subsection{Single Mean-Covariance Change-Point (MeanCov-CP)}
\label{sec:smscp}

We now merge the mean- and cov-cp settings by allowing $\mathbf{y}_{1:T}$ to have a single change-point where both the mean and covariance shift simultaneously. We again assume that we have $T$ observations from (\ref{eq:dgp}), only now the concurrent change to the piece-wise constant structure of $\boldsymbol{\mu}_{1:T}$ and $\boldsymbol{\Lambda}_{1:T}$ is generated by the following Single Mean-Covariance Change-Point (meancov-cp) model:
\begin{align}
    \label{eq:smscp-start}
    \boldsymbol{\mu}_t &= \mathbf{b}\mathbbm{1}{\left\{t\geq \gamma \right\}} \\
    \boldsymbol{\Lambda}_t &= 
    \begin{cases}
        \boldsymbol{\Psi}_{0}, & t < \gamma  \\
        \boldsymbol{\Psi}^\frac{1} {2}_{0}\mathbf{S}\boldsymbol{\Psi}^\frac{1}{2}_{0}, & t \geq \gamma
    \end{cases} \label{eq:psi_0}
    \\
    \mathbf{b} \:|\: \mathbf{S} &\sim \mathcal{N}_d\left(\mathbf{0},\left[\lambda_0\boldsymbol{\Psi}^\frac{1}{2}_{0}\mathbf{S}\boldsymbol{\Psi}^\frac{1}{2}_{0}\right]^{-1}\right) \\
    \mathbf{S} &\sim \text{Wishart}(u_0, v_0^{-1}\mathbf{I}_d) \\    
    \gamma &\sim \text{Categorical}(\boldsymbol{\pi}_{1:T}) \\
    \gamma &\indep \{\mathbf{b},\mathbf{S}\}. 
    \label{eq:smscp-end}
\end{align}
Note that the construction of $\boldsymbol{\Lambda}_t$ in (\ref{eq:psi_0}) differs from (\ref{eq:sscp-start}) in the cov-cp model in that $\boldsymbol{\Psi}_t = \boldsymbol{\Psi}_0$ for all $t$. If the parameter $\boldsymbol{\Psi}_t$ is allowed to vary over time, then we lose conjugacy in the meancov-cp model. The posterior distribution for $p(\mathbf{b}, \mathbf{S}, \gamma \:|\: \mathbf{y}_{1:T})$ under the meancov-cp setting is given by:
\begin{align}
    \mathbf{b} \:|\: \mathbf{S}, \gamma = t, \mathbf{y}_{1:T} &\sim \mathcal{N}_d\left(\overline{\mathbf{b}}_t, \left[\overline{\boldsymbol{\Psi}}^{\frac{1}{2}}_t \mathbf{S} \overline{\boldsymbol{\Psi}}^{\frac{1}{2}}_t\right]^{-1} \right) \label{eq:b-smscp} \\
    \mathbf{S} \:|\: \gamma = t, \mathbf{y}_{1:T} &\sim \text{Wishart}(\overline{u}_t, \overline{\mathbf{V}}_t)  \\
    \gamma \:|\: \mathbf{y}_{1:T} &\sim \text{Categorical}(\overline{\boldsymbol{\pi}}_{1:T}) \label{eq:gamma-smscp} \\
    \overline{\boldsymbol{\Psi}}_t &= (\lambda_0 + T-t+1)\boldsymbol{\Psi}_0 \\
    \overline{\mathbf{b}}_t &= \overline{\boldsymbol{\Psi}}_t^{-1}\sum_{t'=t}^{T} \boldsymbol{\Psi}_0\mathbf{y}_{t'} \\
    \overline{u}_t &= u_0 + T - t + 1 \\
    \overline{\mathbf{V}}_t &= v_0\mathbf{I}_d - \left\lVert\overline{\boldsymbol{\Psi}}^{\frac{1}{2}}_t\overline{\mathbf{b}}_t\right\rVert_2^2 +  \sum_{t'=t}^{T} \boldsymbol{\Psi}_0^{\frac{1}{2}}\mathbf{y}_{t'} \mathbf{y}_{t'}^\intercal\boldsymbol{\Psi}_0^{\frac{1}{2}}\\
    \overline{\pi}_t &\propto \pi_t |\overline{\boldsymbol{\Psi}}_t |^{-\frac{1}{2}} \left(2^d |\overline{\mathbf{V}}_{t}|\right)^{\frac{\overline{u}_{t}}{2}}\Gamma_d\left(\frac{\overline{u}_{t}}{2}\right)\exp\left(- \frac{1}{2}\sum_{t'=1}^{t-1} \lVert\boldsymbol{\Psi}_0^{\frac{1}{2}}\mathbf{y}_{t'}\rVert_2^2\right).
\end{align}
As was the case with the cov-cp model, the meancov-cp model loses its utility in the multiple change-point setting. However, if we are again willing to make the assumption that  therefore, we We again define a function \texttt{meancov-cp} that takes $\mathbf{y}_{1:T}$ and the model parameters as inputs and returns the posterior parameters of $p(\mathbf{b}, \mathbf{S}, \gamma\:|\:\mathbf{y}_{1:T})$ as its output:
\begin{align}
    \texttt{meancov-cp}\left(\mathbf{y}_{1:T} \:;\: \lambda_0, \boldsymbol{\Psi}_0, u_0, v_0, \boldsymbol{\pi}_{1:T}\right) := \{\overline{\mathbf{b}}_t, \overline{\boldsymbol{\Psi}}_t, \overline{u}_t, \overline{\mathbf{V}}_t, \overline{\pi}_t\}_{t=1}^T
\end{align}
as well as a shorthand for the distribution (\ref{eq:b-smscp})-(\ref{eq:gamma-smscp})
\begin{align}
    \{\mathbf{b}, \mathbf{S}, \gamma\}\sim\text{meancov-cp}(\{\overline{\mathbf{b}}_t, \overline{\boldsymbol{\Psi}}_t, \overline{u}_t, \overline{\mathbf{V}}_t, \overline{\pi}_t\}_{t=1}^T).
\end{align}
As was the case with the cov-cp model, the conjugate meancov-cp model does not translate nicely to the multiple change-point setting, so we again replace $\boldsymbol{\Lambda}_t$ in (\ref{eq:psi_0}) with the same construction as in (\ref{eq:sscp-hadamard}). Using the same approximating technique from Section \ref{sec:sscp} we get an approximate posterior where $\{\mathbf{b}, \mathbf{S}, \gamma\}\sim\text{meancov-cp}(\{\overline{\mathbf{b}}_t, \overline{\boldsymbol{\Psi}}_t, \overline{u}_t, \overline{\mathbf{V}}_t, \overline{\pi}_t\}_{t=1}^T)$ with:



\subsection{Localization Rates}
\label{sec:localization}

Suppose that $t_0 \in [T]$ is the true location of the change-point for one of the SCP models. Then given $\overline{\boldsymbol{\pi}}_{1:T}$ from any of the SCP models, a natural estimator for $t_0$ is the posterior most probable location of $\gamma$, i.e. the maximum \textit{a posteriori} (MAP) estimator:
\begin{align}\label{eq:map}
    \hat{t}_{\text{MAP}} := \argmax{1 \leq t \leq T} \; \overline{\pi}_t.
\end{align}
In this section, we supplement the Bayesian perspective of the SCP models by studying the asymptotic behavior of $\hat{t}_{\text{MAP}}$. In particular, we show that under mild regularity conditions, $\hat{t}_{\text{MAP}}$ is consistent in the sense that there exists some error bound $\epsilon_T$, where:
\begin{align}
    \lim_{T\to\infty} \Pr\left(|t_0 - \hat{t}_{\text{MAP}}| \leq \epsilon_T\right) &= 1 \label{def:consistency} \\
    \lim_{T\to\infty} \frac{\epsilon_T}{T} &= 0. \label{def:loc-rate}
\end{align}
Throughout this article we refer to $\epsilon_T$ as the \textit{localization rate}. Our aim is to find the smallest localization rate that guarantees (\ref{def:consistency}) while satisfying (\ref{def:loc-rate}). We begin by making the following assumption:
\begin{assumption}\label{assumption:1}
    Let $t_0 \in [T]$ be the time such that $y_t \sim F_0$ for $t < t_0$ and $y_t \sim F_1$ for $t_0 \geq t$. Assume that: \vspace{-10pt}
    \begin{enumerate}[label=(\roman*)]
        \item (Minimum Spacing) $\min\{t_0,T-t_0 + 1\} > \log^{1+\varepsilon} T$ for some $\varepsilon > 0$. 
        \item (Proper Prior) The hyper-parameters in the SCP models are chosen so that $\lambda_0, u_0, v_0 > 0$.
        \item (Bounded Prior) The prior distribution for the change-point location, $\pi_t := \Pr(\gamma = t \:; \boldsymbol{\pi}_{1:T})$, is chosen so that for each $t\in[T]$, $\log \frac{\pi_{t_0}}{\pi_{t}}$ is of order: a) $\mathcal{O}(\log T)$, b) $\mathcal{O}(\sqrt{T\log T})$, or c) $\mathcal{O}(\sqrt{T}\log T)$.
    \end{enumerate}
\end{assumption}
\vspace{-10pt}

In the context of multiple CPD, Assumption \ref{assumption:1} (i) can be interpreted as a minimum spacing condition, i.e. two consecutive change-points must be separated by an interval of minimum length $\log^{1+\varepsilon} T$. As per Table 1 in \cite{Cho15}, this appears to be smallest minimum spacing condition under which other state-of-the-art methods can consistently recover $t_0$. Assumption \ref{assumption:1} (ii) simply requires that the parameters in the SCP models are chosen so that the priors are non-degenerate, which will ensure the posteriors are proper. Lastly, Assumption \ref{assumption:1} (iii) ensures that our choice of $\boldsymbol{\pi}_{1:T}$ does not end up overwhelming the evidence in the data. Note that this assumption is trivially satisfied if we set $\pi_t = T^{-1}$, i.e. each $t \in [T]$ is equally likely to be the location of the change-point \textit{a priori}. We leave a more detailed discussion of the choice of $\boldsymbol{\pi}_{1:T}$ for Appendix \ref{app:prior}. Now suppose that $\boldsymbol{\mu}_{1:T}$ jumps by $\mathbf{b}_0\in\mathbb{R}^d$ at time $t_0$, then the conditions in Assumption \ref{assumption:1} will be sufficient for our models to detect this change provided that the aggregated jump-size as measured by $\lVert \mathbf{b}_0 \rVert_2$ is large enough. Assumption \ref{assumption:mean} formalizes this condition:

\begin{assumption}[Detectable Mean]\label{assumption:mean}  
    Suppose $\E[\mathbf{y}_t] = \mathbf{b}_0\mathbbm{1}\{t \geq t_0\}$ for some $t_0 \in [T]$. Assume that $\lVert \mathbf{b}_0 \rVert^2_2 \gtrsim d$.
\end{assumption}
\vspace{-10pt}

\begin{remark}\label{rmk:vanishing-signal}
    When $\normalfont{\Var}(\mathbf{y}_t) = \boldsymbol{\Lambda}^{-1}$ for all $t \in [T]$, we can weaken Assumption \ref{assumption:mean} to $\lVert \boldsymbol{\Lambda}^{\frac{1}{2}} \mathbf{b}_0\rVert_2^2 \gtrsim d$, where $\lVert \boldsymbol{\Lambda}^{\frac{1}{2}} \mathbf{b}_0\rVert_2^2$ is the multivariate signal to noise ratio. This condition is implied in Theorem \ref{theorem:smcp} since $\lVert \boldsymbol{\Lambda}^{\frac{1}{2}} \mathbf{b}_0\rVert_2^2 \geq \lambda_{\min} \lVert \mathbf{b}_0\rVert_2^2$, where $\lambda_{\min}$ is the smallest eigenvalue of $\boldsymbol{\Lambda}$. Additionally, if Assumption \ref{assumption:1} (i) holds for $\varepsilon > 0$, then we can weaken Assumption \ref{assumption:mean} further to to $\lVert \mathbf{b}_0 \rVert^2_2 \gtrsim d\log^{-\xi} T$ for some $\xi < \min\{\varepsilon, 1/2\}$. For fixed $d$, this weaker assumption permits the signal $\mathbf{b}_0$ to vanish at the rate $\log^{\xi} T$.
\end{remark}
\vspace{-10pt}

When $d$ grows with $T$, Assumption \ref{assumption:mean} implies a non-sparse condition on $\mathbf{b}_0$. As an example, if some proportion $\alpha \in (0,1)$ of the elements of $\mathbf{b}_0$ are equal to $b_0 > 0$ and the rest are equal to zero, then the Assumption \ref{assumption:mean} is met since $(b^2_0\alpha)^{-1/2}\lVert \mathbf{b}_0 \rVert_2 = \sqrt{d}$. Assumptions of this form are standard in the high-dimensional CPD for methods that depend on $\ell_2$-based aggregation of the signal (see e.g. \citealp{Bai10, Horv√°th12, Li23}). The sparse signal setting is inherently challenging in high-dimensions; if $\lVert \mathbf{b}_0\rVert_1 = o(d)$, then as $d$ grows, we are adding more noise with each new series, potentially drowning out the signal (for more on high-dimensional CPD with a sparse signal, see \citealp{Cho15, Jirak15, Wang17, Yu21, Chen22}). On the other hand, if $d$ is fixed, then Assumption \ref{assumption:mean} simply implies $\mathbf{b}_0$ is bounded away from the origin.

\begin{theorem}[<ean-CP Localization Rate]\label{theorem:smcp}
    Let $\{\mathbf{y}_t\}_{t=1}^T$ be an independent, sub-Gaussian sequence with $\mathbf{y}_t \in \mathbb{R}^d$, $\normalfont{\Var}(\mathbf{y}_t) = \boldsymbol{\Lambda}^{-1}$ for all $t \in [T]$, and $\sup_{t \geq 1, d \geq 1} \lVert \mathbf{y}_t\rVert_{\psi_2} < \infty$. Let $\lambda_{\max}$ and $\lambda_{\min}$ are the largest and smallest eigenvalues of $\boldsymbol{\Lambda}$ respectively and assume that $\sup_{d\geq 1}  \lambda_{\max} < \infty$ and $\inf_{d\geq 1} \lambda_{\min} > 0$. Additionally, suppose that for some $\varepsilon >0$, Assumptions \ref{assumption:1} (i), (ii), (iii) (a) and Assumption \ref{assumption:mean} hold. Then there exists some $\kappa > 0$ such that:
    \vspace{-5pt}
    \begin{align*}
        \lim_{T\to\infty}\Pr\left(|t_0 - \hat{t}_{\normalfont \text{MAP}}| \leq \frac{\kappa \log T}{\lVert\boldsymbol{\Lambda}^{\frac{1}{2}}\mathbf{b}_0\rVert_2^2}\right) = 1  
    \end{align*}
    where $\hat{t}_{\normalfont \text{MAP}}$ is constructed as in (\ref{eq:map}) by fitting the mean-cp model in (\ref{eq:smcp-start})-(\ref{eq:smcp-end}). 
\end{theorem}
\vspace{-10pt}

\begin{remark}\label{rmk:sub-g}
    When $d$ is allowed to grow with $T$, the assumption that $\lVert \mathbf{y}_t\rVert_{\psi_2}$ is uniformly bounded restricts the strength of the dependence between the $d$ time-series. Two sufficient conditions under which this restriction holds include: i) each entry of $\mathbf{y}_t$ is independent and $\mathcal{SG}(\sigma)$ for some $\sigma < \infty$, and ii) $\mathbf{y}_t \sim \mathcal{N}_d(\boldsymbol{\mu}_t, \boldsymbol{\Lambda}^{-1})$ and $\inf_{d\geq 1}  \lambda_{\min} > 0$ as in Theorem \ref{theorem:smcp}. When $d$ is fixed, the coordinates of $\mathbf{y}_t$ are allowed to display arbitrary levels of dependence so long as they are sub-Gaussian. See Lemma \ref{lemma:thm1-event-bound} for more detail.
\end{remark}
\vspace{-10pt}

In light of Assumption \ref{assumption:mean}, we have $\frac{\log T}{d} \gtrsim \frac{\log T}{\lVert\boldsymbol{\Lambda}^{1/2}\mathbf{b}_0\rVert_2^2}$, so Theorem \ref{theorem:smcp} establishes the localization rate for the SMCP model is of order $d^{-1} \log T$. The quantity $\lVert \boldsymbol{\Lambda}^{\frac{1}{2}} \mathbf{b}_0\rVert_2^2$ is the multivariate signal to noise ratio, so by Lemma 2 of \cite{Wang2020_localization} this rate minimax optimal aside from a $\log T$ factor when $d$ is fixed. On the other hand, if $d$ is allowed to grow with $T$, then Theorem \ref{theorem:smcp} shows a phase change occurs when $d\gtrsim \log^{1+\varepsilon}T$ for some $\varepsilon > 0$. In this case, the localization rate is itself converging to zero and thus $\lim_{T\to\infty}\Pr(\hat{t}_{\normalfont \text{MAP}} = t_0) = 1$. Therefore, in-high dimension our model recovers the same consistency result as the least-squares estimator introduce by \cite{Bai10}. To our knowledge it is the first Bayesian method to do so.  

The precision matrix $\boldsymbol{\Lambda}$ is taken as known in Theorem \ref{theorem:smcp}. Corollary \ref{cor:lambda-hat} shows that under certain conditions on the growth of $d$ and $\lVert \mathbf{b}_0\rVert_0$, it is possible to construct a consistent estimator $\hat{\boldsymbol{\Lambda}}$ and use the normalized observations $\hat{\boldsymbol{\Lambda}}^{\frac{1}{2}}\mathbf{y}_t$ to localize $t_0$.

\begin{corollary}
    \label{cor:lambda-hat}
\end{corollary}

Before stating the result, we define a restricted MAP estimator for some $\alpha \in (0,1/2)$:
\begin{align}
    \label{eq:restricted-map}
        \hat{t}_{\alpha} := \argmax{t \::\: \min\{t, T-t\} \geq \alpha T} \; \overline{\pi}_t.
\end{align}
Such an estimator is reasonable in practice if we can confidently pick an $\alpha$ smaller than $c$ from Assumption \ref{assumption:1} (i). 


We cannot remove the independence assumption in Theorem \ref{theorem:smscp} without sacrificing the $\log T$ localization rate. In Theorems \ref{theorem:sscp} and \ref{theorem:smscp}, we turn our attention to characterizing the localization rate when there is a change in $\boldsymbol{\lambda}$ and examine the behavior of $\hat{t}_{\text{MAP}}$ under a more general dependence setting. In particular, when $\mathbf{y}$ is an $\alpha$-mixing process (see Definition \ref{def:alpha-mixing} of Appendix \ref{app:notation}), then under mild regularity conditions both the SSCP and SMCP models can identify a change in the variance (or the mean in the case of SMSCP) with a localization rate of order $\sqrt{T}\log^{1+\varepsilon} T$ for some $\varepsilon >0 $. This rate can be reduced to $\sqrt{T\log^{1+\varepsilon} T}$ if we are willing to assume that each $y_t$ is an independent draw from a Gaussian distribution as in (\ref{eq:dgp}). Assumption \ref{assumption:scale} formalizes the conditions under which our models can detect a change in the scale of $\mathbf{y}$ and achieve these localization rates:

\begin{assumption}[Detectable Scale]\label{assumption:scale}
   Suppose $\Var(y_t) = s_0^{\mathbbm{1}\{t\geq t_0\}}$ for some $t_0 \in [T]$. Assume that there exist some compact intervals $I_1 \subset (0, 1)$ and $I_2 \subset (1, \infty)$ such that $s_0^2 \in I_1 \cup I_2$. 
\end{assumption}

\begin{theorem}[SSCP Localization Rate]\label{theorem:sscp}
    Suppose that Assumption \ref{assumption:1} (i) and (ii) hold with $b_0=0$. Define $z_t$ as in (\ref{eq:normalized}) and assume that:
    \vspace{-10pt}
    \begin{enumerate}[label=(\roman*)]
        \item $\{y_t\}_{t=1}^T$ is an $\alpha$-mixing process with coefficients that satisfy $\alpha_k \leq e^{-Ck}$ for some $C > 0$.
        \item There exists constants $\delta, \; D > 0$ such that $\max_{1\leq t \leq T} \E\left[|z_t^2 - 1|^{2+\delta}\right]\leq D.$ 
    \end{enumerate}
    \vspace{-5pt}
    For any $\varepsilon > 0$ if Assumption $\ref{assumption:scale}$ holds along with Assumption $\ref{assumption:1}$ (iii) (c), then there exists some $\kappa > 0$ so that: 
    \vspace{-5pt}
    \begin{align*}
        \lim_{T\to\infty}\Pr\left(|t_0 - \hat{t}_{\normalfont \text{MAP}}| \leq \kappa \sqrt{T}\log^{1+\varepsilon} T\right) = 1  
    \end{align*}
    where $\hat{t}_{\normalfont \text{MAP}}$ is constructed as in (\ref{eq:map}) by fitting the SSCP model in (\ref{eq:sscp-start})-(\ref{eq:sscp-end}). Furthermore, if $\mathbf{y}$ is generated according to (\ref{eq:dgp}) and Assumption $\ref{assumption:1}$ (iii) (b) holds, then we have:
    \vspace{-5pt}
    \begin{align*}
        \lim_{T\to\infty}\Pr\left(|t_0 - \hat{t}_{\normalfont \text{MAP}}| \leq \kappa \sqrt{T\log T}\right) = 1.  
    \end{align*}
\end{theorem}

\begin{theorem}[SMSCP Localization Rate]\label{theorem:smscp}
    Suppose that Assumption \ref{assumption:1} (i) and (ii) hold. Define $z_t$ as in (\ref{eq:normalized}) and assume that:
    \vspace{-10pt}
    \begin{enumerate}[label=(\roman*)]
        \item $\{y_t\}_{t=1}^T$ is an $\alpha$-mixing process with coefficients that satisfy $\alpha_k \leq e^{-Ck}$ for some $C > 0$.
        \item There exists constants $\delta_1, \; D_1 > 0$ such that $\max_{1\leq t \leq T} \E\left[|z_t|^{2+\delta_1}\right]\leq D_1.$ 
        \item There exists constants $\delta_2, \; D_2 > 0$ such that $\max_{1\leq t \leq T} \E\left[|z_t^2 - 1|^{2+\delta_2}\right]\leq D_2.$ 
    \end{enumerate}
    \vspace{-5pt}
    For any $\varepsilon > 0$ if either Assumption $\ref{assumption:mean}$ holds for the constant $\xi / 2$ where $\xi \in [0,\varepsilon)$ or Assumption $\ref{assumption:scale}$ holds along with Assumption $\ref{assumption:1}$ (iii) (c), then there exists some $\kappa > 0$ so that: 
    \vspace{-5pt}
    \begin{align*}
        \lim_{T\to\infty}\Pr\left(|t_0 - \hat{t}_{\normalfont \text{MAP}}| \leq \kappa \sqrt{T}\log^{1+\varepsilon}\right) = 1  
    \end{align*}
    where $\hat{t}_{\normalfont \text{MAP}}$ is constructed as in (\ref{eq:map}) by fitting the SMSCP model in (\ref{eq:smscp-start})-(\ref{eq:smscp-end}). Furthermore, if $\mathbf{y}$ is generated according to (\ref{eq:dgp}) and Assumption $\ref{assumption:1}$ (iii) (b) holds, then we have:
        \vspace{-5pt}
    \begin{align*}
        \lim_{T\to\infty}\Pr\left(|t_0 - \hat{t}_{\normalfont \text{MAP}}| \leq \kappa \sqrt{T\log^{1+\xi} T}\right) = 1 .
    \end{align*}
\end{theorem}
Theorem \ref{theorem:sscp} is borrowed directly from \cite{Cappello22}, while Theorem \ref{theorem:smscp} is a new result whose proof is given in Appendix \ref{app:localization-smscp}. Note that in the statement of Theorem \ref{theorem:smscp}, it is possible that $\xi =0$ so that if we have independent Gaussian data and either $|b_0|$ or $|s^2_0-1|$ does not vanish as $T$ gets large, then the localization rate in Theorem \ref{theorem:smscp} reduces to $\kappa \sqrt{T\log T}$, which is precisely the rate achieved by the BSOP method of \cite{Wang21}. Theorem \ref{theorem:smscp} also shows that when we have independent Gaussian data and Assumption \ref{assumption:mean} holds with $s_0^2 = 1$, then the localization rate for $\hat{t}_{\text{MAP}}$ based on the SMSCP model will be a factor $\sqrt{T/\log T}$ larger than the localization rate based on the SMCP model. Therefore, even though the SMSCP model can identify a change in the mean of $\mathbf{y}$ even when the variance is constant, using SMCP model in this case will result in more rapid localization and tighter credible intervals around $t_0$. 

\subsection{Credible Sets}
\label{sec:cred-sets}

In addition to generating the point estimate $\hat{t}_\text{MAP}$, we can use the posterior probabilities $\overline{\boldsymbol{\pi}}_{1:T}$ returned by each of the SCP models to construct $\alpha$-level credible sets around $t_0$ by solving:\footnote{Problem (\ref{eq:cs}) is an example of the well-known knapsack problem, i.e. it is an integer program where we solve $\mathbf{z} = \text{arg }\min_{\mathbf{x}\in\{0,1\}^T} \; \langle\mathbf{1}, \mathbf{x}\rangle \text{ s.t. } \langle \overline{\boldsymbol{\pi}}_{1:T}, \mathbf{x}\rangle \geq \alpha$, then set $\mathcal{CS}(\alpha, \overline{\boldsymbol{\pi}}_{1:T})$ equal to the elements of $[T]$ that correspond to the elements of $\mathbf{z}$ that equal one.}
\begin{align}\label{eq:cs}
    \mathcal{CS}(\alpha, \overline{\boldsymbol{\pi}}_{1:T}) := \argmin{S \subset [T]} |S| \;\text{ s.t. } \sum_{t \in S} \overline{\pi}_t \geq \alpha.
\end{align}
In light of the theoretical results in the previous section, a credible set constructed as per (\ref{eq:cs}) will contain a point that is within the localization rate of the true change-point with high probability:
\begin{corollary} \label{cor:cred-sets}
Let $\epsilon_T$ be the localization rate corresponding to one of Theorems \ref{theorem:smcp}, \ref{theorem:sscp}, or \ref{theorem:smscp}, then under the conditions of the these theorems $\lim_{T \to \infty} \Pr\left(\min_{t\in \mathcal{CS}(\alpha, \overline{\boldsymbol{\pi}}_{1:T})} \; |t - t_0| \leq \epsilon_T \right) = 1.$
\end{corollary}

\cite{Cappello22} showed that in models where there is no actual change-point, credible sets constructed according to (\ref{eq:cs}) tend to encompass large portions of the series since the elements $\overline{\boldsymbol{\pi}}_{1:T}$ tend to be quite diffuse. This observation motivated \cite{Cappello22} to adopt the heuristic of classifying a change-point as detected only if $|\mathcal{CS}(\alpha,\overline{\boldsymbol{\pi}}_{1:T})| \leq T/2$, a convention which we also adopt.