\subsection{Proof of Proposition \ref{prop:coord-ascent}}
\label{app:prop1-proof}

First, we can see that Algorithm \ref{alg:mich} begins by initializing some values $\overline{\boldsymbol{\Theta}}$ that parameterize a distribution $q\in\mathcal{Q}_{\text{MF}}$. Then, Proposition \ref{prop:vb} below shows that the update step for $\overline{\boldsymbol{\theta}}_i$ in Algorithm \ref{alg:mich} is equivalent to solving $\max_{q_i} \text{\normalfont ELBO}\left(q\:;\:\mu_0,\lambda_0\right)$ while holding all the other component distributions of $q$ fixed. Therefore, Algorithm \ref{alg:mich} is a coordinate ascent procedure for maximizing $\text{\normalfont ELBO}\left(q\:;\:\mu_0,\lambda_0\right)$. Proposition \ref{prop:vb} also shows that the $\overline{\boldsymbol{\theta}}_i$ that parameterizes the solution to $\text{arg}\max_{q_i} \text{\normalfont ELBO}\left(q\:;\:\mu_0,\lambda_0\right)$ is the unique maximizer of some continuously differentiable objective function $F(\overline{\boldsymbol{\Theta}})$ when $\overline{\boldsymbol{\Theta}}\setminus\overline{\boldsymbol{\theta}}_i$ is held fixed. Then by Proposition 2.7.1 of \cite{Bertsekas97}, the sequence of parameters $\{\overline{\boldsymbol{\Theta}}_{n}\}_{n\geq 1}$ returned by Algorithm \ref{alg:mich} is converging to some limit point. Because the distribution $q_{\text{mich}}$ returned by Algorithm \ref{alg:mich} is fully characterized by $\overline{\boldsymbol{\Theta}}$, the sequence of distributions $\{q_{n}\}_{\n \geq 1}$ returned by Algorithm \ref{alg:mich} are converging to a stationary point of $\text{\normalfont ELBO}\left(q\:;\:\mu_0,\lambda_0\right)$. Therefore, Proposition \ref{prop:coord-ascent} directly follows from Proposition \ref{prop:vb}.

\begin{proposition} 
\label{prop:vb}
Given the distributions $\{q_\ell\}_{\ell=1}^L$, $\{q_k\}_{k=1}^K$, and $\{q_j\}_{j=1}^J$, define $q$ as in (\ref{eq:mean-field}). Define the residual $\tilde{r}_t$, expected precision $\overline{\lambda}_t$ and variance correction term $\delta_t$ as in (\ref{eq:mod-resid}), (\ref{eq:lambda-bar}), and (\ref{eq:delta}).
\vspace{-10pt}

\begin{enumerate}[label=\normalfont(\roman*)]
    \itemsep0em 
    \item Define the partial mean residual $\tilde{r}_{-\ell t} :=  \tilde{r}_{t} + \E_{q_{\ell}}[\mu_{\ell t}]$. Then $\text{\normalfont{arg}}\max_{q_\ell} \text{\normalfont ELBO}\left(q\:;\:\mu_0,\lambda_0\right)$ is equivalent to the mean-scp posterior in (\ref{eq:gamma-post-cat1}) and (\ref{eq:b-smcp}) with parameters $\overline{\boldsymbol{\theta}}_\ell = \normalfont{\texttt{mean-scp}}(\tilde{\mathbf{r}}_{-\ell,1:T} \:;\: \overline{\boldsymbol{\lambda}}_{1:T}, \omega_{\ell}, \boldsymbol{\pi}_{\ell,1:T}).$ 
    
    \item Define the partial scale residual $\overline{\lambda}_{-kt} := \E_{q_k}[\lambda_{k t}]^{-1}\overline{\lambda}_t$ and the variance corrected priors $\tilde{v}_{kt}$ and $\tilde{\pi}_{kt}$ as in (\ref{eq:v-k-corrected}) and (\ref{eq:pi-k-corrected}). Then $\text{\normalfont{arg}}\max_{q_k} \text{\normalfont ELBO}\left(q\:;\:\mu_0,\lambda_0\right)$ is equivalent to the var-scp posterior in (\ref{eq:gamma-post-cat1}) and (\ref{eq:s-sscp}) with parameters $\overline{\boldsymbol{\theta}}_k = \normalfont{\texttt{var-scp}}(\tilde{\mathbf{r}}_{1:T} \:;\:\overline{\boldsymbol{\lambda}}_{-k,1:T}, u_k, \tilde{\mathbf{v}}_{k,1:T}, \tilde{\boldsymbol{\pi}}_{k,1:T}).$
    
    \item Define the partial mean residual $\tilde{r}_{-jt} := \tilde{r}_{t} +\E_{q_j}[\lambda_{jt}]^{-1} \E_{q_j}[\lambda_{jt} \mu_{j t}]$, the partial scale residual $\overline{\lambda}_{-jt} := \E_{q_j}[\lambda_{j t}]^{-1}\overline{\lambda}_t$, the partial correction term $\delta_{-jt}$ as per (\ref{eq:delta-j}), and the variance corrected priors $\tilde{v}_{jt}$ and $\tilde{\pi}_{jt}$ as in (\ref{eq:v-j-corrected}) and (\ref{eq:pi-j-corrected}). Then $\text{\normalfont{arg}}\max_{q_j} \text{\normalfont ELBO}\left(q\:;\:\mu_0,\lambda_0\right)$ is equivalent to the meanvar-scp posterior in (\ref{eq:gamma-post-cat1}) and (\ref{eq:bs-smscp}) with parameters $\overline{\boldsymbol{\theta}}_j := \normalfont{\texttt{meanvar-scp}}(\tilde{\mathbf{r}}_{-j,1:T} \:;\:\overline{\boldsymbol{\lambda}}_{-j,1:T}, \omega_j, u_j, \tilde{\mathbf{v}}_{j,1:T}, \tilde{\boldsymbol{\pi}}_{j,1:T}).$
\end{enumerate}
\end{proposition}
\vspace{-10pt}
%The proof of Proposition \ref{prop:vb} is given in Appendix \ref{app:prop1-proof}. Backfitting procedures for models with additive effects typically work with the expected residual $\E_q[r_t]$ (\citealp{Breiman85,Hastie90,Friedman00,Wang20}). Algorithm \ref{alg:mich} on the other hand uses the modified residual $\tilde{r}_t$ to fit the individual SCP models, which is necessitated by the inclusion of both mean and variance changes in the MICH model. Similarly, the variance correction term $\delta_t$ plays an important role in Algorithm \ref{alg:mich} that did not previously appear in either the IBSS or ProSCALE algorithms. The variance of $\mathbf{y}_{1:T}$ is inflated by the uncertainty around each mean change, and Proposition \ref{prop:vb} shows how to use $\delta_t$ to account for the added noise from $\mu_{\ell t}$ and $\mu_{j t}$ when fitting the var-scp and meanvar-scp components.

\begin{proof}

Consider the following generic model:
\begin{align*}
    \mathbf{y}_{1:T} \:|\: \boldsymbol{\Theta} &\;\sim f(\mathbf{y}_{1:T}\:|\: \boldsymbol{\Theta};\eta), \\ 
    \boldsymbol{\Theta} &\;= \{\boldsymbol{\theta}_1, \ldots, \boldsymbol{\theta}_M\}, \\
    \boldsymbol{\theta}_i &\overset{\text{ind.}}{\sim} g_i(\boldsymbol{\theta}_i),\; \sforall i \in \{1, \ldots, M\},
\end{align*}
where $\eta$ is nuisance parameter and $g_i$ is the prior over the block $\boldsymbol{\theta}_i$. Define the mean-field family of distributions over $\boldsymbol{\Theta}$:
\begin{align*} 
    \mathcal{Q}_{\text{MF}} &:= \left\{q \::\: q(\boldsymbol{\Theta}) = \prod_{i=1}^M q_i(\boldsymbol{\theta}_i)\right\}.
\end{align*}
The task of variational Bayesian inference is to find the $q^* \in \mathcal{Q}_{\text{MF}}$ that maximizes $\text{ELBO}(q;\eta)$ as defined in (\ref{eq:elbo}). If $q \in \mathcal{Q}_{\text{MF}}$ and we define:
\begin{align*}
    q_{-i}(\boldsymbol{\theta}_{-i}) := \prod_{1\leq i' \leq M,\:i'\neq i} q_{i'}(\boldsymbol{\theta}_{i'}),
\end{align*}
then we can write:
\begin{align*}
    \text{ELBO}(q;\eta) &= \int q(\boldsymbol{\Theta}) \log \frac{f(\mathbf{y}_{1:T}\:|\: \boldsymbol{\Theta};\eta) g(\boldsymbol{\Theta})}{q(\boldsymbol{\Theta})} \; d\boldsymbol{\Theta} \\
    &= \int \prod_{i'=1}^M q_{i'}(\boldsymbol{\theta}_{i'}) \log \frac{ f(\mathbf{y}_{1:T}\:|\: \boldsymbol{\Theta};\eta) \prod_{i'=1}^M g_{i'}(\boldsymbol{\theta}_{i'})}{\prod_{i'=1}^M q_{i'}(\boldsymbol{\theta}_{i'})} \; d\boldsymbol{\Theta} \\
    &= \int q_{i}(\boldsymbol{\theta}_i) \log \frac{\exp\left\{\E_{q_{-i}}[\log f(\mathbf{y}_{1:T}\:|\: \boldsymbol{\Theta};\eta)]\right\}g_{i}(\boldsymbol{\theta}_i)}{q_{i}(\boldsymbol{\theta}_i)} \; d\boldsymbol{\theta}_i - \sum_{\substack{1 \leq i' \leq M \\ i' \neq i}} \text{KL}(q_{i'}(\boldsymbol{\theta}_{i'})\:\lVert\: g_{i'}(\boldsymbol{\theta}_{i'})).
\end{align*}
Note that only the first term in the last line above depends on $q_i$, so if we treat $q_{-i}$ as fixed, then: 
\begin{align}
    \max_{q_i}  \; \text{ELBO}( q \:;\: \eta) \label{eq:max-elbo-m}
\end{align}
is equivalent to solving:
\begin{align*}
    \max_{q_i}  \; \int q_{i}(\boldsymbol{\theta}_i) \log \frac{\exp\left\{\E_{q_{-i}}[\log f(\mathbf{y}_{1:T}\:|\: \boldsymbol{\Theta};\eta)]\right\}g_{i}(\boldsymbol{\theta}_i)}{q_{i}(\boldsymbol{\theta}_i)} \; d\boldsymbol{\theta}_i
\end{align*}
If we define the normalizing constant:
\begin{align*}
    C_i := \int \exp\left\{\E_{q_{-i}}[\log f(\mathbf{y}_{1:T}\:|\: \boldsymbol{\Theta};\eta)]\right\} \; d\boldsymbol{\theta}_i
\end{align*}
then we can define a new distribution over $\boldsymbol{\theta}_i$:
\begin{align*}
    \tilde{q}_i(\boldsymbol{\theta}_i) := C^{-1}_i\exp\left\{\E_{q_{-i}}[\log f(\mathbf{y}_{1:T}\:|\: \boldsymbol{\Theta};\eta)]\right\}g_{i}(\boldsymbol{\theta}_i)
\end{align*}
Because $C_i$ does not depend on our choice of $q_i$, we have 
\begin{align*}
    \argmax{q_i} \; \text{ELBO}( q \:;\: \eta) &= \argmax{q_i} \; \int q_i(\boldsymbol{\theta}_i) \log \frac{\exp\left\{\E_{q_{-i}}[\log f(\mathbf{y}_{1:T}\:|\: \boldsymbol{\Theta};\eta)]\right\}g_i(\boldsymbol{\theta}_i)}{q_i(\boldsymbol{\theta}_i)} \; d\boldsymbol{\theta}_i\\
    &= \argmax{q_i} \; \int q_i(\boldsymbol{\theta}_i) \log \frac{C^{-1}_i\exp\left\{\E_{q_{-i}}[\log f(\mathbf{y}_{1:T}\:|\: \boldsymbol{\Theta};\eta)]\right\}g_i(\boldsymbol{\theta}_i)}{q_i(\boldsymbol{\theta}_i)} \; d\boldsymbol{\theta}_i \\
    &=  \argmax{q_i} \; \text{KL}(q_i(\boldsymbol{\theta}_i) \:\lVert\: \tilde{q}_i(\boldsymbol{\theta}_i)).
\end{align*}
If we have a closed-form expression for $\tilde{q}_i$, then the last line above shows that the solution to (\ref{eq:max-elbo-m}) is to set $q_i \equiv \tilde{q}_i$, i.e. we will have:
\begin{align}
    q_i(\boldsymbol{\theta}_i) \propto \exp\left\{\E_{q_{-i}}[\log f(\mathbf{y}_{1:T}\:|\: 
    \boldsymbol{\Theta};\eta)]\right\}g_i(\boldsymbol{\theta}_i). \label{eq:q-soln}
\end{align}
Therefore, to prove Proposition \ref{prop:vb}, we must show that (\ref{eq:q-soln}) implies that each $q_\ell$, $q_k$, and $q_j$ that make up $q_\text{mich}$ in (\ref{eq:mean-field}) have the functional forms specified in (i)-(iii) of Proposition \ref{prop:vb}. We begin by defining the key terms of the backfitting procedure:
\begin{align}
    \tilde{r}_t &:= y_t -\mu_0- \sum_{\ell = 1}^L \E_{q_\ell}[\mu_{\ell t}] - \sum_{j = 1}^J \frac{\E_{q_j}[\lambda_{jt} \mu_{jt}]}{\E_{q_j}[\lambda_{jt}]}, \label{eq:mod-resid} \\
    \overline{\lambda}_t &:= \lambda_0\prod_{k=1}^K \E_{q_k}[\lambda_{kt}] \prod_{j=1}^J \E_{q_j}[\lambda_{jt}], \label{eq:lambda-bar} \\
    \delta_t &:= \sum_{\ell=1}^L \Var_{q_\ell}\left(\mu_{\ell t} \right) +  \sum_{j=1}^J\left(\frac{\E_{q_j}[\lambda_{jt} \mu^2_{jt}]}{\E_{q_j}[\lambda_{jt}]} -\frac{\E_{q_j}[\lambda_{jt} \mu_{jt}]^2}{\E_{q_j}[\lambda_{jt}]^2} \right). \label{eq:delta}
\end{align}
We also introduce notation to help simplify the derivation of the distributions $q_\ell$, $q_j$, and $q_j$ that solve (\ref{eq:restriced-elbo}). Let $i \in \{\ell, k, j\}$ be a generic block index, then we define:
\small
\begin{align*}
    \overline{\pi}_{i t} &:= \E_{q_i}\left[\mathbbm{1}{\left\{\tau_i = t\right\}}\right],\\
    \overline{b}_{i t} &:= \E_{q_i}[b_i \:|\: \tau_i = t], \\
    \overline{\mu}_{it} &:= \E_{q_i}[\mu_{i t}] = \sum_{t'=1}^{T} \mathbbm{1}{\{t \geq t'\}}\E_{q_i}[b_i \:|\:\tau_i = t']q_i(\tau_i =t') = \sum_{t'=1}^t \overline{b}_{i t'} \overline{\pi}_{i t'}, \\
    \overline{\sigma}^2_{it} &:= \E_{q_i}\left[\left(b_i - \overline{b}_{i t}\right)^2\:|\: \tau_i = t\right], \\
    \overline{\mu^2_{it}} &:= \E_{q_i}[\mu^2_{it}] = \sum_{t'=1}^{T} \mathbbm{1}{\{t \geq t'\}}\E_{q_i}[b^2_i \:|\:\tau_i = t']q_i(\tau_i =t') = \sum_{t'=1}^t (\overline{b}^2_{i t'} + \overline{\sigma}^2_{i t'}) \overline{\pi}_{it'}, \\
    \overline{\mu}_{1:J,t} &:= \sum_{j=1}^J \overline{\mu}_{jt}, \quad \overline{\mu}_{1:L,t} := \sum_{\ell=1}^L \overline{\mu}_{\ell t}, \\
    \overline{\mu}_t &:= \E_q[\mu_t] = \mu_0 + \overline{\mu}_{1:J,t} + \overline{\mu}_{1:L,t}, \quad \overline{\mu}_{-jt} := \overline{\mu}_t - \overline{\mu}_{jt}, \quad \overline{\mu}_{-\ell t} := \overline{\mu}_t - \overline{\mu}_{\ell t}, \\
    \overline{s}_{i t} &:= \E_{q_i}[s_i \:|\: \tau_i = t], \\
    \overline{\lambda}_{it} &:= \E_{q_i}[\lambda_{it}] = \sum_{t'=1}^T \E_{q_i}[s_i \:|\: \tau_i = t']^{\mathbbm{1}\{t\geq t'\}} q_i(\tau_i = t') = \sum_{t'=1}^t \overline{s}_{i t'} \overline{\pi}_{i t'} + \sum_{t'=t+1}^T \overline{\pi}_{i t'}, \\
    \overline{\lambda}_{1:J, t} &:= \prod_{j=1}^J \overline{\lambda}_{j t}, \quad \overline{\lambda}_{1:K, t} := \prod_{k=1}^K \overline{\lambda}_{k t}.
\end{align*}
\normalsize
Let $p\left(\mathbf{y}_{1:T} \:|\:\boldsymbol{ \theta};\mu_0,\lambda_0\right)$ denote the marginal likelihood (\ref{eq:dgp}) under the MICH model. Let $p_\ell$, $p_k$, and $p_j$ be the respective priors for the parameter blocks $\boldsymbol{\theta}_\ell$, $\boldsymbol{\theta}_k$, and $\boldsymbol{\theta}_j$. We now show that the marginal distributions $q_\ell(\boldsymbol{\theta}_\ell)$, $q_k(\boldsymbol{\theta}_k)$, and $q_j(\boldsymbol{\theta}_j)$ that are implied by the expression in (\ref{eq:q-soln}) are precisely the same distributions we claim solve the appropriate maximization problems in (i)-(iii) of Proposition \ref{prop:vb}.

\begin{enumerate}[label=\roman*.]

\item For fixed a $\ell' \in\{1,\ldots,L\}$, we treat the distributions $\{q_\ell\}_{\ell\neq\ell'}$, $\{q_k\}_{k=1}^K$, and $\{q_j\}_{j=1}^J$ as fixed and known. Then by (\ref{eq:q-soln}), the solution to:
\begin{align}
    \max_{q_{\ell'}} \text{\normalfont ELBO}\left(q;\mu_0,\lambda_0\right), \label{eq:elbo-l}
\end{align}
is to pick $q_{\ell'}$ so that:
\begin{align*}
    \log q_{\ell'}(\boldsymbol{\theta}_{\ell'}) &\underset{\boldsymbol{\theta}_{\ell'}}{\propto} \E_{q_{-\ell'}} \left[\log p\left(\mathbf{y} \:|\: \boldsymbol{\Theta};\mu_0,\lambda_0\right)\right] + \log p_{\ell'}(\boldsymbol{\theta}_{\ell'}) \\
    &\underset{\boldsymbol{\theta}_{\ell'}}{\propto} - \frac{1}{2}\sum_{t=1}^{T}  \E_{q_{-\ell'}}\left[\lambda_{t}\left(y_t - \mu_t\right)^2\right] + \log p_{\ell'}(\boldsymbol{\theta}_{\ell'}).
\end{align*}
Define the partial residual:
\begin{align*}
    r_{-\ell' t} &:= y_t - \mu_t + \mu_{\ell' t}.
\end{align*}
If we fix $\tau_{\ell'} = t'$, then we have:
\begin{align*}
    \log q_{\ell'}(b_{\ell'},\tau_{\ell'}=t') 
    &\underset{\boldsymbol{\theta}_{\ell'}}{\propto}- \frac{1}{2}\sum_{t=t'}^{T}  \E_{q_{-\ell'}}\left[\lambda_{t}\left(b_{\ell'}^2 - 2r_{-\ell't}b_{\ell'} \right)\right] - \frac{b_{\ell'}^2\omega_{\ell'}}{2} + \log \pi_{\ell' t} \\
    &\;= - \frac{1}{2}\sum_{t=t'}^{T}  \left(\E_{q_{-\ell'}}\left[\lambda_{t}\right]b_{\ell'}^2 - 2\E_{q_{-\ell'}}\left[\lambda_{t}r_{-\ell't}\right]b_{\ell'}\right) - \frac{b_{\ell'}^2\omega_{\ell'}}{2} + \log \pi_{\ell' t}\\
    &\;= -\frac{1}{2}\left[b_{\ell'}^2\left(\omega_{\ell'} +\sum_{t=t'}^{T} \overline{\lambda}_t\right) - 2 b_{\ell'}\sum_{t=t'}^{T}\E_{q_{-\ell'}}\left[\lambda_{t}r_{-\ell't}\right]\right]  + \log \pi_{\ell' t}.
\end{align*}
We can now calculate the posterior parameters for $b_{\ell'}$ given $\tau_{\ell'} = t'$:
\begin{align*}
    \overline{\omega}_{\ell't'} &= \omega_{\ell'} +\sum_{t=t'}^{T} \overline{\lambda}_t \\
    \overline{b}_{\ell't'} &= \overline{\omega}_{\ell't'}^{-1}  \sum_{t=t'}^{T} \E_{q_{-\ell'}}\left[\lambda_{t}r_{-\ell't}\right].
\end{align*}
We now have:
\begin{align*}
    \log q_{\ell'}(b_{\ell'},\tau_{\ell'}=t') 
    &\underset{\boldsymbol{\theta}_{\ell'}}{\propto} \log \overline{\omega}_{\ell't'} - \frac{\overline{\omega}_{\ell't'}(b_{\ell'} - \overline{b}_{\ell't'})^2}{2} - \log \overline{\omega}_{\ell't'} + \frac{\overline{\omega}_{\ell't'}\overline{b}^2_{\ell't'}}{2}+ \log \pi_{\ell' t'}
\end{align*}
The first two terms above are just the log density of a Gaussian distribution with mean $\overline{b}_{\ell't'} $ and precision $\overline{\omega}_{\ell't'}$. Therefore, the $q_{\ell'}$ that solves (\ref{eq:elbo-l}) satisfies:
\begin{align*}
    b_{\ell'} \:|\: \tau_{\ell'} = t' \sim \mathcal{N}(\overline{b}_{j't'}, \overline{\omega}^{-1}_{j't'}),
\end{align*}
which matches (\ref{eq:b-smcp}) of the mean-scp posterior. Integrating the joint density $q_{\ell'}(\boldsymbol{\theta}_{\ell'})$ with respect to $b_{\ell'}$ and calculating the normalizing constant, we are left with: 
\begin{align*}
    q_{\ell'}(\tau_{\ell'} = t') &= \frac{\pi_{\ell't'} \overline{\omega}_{\ell't'}^{-\frac{1}{2}} \exp[\overline{\omega}_{\ell't'} \overline{b}^2_{\ell't'}/2]}{\sum_{t=1}^T \pi_{\ell't} \overline{\omega}_{\ell't}^{-\frac{1}{2}} \exp[\overline{\omega}_{\ell't} \overline{b}^2_{\ell't}/2]}\\ 
    &:= \overline{\pi}_{\ell't'}, 
\end{align*}
which matches (\ref{eq:gamma-post-cat1}). Therefore, the $q_{\ell'}$ that solves (\ref{eq:elbo-l}) has the same functional form as the posterior distribution of the mean-scp with parameters $\overline{\boldsymbol{\theta}}_{\ell'} := \{\overline{b}_{\ell't},\overline{\omega}_{\ell't},\overline{\pi}_{\ell't}\}_{t=1}^T$. All that remains is to show $\overline{\boldsymbol{\theta}}_{\ell'}$ is the same set of parameters defined in (i) of Proposition \ref{prop:vb}. Define a partial mean residual $\tilde{r}_{-\ell t} :=  \tilde{r}_{t} + \E_{q_{\ell}}[\mu_{\ell t}]$. Then we have:
\begin{align*}
    \E_{q_{-\ell}}\left[\lambda_{t}r_{-\ell t}\right] &= \E_{q_{-\ell}}\left[\lambda_0\prod_{j=1}^J\lambda_{jt}\prod_{k=1}^K\lambda_{kt} \left(y_t - \mu_0 - \sum_{\ell'\neq \ell} \mu_{\ell't} - \sum_{j=1}^J \mu_{jt}\right)\right] \\
    &= \lambda_0 \prod_{k=1}^K\E_{q_{k}}\left[\lambda_{kt}\right]\left(y_t -\mu_0 - \sum_{\ell'\neq \ell} \E_{q_{\ell'}}\left[\mu_{\ell' t}\right] - \sum_{j=1}^J \E_{q_{-\ell}}\left[\lambda_{jt}\mu_{jt}\prod_{j'\neq j} \lambda_{j't}\right]\right) \tag{by (\ref{eq:mean-field})} \\
    &= \overline{\lambda}_{t}\left(y_t - \mu_0 - \overline{\mu}_{-\ell t} -  \sum_{j=1}^J\overline{\lambda}_{jt}^{-1} \E_{q_{j}}\left[\lambda_{jt}\mu_{jt}\right]\right) \\
    &=  \overline{\lambda}_{t} \tilde{r}_{-\ell t}.
\end{align*}
Returning to the posterior parameters for $b_{\ell}$ and $\tau_{\ell}$, we can now write: 
\begin{align*}
    \overline{\omega}_{jt} &=  \omega_{\ell} +  \sum_{t'=t}^{T} \overline{\lambda}_{t'}, \\
    \overline{b}_{\ell t}  &= \overline{\omega}_{\ell t}^{-1}  \sum_{t'=t}^{T} \overline{\lambda}_{t'}\tilde{r}_{-\ell t'},
\end{align*}
Examining the mean-scp posterior parameters (\ref{eq:mean-scp-post-omega})-(\ref{eq:mean-scp-post-pi}), it is clear that:
\begin{align*}
    \overline{\boldsymbol{\theta}}_\ell = \normalfont{\texttt{mean-scp}}(\tilde{\mathbf{r}}_{-\ell,1:T} \:;\: \overline{\boldsymbol{\lambda}}_{1:T}, \omega_{\ell}, \boldsymbol{\pi}_{\ell,1:T}).
\end{align*}

\item For fixed $k' \in\{1,\ldots,K\}$, we treat the distributions $\{q(\boldsymbol{\theta}_\ell)\}_{\ell=1}^L$, $\{q(\boldsymbol{\theta}_k)\}_{k \neq k'}$, and $\{q(\boldsymbol{\theta}_{j})\}_{j=1}^J$ as fixed and known. Then by (\ref{eq:q-soln}), the solution to:
\begin{align}
    \max_{q_{k'}} \text{\normalfont ELBO}\left(q;\mu_0,\lambda_0\right), \label{eq:elbo-k}
\end{align}
is to pick $q_{k'}$ so that:
\begin{align*}
    \log q_{k'}(\boldsymbol{\theta}_{k'}) &\underset{\boldsymbol{\theta}_{k'}}{\propto} \E_{q_{-k'}} \left[\log p\left(\mathbf{y}_{1:T} \:|\: \boldsymbol{\Theta};\mu_0,\lambda_0\right)\right] + \log p_{k'}(\boldsymbol{\theta}_{k'}) \\
    &\underset{\boldsymbol{\theta}_{k'}}{\propto}  \E_{q_{-k'}}\left[\frac{1}{2}\sum_{t=1}^{T} \log\lambda_t - \frac{1}{2}\sum_{t=1}^{T} \lambda_{t}\left(y_t - \mu_t\right)^2\right] + \log p_{k'}(\boldsymbol{\theta}_{k'}).
\end{align*}
Define $\lambda_{-k't} := \lambda_{k't}^{-1}\lambda_t$. If we fix $\tau_{k'} = t'$, then we have:
\begin{align*}
    \log q_{k'}(s_{k'},\tau_{k'} = t') &\underset{\boldsymbol{\theta}_{k'}}{\propto} \frac{T - t' + 1}{2} \log s_{k'} - \frac{s_{k'}}{2}\sum_{t=t'}^{T} \E_{q_{-k'}}\left[\lambda_{-k't}\left(y_t - \mu_t\right)^2\right] \\
    &\quad - \frac{1}{2}\sum_{t=1}^{t'-1} \E_{q_{-k'}}\left[\lambda_{-k't}\left(y_t - \mu_t\right)^2\right]  + (u_{k'}-1)\log s_{k'} - v_{k'} s_{k'} + \log \pi_{k't'}.
\end{align*}
Define:
\begin{align*}
    \overline{u}_{k't'} &= u_{k'} + \frac{T-t'+1}{2} \\
    \overline{v}_{k't'} &= v_{k'} + \frac{1}{2}\sum_{t=t'}^{T} \E_{q_{-k'}}\left[\lambda_{-k't}\left(y_t - \mu_t\right)^2\right].
\end{align*}
Then we have:
\begin{align*}
    \log q_{k'}(s_{k'},\tau_{k'} = t') 
    &\underset{\boldsymbol{\theta}_{k'}}{\propto} -  \log\Gamma(\overline{u}_{k't'}) + \overline{u}_{k't'} \log \overline{v}_{k't'} + \left(\overline{u}_{k't'} -1\right)\log s_{k'}  - \overline{v}_{k't'} s_{k'} \\
    &\quad + \log \pi_{k't'} + \log\Gamma(\overline{u}_{k't'}) - \overline{u}_{k't'} \log \overline{v}_{k't'} - \frac{1}{2}\sum_{t=1}^{t'-1} \E_{q_{-k'}}\left[\lambda_{-k't}\left(y_t - \mu_t\right)^2\right]
\end{align*}
The first line above is just the log density of a Gamma distribution with shape and rate parameters $\overline{u}_{k't'}$, and $\overline{v}_{k't'}$. Therefore, the $q_{k'}$ that solves (\ref{eq:elbo-k}) satisfies:
\begin{align*}
    s_{k'} \:|\: \tau_{k'} = t' \sim \text{Gamma}(\overline{u}_{k't'}, \overline{v}_{k't'}),
\end{align*}
which matches (\ref{eq:s-sscp}) of the var-scp posterior. Integrating the joint density $q_{k'}(\boldsymbol{\theta}_{k'})$ with respect to $s_{k'}$ and calculating the normalizing constant, we are left with: 
\begin{align*}
    q_{k'}(\tau_{k'} = t) &= \frac{\pi_{k't}\Gamma(\overline{u}_{k't})\overline{v}_{k't}^{-\overline{u}_{k't}}\exp\left( -\frac{1}{2} \sum_{t'=1}^{t-1} \E_{q_{-k'}}\left[\lambda_{-k't'}(y_{t'} - \mu_{t'})^2\right] \right)}{\sum_{t'=1}^T \pi_{k't'}\Gamma(\overline{u}_{k't'})\overline{v}_{k't'}^{-\overline{u}_{k't'}}\exp\left( -\frac{1}{2} \sum_{t''=1}^{t'-1} \E_{q_{-k'}}\left[\lambda_{-k't''}(y_{t''} - \mu_{t''})^2\right] \right)} \\
    &:= \overline{\pi}_{k't}, 
\end{align*}
which matches (\ref{eq:gamma-post-cat1}). Therefore, the $q_{k'}$ that solves (\ref{eq:elbo-k}) has the same functional form as the posterior distribution of the var-scp model with parameters $\overline{\boldsymbol{\theta}}_{k'} := \{\overline{u}_{k't},\overline{v}_{k't},\overline{\pi}_{k't}\}_{t=1}^T$. All that remains is to show $\overline{\boldsymbol{\theta}}_{k'}$ is the same set of parameters defined in (ii) of Proposition \ref{prop:vb}. Define a partial scale residual $\overline{\lambda}_{-k t} := \overline{\lambda}_{kt}^{-1}\overline{\lambda}_t$. Then using the residual $\tilde{r}_t$ (\ref{eq:mod-resid}) and the variance correction term $\delta_t$ (\ref{eq:delta}), we have:
\footnotesize
\begin{align*}
    \E_{q_{-k}}\left[\lambda_{-kt}(y_t - \mu_t)^2\right] &= \E_{q_{-k}}\left[\lambda_0\prod_{j =1}^J\lambda_{jt} \prod_{k' \neq k} \lambda_{k't}\left(y_t - \mu_0 - \sum_{j'=1}^J \mu_{j't} - \sum_{\ell=1}^L \mu_{\ell t}\right)^2\right] \\
    &= \lambda_0\prod_{j =1}^J\E_{q_{j}}\left[\lambda_{jt}\right] \prod_{k'\neq k} \E_{q_{k'}}\left[\lambda_{k't}\right]\E_{q_{-k}}\left[\left(y_t - \mu_0 - \sum_{\ell=1}^L \mu_{\ell t}\right)^2\right] \\
    &\quad -2 \lambda_0 \prod_{k'\neq k} \E_{q_{k'}}\left[\lambda_{k't}\right]\E_{q_{-k}}\left[y_t - \mu_0  - \sum_{\ell=1}^L \mu_{\ell t}\right] \sum_{j=1}^J \E_{q_{j}}\left[\lambda_{jt} \mu_{jt}\right]\prod_{j' \neq j}\E_{q_{j'}}[\lambda_{j't}]  \\
    &\quad + \lambda_0 \prod_{k' \neq k} \E_{q_{k'}}\left[ \lambda_{k't}\right]\E_{q_{-k}}\left[\sum_{j=1}^J\sum_{j'=1}^J  \prod_{j''=1}^J \lambda_{j''t} \mu_{jt} \mu_{j't}\right] \\
    &= \overline{\lambda}_{-kt}\left[\left(y_t - \mu_0 - \sum_{\ell=1}^L \overline{\mu}_{\ell t}\right)^2 + \sum_{\ell=1}^L \left(\overline{\mu^2_{\ell t}} - \overline{\mu}^2_{\ell t} \right) \right] \\
    &\quad -2  \overline{\lambda}_{-kt}\left(y_t - \mu_0  - \overline{\mu}_{1:L, t}\right) \sum_{j=1}^J \overline{\lambda}_{jt}^{-1} \E_{q_{j}}\left[\lambda_{jt} \mu_{jt}\right] \\
    &\quad + \overline{\lambda}_{-kt} \sum_{j=1}^J\sum_{j'=1}^J \overline{\lambda}_{jt}^{-1}\E_{q_{-k}}\left[\lambda_{jt} \mu_{jt}\right]\overline{\lambda}_{j't}^{-1}\E_{q_{j'}}\left[ \lambda_{j't}\mu_{j't}\right] \\
    &\quad + \overline{\lambda}_{-kt} \sum_{j=1}^J\left(\overline{\lambda}_{jt}^{-1}\E_{q_{j}}\left[ \lambda_{jt}\mu_{jt}^2\right] -\overline{\lambda}_{jt}^{-2}\E_{q_{j}}\left[ \lambda_{jt}\mu_{jt}\right]^2\right) \\
    &=  \overline{\lambda}_{-kt}\left(y_t - \mu_0 - \sum_{j=1}^J \overline{\lambda}_{jt}^{-1} \E_{q_{j}}\left[\lambda_{jt} \mu_{jt}\right] - \overline{\mu}_{1:L, t}\right)^2 \\
    &\quad + \overline{\lambda}_{-kt}\left[\sum_{\ell=1}^L \left(\overline{\mu^2_{\ell t}} - \overline{\mu}^2_{\ell t} \right) + \sum_{j=1}^J\left(\overline{\lambda}_{jt}^{-1}\E_{q_{j}}\left[ \lambda_{jt}\mu_{jt}^2\right] -\overline{\lambda}_{jt}^{-2}\E_{q_{j}}\left[ \lambda_{jt}\mu_{jt}\right]^2\right)\right] \\
    &= \overline{\lambda}_{-kt}(\tilde{r}^2_t + \delta_t). \tag{by (\ref{eq:mod-resid}) and (\ref{eq:delta})} 
\end{align*}
\normalsize
We now define variance corrected prior parameters for the rate of $s_{k}$ and location of the change-point:
\begin{align}
    \tilde{v}_{kt} &:= v_k + \frac{1}{2}\sum_{t'=t}^{T}\overline{\lambda}_{-kt'}\delta_{t'}, \label{eq:v-k-corrected} \\
    \tilde{\pi}_{kt} &:= \frac{\pi_{kt} \exp\left(-\frac{1}{2}\sum_{t'=1}^{t-1}\overline{\lambda}_{-kt'}\delta_{t'}\right)}{\sum_{t'=1}^T \pi_{kt'} \exp\left(-\frac{1}{2}\sum_{t''=1}^{t'-1}\overline{\lambda}_{-kt''}\delta_{t''}\right)}. \label{eq:pi-k-corrected}
\end{align}
Then, returning to the posterior parameters for $s_{k}$ when $\tau_{k}=t$, we now have:
\begin{align*}
    \overline{u}_{kt} &= u_{k} + \frac{T-t+1}{2}, \\
    \overline{v}_{kt} &= v_{k} + \frac{1}{2}\sum_{t'=t}^{T}\overline{\lambda}_{-kt'}\tilde{r}^2_{t'} + \frac{1}{2}\sum_{t'=t}^{T}\overline{\lambda}_{-kt'}\delta_{t'} \\
    &= \tilde{v}_{kt} + \frac{1}{2}\sum_{t'=t}^{T}\overline{\lambda}_{-kt'}\tilde{r}^2_{t'}, \\
    \overline{\pi}_{kt}  &= \frac{\exp\left(-\frac{1}{2}\sum_{t'=1}^{t-1}\overline{\lambda}_{-kt'}\delta_{t'}\right)\pi_{kt}\Gamma(\overline{u}_{kt})\overline{v}_{kt}^{-\overline{u}_{kt}}\exp\left( -\frac{1}{2} \sum_{t'=1}^{t-1} \overline{\lambda}_{-kt'}\tilde{r}_{t'}^2 \right)}{\sum_{t'=1}^T \exp\left(-\frac{1}{2}\sum_{t''=1}^{t'-1}\overline{\lambda}_{-kt''}\delta_{t''}\right) \pi_{kt'}\Gamma(\overline{u}_{kt'})\overline{v}_{kt'}^{-\overline{u}_{kt'}}\exp\left( -\frac{1}{2} \sum_{t''=1}^{t'-1} \overline{\lambda}_{-kt''}\tilde{r}_{t''}^2\right)} \\
    &= \frac{\tilde{\pi}_{kt}\Gamma(\overline{u}_{kt})\overline{v}_{kt}^{-\overline{u}_{kt}}\exp\left( -\frac{1}{2} \sum_{t'=1}^{t-1} \overline{\lambda}_{-kt'}\tilde{r}_{t'}^2 \right)}{\sum_{t'=1}^T \tilde{\pi}_{kt'}\Gamma(\overline{u}_{kt'})\overline{v}_{kt'}^{-\overline{u}_{kt'}}\exp\left( -\frac{1}{2} \sum_{t''=1}^{t'-1} \overline{\lambda}_{-kt''}\tilde{r}_{t''}^2\right)}.
\end{align*}
Examining the var-scp posterior parameters (\ref{eq:var-scp-post-u})-(\ref{eq:var-scp-post-pi}), it is clear that:
\begin{align*}
    \overline{\boldsymbol{\theta}}_k = \normalfont{\texttt{var-scp}}(\tilde{\mathbf{r}}_{1:T} \:;\:\overline{\boldsymbol{\lambda}}_{-k,1:T}, u_k, \tilde{\mathbf{v}}_{k,1:T}, \tilde{\boldsymbol{\pi}}_{k,1:T}).  
\end{align*}

\item For fixed $j' \in\{1,\ldots,J\}$, we treat the distributions $\{q_\ell(\boldsymbol{\theta}_\ell)\}_{\ell=1}^L$, $\{q_k(\boldsymbol{\theta}_k)\}_{k=1}^K$, and $\{q_j(\boldsymbol{\theta}_j)\}_{j\neq j'}$ as fixed and known. Then by (\ref{eq:q-soln}), the solution to:
\begin{align}
    \max_{q_{j'}} \text{\normalfont ELBO}\left(q;\mu_0,\lambda_0\right), \label{eq:elbo-j}
\end{align}
is to pick $q_{j'}$ so that:
\begin{align*}
    \log q_{j'}(\theta_{j'}) &\underset{\theta_{j'}}{\propto} \E_{q_{-j'}} \left[\log p\left(\mathbf{y}_{1:T} \:|\: \boldsymbol{\Theta};\mu_0,\lambda_0\right)\right]  + \log p_{j'}(\theta_{j'}) \\
    &\underset{\theta_{j'}}{\propto}  \E_{q_{-j'}}\left[\frac{1}{2}\sum_{t=1}^{T} \log\lambda_t - \frac{1}{2}\sum_{t=1}^{T} \lambda_{t}\left(y_t - \mu_t\right)^2\right] + \log p_{j'}(\theta_{j'}).
\end{align*}
Define $\lambda_{-j't} := \lambda_{j't}^{-1}\lambda_t$. If we fix $\tau_{j'} = t'$, then we have:
\begin{align*}
     \log q_{j'}(b_{j'}, s_{j'}, \tau_{j'} = t')&\underset{\theta_{j'}}{\propto} \frac{T - t' + 1}{2} \log s_{j'}  -\frac{s_{j'}}{2} \sum_{t=t'}^{T} \E_{q_{-j'}}\left[\lambda_{-j't}(r_{-j't} - b_{j'})^2\right] \\
    &\quad -\frac{1}{2} \sum_{t=1}^{t'-1} \E_{q_{-j'}}\left[\lambda_{-j't}r_{-j't}^2\right] \\
    &\quad + \frac{1}{2}\log s_{j'} - \frac{s_{j'}\omega_{j'}b_{j'}^2}{2} + (u_{j'}-1)\log s_{j'} - v_{j'}s_{j'} + \log \pi_{j't'}.  
\end{align*}
Define:
\begin{align*}
    \overline{\omega}_{j't'} &=  \omega_{j'} +  \sum_{t=t'}^{T} \E_{q_{-j'}}\left[\lambda_{-j't}\right], \\
    \overline{b}_{j't'}  &= \overline{\omega}_{j't'}^{-1}  \sum_{t=t'}^{T} \E_{q_{-j'}}\left[\lambda_{-j't}r_{-j't}\right], \\
    \overline{u}_{j't'} &= u_{j'} + \frac{T-t'+1}{2}, \\
    \overline{v}_{j't'} &= v_{j'} - \frac{\overline{\omega}_{j't'}\overline{\mu}^2_{j't'}}{2} + \frac{1}{2}\sum_{t=t'}^{T}\E_{q_{-j'}}\left[\lambda_{-j't}r^2_{-j't}\right].
\end{align*}
Then we have:
\begin{align*}
    \log q_{j'}(b_{j'}, s_{j'}, \tau_{j'} = t')
    &\underset{\theta_{j'}}{\propto} -\log\Gamma(\overline{u}_{j't'}) + \overline{u}_{j't'} \log \overline{v}_{j't'}  + \left(\overline{u}_{j't'} -1\right)\log s_{j'}  - \overline{v}_{j't'} s_{j'} \\
    &\quad + \frac{1}{2}\log \overline{\omega}_{j't'}s_{j'} - \frac{\overline{\omega}_{j't'} s_{j'}}{2} (b_{j'} - \overline{b}_{j't'} )^2 \\
    &\quad + \log \pi_{j't'} + \log\Gamma(\overline{u}_{j't'}) - \overline{u}_{j't'} \log \overline{v}_{j't'} - \frac{1}{2}\log \overline{\omega}_{j't'} -\frac{1}{2} \sum_{t=1}^{t'-1} \E_{q_{-j'}}\left[\lambda_{-j't}r_{-j't}^2\right]. 
\end{align*}
The first two lines above are just the log density of a Normal-Gamma distribution with parameters $\overline{b}_{j't'} $, $\overline{\omega}_{j't'}$, $\overline{u}_{j't'}$, and $\overline{v}_{j't'}$. Therefore, the $q_{j'}$ that solves (\ref{eq:elbo-j}) satisfies:
\begin{align*}
    b_{j'}, s_{j'} \:|\: \tau_{j'} = t' \sim \text{Normal-Gamma}(\overline{b}_{j't'} , \overline{\omega}_{j't'}, \overline{u}_{j't'}, \overline{v}_{j't'}),
\end{align*}
which matches (\ref{eq:bs-smscp}) of the meanvar-scp posterior. Integrating the joint density $q_{j'}(\theta_{j'})$ with respect to $b_{j'}$ and $s_{j'}$ and calculating the normalizing constant, we are left with:
\begin{align*}
    q_{j'}(\tau_{j'} = t) &= \frac{\pi_{j't}\Gamma(\overline{u}_{j't})\overline{v}_{j't}^{-\overline{u}_{j't}}\overline{\omega}_{j't}^{-\frac{1}{2}}\exp\left( -\frac{1}{2} \sum_{t'=1}^{t-1} \E_{q_{-j'}}\left[\lambda_{-j't'}r_{-j't'}^2\right] \right)}{\sum_{t'=1}^T \pi_{j't'}\Gamma(\overline{u}_{j't'})\overline{v}_{j't'}^{-\overline{u}_{j't'}}\overline{\omega}_{j't'}^{-\frac{1}{2}}\exp\left( -\frac{1}{2} \sum_{t''=1}^{t'-1} \E_{q_{-j'}}\left[\lambda_{-j't''}r_{-j't''}^2\right] \right)} \\
    &:= \overline{\pi}_{j't}, 
\end{align*}
which matches (\ref{eq:gamma-post-cat1}). Therefore, the $q_{j'}$ that solves (\ref{eq:elbo-j}) has the same functional form as the posterior distribution of the meavar-scp model with parameters $\overline{\boldsymbol{\theta}}_{j'} := \{\overline{b}_{j't}, \overline{\omega}_{j't},\overline{u}_{j't},\overline{v}_{j't},\overline{\pi}_{j't}\}_{t=1}^T$. All that remains is to show $\overline{\boldsymbol{\theta}}_{j'}$ is the same set of parameters defined in (iii) of Proposition \ref{prop:vb}. We have:
\footnotesize
\begin{align*}
    \E_{q_{-j'}}\left[\lambda_{-j't}\right] &= \E_{q_{-j'}}\left[\lambda_0\prod_{j \neq j'}\lambda_{jt} \prod_{k=1}^K \lambda_{kt}\right] \\
    &= \lambda_0\prod_{j \neq j'}\E_{q_{j}}\left[\lambda_{jt}\right] \prod_{k=1}^K \E_{q_{k}}\left[\lambda_{kt}\right] \tag{by (\ref{eq:mean-field})} \\
    &= \overline{\lambda}_{-jt}, \\
\end{align*}
\begin{align*}
    \E_{q_{-j'}}\left[\lambda_{-j't}r_{-j't}\right] &= \E_{q_{-j'}}\left[\lambda_0\prod_{j \neq j'}\lambda_{jt} \prod_{k=1}^K \lambda_{kt}\left(y_t - \mu_0 - \sum_{j''\neq j'} \mu_{j''t} - \sum_{\ell=1}^L \mu_{\ell t}\right)\right] \\
    &= \lambda_0\prod_{j \neq j'}\E_{q_{j}}\left[\lambda_{jt}\right] \prod_{k=1}^K \E_{q_{k}}\left[\lambda_{kt}\right]\left(y_t -\mu_0 -\sum_{\ell=1}^L\E_{q_{\ell}}\left[\mu_{\ell t}\right]\right) \\
    &\quad-\lambda_0\prod_{k=1}^K \E_{q_{k}}\left[\lambda_{kt}\right]\E_{q_{-j'}}\left[\sum_{j'' \neq j'}\prod_{j \neq j'} \mu_{j''t}\lambda_{jt}\right]  \tag{by (\ref{eq:mean-field})} \\
    &=  \overline{\lambda}_{-j't} \left(y_t -\mu_0 -\overline{\mu}_{1:L, t}\right) \\
    &\quad-\lambda_0\overline{\lambda}_{1:K,t} \sum_{j'' \neq j'}\E_{q_{-j'}}\left[ \mu_{j''t}\lambda_{j''t}\prod_{j \not\in\{j',j''\}} \lambda_{jt}\right]  \\
    &= \overline{\lambda}_{-j't} \left(y_t -\mu_0 -\overline{\mu}_{1:L, t}\right) \\
    &\quad-\lambda_0\overline{\lambda}_{1:K,t} \sum_{j'' \neq j'}\E_{q_{j''}}\left[ \mu_{j''t}\lambda_{j''t} \right] \prod_{j \not\in\{j',j''\}} \E_{q_{j}}\left[\lambda_{jt}\right] \\
    &= \overline{\lambda}_{-j't} \left(y_t -\mu_0 -\overline{\mu}_{1:L, t}\right) \\
    &\quad-\lambda_0\overline{\lambda}_{1:K,t} \sum_{j \neq j'}\E_{q_{j}}\left[ \mu_{jt}\lambda_{jt} \right] \overline{\lambda}_{jt}^{-1}\overline{\lambda}_{-j't}\tag{multiply and divide by $\overline{\lambda}_{jt}$}\\
    &=\overline{\lambda}_{-j't} \left(y_t -\mu_0 -  \sum_{j \neq j'}\overline{\lambda}_{jt}^{-1}\E_{q_{j}}\left[ \mu_{jt}\lambda_{jt} \right] -\overline{\mu}_{1:L, t}\right), \\
\end{align*}
\begin{align*} 
    \E_{q_{j}}\left[ \mu_{jt}\lambda_{jt} \right] &= \E_{q_{j}}\left\{\E_{q_{j}}\left[\E_{q_{j}}\left[\mu_{j t}\:|\: s_j, \tau_j\right]\lambda_{j t}\:|\: \tau_j\right]\right\} \label{eq:lambda-beta} \\
    &= \sum_{t'=1}^T  \E_{q_{j}}\left[\E_{q_{j}}\left[\mu_{j t}\:|\: s_j, \tau_j = t'\right]\lambda_{j t}\:|\: \tau_j = t'\right]\overline{\pi}_{j t'} \\
    &= \sum_{t'=1}^t \overline{b}_{j t'}\E_{q_j}\left[s_j^{\mathbbm{1}\{t \geq t'\}}\:|\: \tau_j= t' \right] \overline{\pi}_{jt'} \tag{$\E_{q_j}\left[\mu_{j t}\:|\: s_j, \tau_j = t'\right] = \mathbbm{1}{\{t \geq t'\}} \overline{b}_{j t'}$} \\
    &= \sum_{t'=1}^t \overline{b}_{j t'}\overline{s}_{jt'} \overline{\pi}_{jt'} \\
    &=  \sum_{t'=1}^t \overline{b}_{j t'}\frac{\overline{u}_{jt'}}{\overline{v}_{jt'}} \overline{\pi}_{jt'} 
\end{align*}
\normalsize
Define the partial mean residual $\tilde{r}_{-j t} := \tilde{r}_{t} + \E_{q_{j}}[\mu_{j t}]$, the partial scale residual $\overline{\lambda}_{-j t} := \overline{\lambda}_{jt}^{-1}\overline{\lambda}_t$, and the partial correction term: 
\begin{align}
    \delta_{-j't} := \delta_t - \overline{\lambda}_{j't}^{-1}\E_{q_{j'}}\left[ \lambda_{jt}\mu_{j't}^2\right] +\overline{\lambda}_{j't}^{-2}\E_{q_{j'}}\left[ \lambda_{j't}\mu_{j't}\right]^2. \label{eq:delta-j}
\end{align}
Then we have:
\begin{align*}
     \E_{q_{-j'}}\left[\lambda_{-j't}r_{-j't}\right] &=\overline{\lambda}_{-j't}\tilde{r}_{-j' t} 
\end{align*}
We also have:
\footnotesize
\begin{align*}
    \E_{q_{-j'}}\left[\lambda_{-j't}r^2_{-j't}\right] &=  \E_{q_{-j'}}\left[\lambda_0\prod_{j \neq j'}\lambda_{jt} \prod_{k=1}^K \lambda_{kt}\left(y_t - \mu_0 - \sum_{j''\neq j'} \mu_{j''t} - \sum_{\ell=1}^L \mu_{\ell t}\right)^2\right] \\
    &= \lambda_0\prod_{j \neq j'}\E_{q_{j}}\left[\lambda_{jt}\right] \prod_{k=1}^K \E_{q_{k}}\left[\lambda_{kt}\right]\E_{q_{-j'}}\left[\left(y_t - \mu_0 - \sum_{\ell=1}^L \mu_{\ell t}\right)^2\right] \\
    &\quad -2 \lambda_0 \prod_{k=1}^K \E_{q_{k}}\left[\lambda_{kt}\right]\E_{q_{-j'}}\left[y_t - \mu_0  - \sum_{\ell=1}^L \mu_{\ell t}\right] \sum_{j''\neq j'} \E_{q_{j''}}\left[\lambda_{j''t} \mu_{j''t}\right]\prod_{j \not\in \{j',j''\}}\E_{q_{j}}[\lambda_{jt}]  \\
    &\quad + \lambda_0 \prod_{k=1}^K \E_{q_{k}}\left[ \lambda_{kt}\right]\E_{q_{-j'}}\left[\sum_{j^*\neq j'}\sum_{j''\neq j'}  \prod_{j \neq j'} \lambda_{jt} \mu_{j''t} \mu_{j^*t}\right] \\
    &= \overline{\lambda}_{-j't}\left[\left(y_t - \mu_0 - \sum_{\ell=1}^L \overline{\mu}_{\ell t}\right)^2 + \sum_{\ell=1}^L \left(\overline{\mu^2_{\ell t}} - \overline{\mu}^2_{\ell t} \right) \right] \\
    &\quad -2  \overline{\lambda}_{-j't}\left(y_t - \mu_0  - \overline{\mu}_{1:L, t}\right) \sum_{j\neq j'} \overline{\lambda}_{jt}^{-1} \E_{q_{j}}\left[\lambda_{jt} \mu_{jt}\right] \\
    &\quad + \overline{\lambda}_{-j't} \sum_{j\neq j'}\sum_{j''\neq j'} \overline{\lambda}_{j''t}^{-1}\E_{q_{j''}}\left[\lambda_{j''t} \mu_{j''t}\right]\overline{\lambda}_{jt}^{-1}\E_{q_{j}}\left[ \lambda_{jt}\mu_{jt}\right] \\
    &\quad + \overline{\lambda}_{-j't} \sum_{j\neq j'}\left(\overline{\lambda}_{jt}^{-1}\E_{q_{j}}\left[ \lambda_{jt}\mu_{jt}^2\right] -\overline{\lambda}_{jt}^{-2}\E_{q_{j}}\left[ \lambda_{jt}\mu_{jt}\right]^2\right) \\
    &=  \overline{\lambda}_{-j't}\left(y_t - \mu_0 - \sum_{j\neq j'} \overline{\lambda}_{j't}^{-1} \E_{q_{j}}\left[\lambda_{jt} \mu_{jt}\right] - \sum_{\ell=1}^L \overline{\mu}_{\ell t}\right)^2 \\
    &\quad + \overline{\lambda}_{-j't}\left[\sum_{\ell=1}^L \left(\overline{\mu^2_{\ell t}} - \overline{\mu}^2_{\ell t} \right) + \sum_{j\neq j'}\left(\overline{\lambda}_{jt}^{-1}\E_{q_{j}}\left[ \lambda_{jt}\mu_{jt}^2\right] -\overline{\lambda}_{jt}^{-2}\E_{q_{j}}\left[ \lambda_{jt}\mu_{jt}\right]^2\right)\right] \\
    &= \overline{\lambda}_{-j't}\left[\tilde{r}^2_{-j' t} +\sum_{\ell=1}^L \Var_{q_\ell} \left(\mu_{\ell t} \right) + \sum_{j\neq j'}\left(\overline{\lambda}_{jt}^{-1}\E_{q_{j}}\left[ \lambda_{jt}\mu_{jt}^2\right] -\overline{\lambda}_{jt}^{-2}\E_{q_{j}}\left[ \lambda_{jt}\mu_{jt}\right]^2\right)\right]\\
    &= \overline{\lambda}_{-j't}\left[\tilde{r}^2_{-j' t} + \delta_{-j't}\right]
\end{align*}
\normalsize
Finally, we have:
\small
\begin{align*}
    \E_{q_{j}}\left[\lambda_{jt}\mu^2_{jt}\right] &= \E_{q_{j}}\left\{\E_{q_{j}}\left[\E_{q_{j}}\left[\mu^2_{jt}\:|\: s_{j}, \tau_{j}\right]\lambda_{jt}\:|\: \tau_{j}\right]\right\} \\
    &= \sum_{t'=1}^T  \E_{q_{j}}\left[\E_{q_{j}}\left[\mu^2_{jt}\:|\: s_{j}, \tau_{j} = t'\right]\lambda_{jt}\:|\: \tau_{j}= t'\right] \overline{\pi}_{jt'} \\
    &= \sum_{t'=1}^t \E_{q_{j}}\left[(\overline{b}_{j t'}^2 + (\overline{\omega}_{j t'}s_{j})^{-1})s_j^{\mathbbm{1}\{t \geq t'\}}\:|\: \tau_j= t'\right] \overline{\pi}_{j t'} \tag{$\E_{q_{j}}[\mu^2_{jt}\:|\: s_j, \tau_j = t'] = \mathbbm{1}{\{t \geq t'\}} (\overline{b}_{jt'}^2 + (\overline{\omega}_{jt'}s_{j})^{-1})$} \\
    &= \sum_{t'=1}^t \left(\overline{b}^2_{jt'}\overline{s}_{jt'} +\overline{\omega}_{jt'}^{-1}\right) \overline{\pi}_{jt'} \\
    &= \sum_{t'=1}^t \left(\frac{\overline{b}^2_{jt'}\overline{u}_{jt'}}{\overline{v}_{jt'}} +\frac{1}{\overline{\omega}_{jt'}}\right) \overline{\pi}_{jt'}.
\end{align*}
\normalsize
We now define variance corrected prior parameters for the rate of $s_{j}$ and location of the change-point:
\begin{align}
    \tilde{v}_{jt} &:= v_j + \frac{1}{2}\sum_{t'=t}^{T}\overline{\lambda}_{-jt'}\delta_{-jt'}, \label{eq:v-j-corrected} \\
    \tilde{\pi}_{jt} &:= \frac{\pi_{jt} \exp\left(-\frac{1}{2}\sum_{t'=1}^{t-1}\overline{\lambda}_{-jt'}\delta_{-jt'}\right)}{\sum_{t'=1}^T \pi_{jt'} \exp\left(-\frac{1}{2}\sum_{t''=1}^{t'-1}\overline{\lambda}_{-jt''}\delta_{-jt''}\right)}. \label{eq:pi-j-corrected}
\end{align}
Then, returning to the posterior parameters for $b_{j}$ and $s_{j}$ when $\tau_{j}=t$, we now have:
\begin{align*}
    \overline{\omega}_{jt} &=  \omega_{j} +  \sum_{t'=t}^{T} \overline{\lambda}_{-jt'}, \\
    \overline{b}_{jt}  &= \overline{\omega}_{jt}^{-1}  \sum_{t'=t}^{T} \overline{\lambda}_{-jt'}\tilde{r}_{-j t'},\\
    \overline{u}_{jt} &= u_{j} + \frac{T-t+1}{2}, \\
    \overline{v}_{jt} &= v_{j} - \frac{\overline{\omega}_{jt}\overline{\mu}^2_{jt}}{2} + \frac{1}{2}\sum_{t'=t}^{T}\overline{\lambda}_{-jt'}\tilde{r}^2_{jt'} + \frac{1}{2}\sum_{t'=t}^{T}\overline{\lambda}_{-jt'}\delta_{-jt'} \\
    &= \tilde{v}_{jt} - \frac{\overline{\omega}_{jt}\overline{\mu}^2_{jt}}{2} + \frac{1}{2}\sum_{t'=t}^{T}\overline{\lambda}_{-jt'}\tilde{r}^2_{jt'} + \frac{1}{2}\sum_{t'=t}^{T}\overline{\lambda}_{-jt'}\delta_{-jt'}, \\
    \overline{\pi}_{jt}  &= \frac{\exp\left(-\frac{1}{2}\sum_{t'=1}^{t-1}\overline{\lambda}_{-jt'}\delta_{-jt'}\right)\pi_{jt}\Gamma(\overline{u}_{jt})\overline{v}_{jt}^{-\overline{u}_{jt}}\overline{\omega}_{jt}^{-\frac{1}{2}}\exp\left( -\frac{1}{2} \sum_{t'=1}^{t-1} \overline{\lambda}_{-jt'}\tilde{r}_{-jt'}^2 \right)}{\sum_{t'=1}^T \exp\left(-\frac{1}{2}\sum_{t''=1}^{t'-1}\overline{\lambda}_{-jt''}\delta_{-j't''}\right) \pi_{jt'}\Gamma(\overline{u}_{jt'})\overline{v}_{jt'}^{-\overline{u}_{jt'}}\overline{\omega}_{jt'}^{-\frac{1}{2}}\exp\left( -\frac{1}{2} \sum_{t''=1}^{t'-1} \overline{\lambda}_{-jt''}\tilde{r}_{-jt''}^2\right)} \\
    &= \frac{\tilde{\pi}_{jt}\Gamma(\overline{u}_{jt})\overline{v}_{jt}^{-\overline{u}_{jt}}\overline{\omega}_{jt}^{-\frac{1}{2}}\exp\left( -\frac{1}{2} \sum_{t'=1}^{t-1} \overline{\lambda}_{-jt'}\tilde{r}_{-jt'}^2 \right)}{\sum_{t'=1}^T \tilde{\pi}_{jt'}\Gamma(\overline{u}_{jt'})\overline{v}_{jt'}^{-\overline{u}_{jt'}}\overline{\omega}_{jt'}^{-\frac{1}{2}}\exp\left( -\frac{1}{2} \sum_{t''=1}^{t'-1} \overline{\lambda}_{-jt''}\tilde{r}_{-jt''}^2\right)}.
\end{align*}
Examining the meanvar-scp posterior parameters (\ref{eq:meanvar-scp-post-omega})-(\ref{eq:meanvar-scp-post-pi}), it is clear that:
\begin{align*}
    \overline{\boldsymbol{\theta}}_j := \normalfont{\texttt{meanvar-scp}}(\tilde{\mathbf{r}}_{-j,1:T} \:;\:\overline{\boldsymbol{\lambda}}_{-j,1:T}, \omega_j, u_j, \tilde{\mathbf{v}}_{j,1:T}, \tilde{\boldsymbol{\pi}}_{j,1:T}).
\end{align*}

\end{enumerate}
\end{proof}