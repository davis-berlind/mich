\subsection{Convergence Criterion}
\label{app:convergence}

Because Algorithm \ref{alg:1} defines a coordinate ascent procedure for maximizing the ELBO (\ref{eq:elbo}), then the ELBO is strictly increases after each iteration of Algorithm \ref{alg:1}. Therefore, one possibility for a stopping rule is to pick an error tolerance $\varepsilon$ and terminate Algorithm \ref{alg:1} when the increase in the ELBO falls below $\varepsilon$. Examining the ELBO, at the end of each iteration of Algorithm \ref{alg:1} we have: 
\scriptsize
\begin{align*} 
    \text{ELBO}(q\:;\mu_0,\lambda_0) &= \frac{T + B_l + B_r}{2} \log \lambda_0 + \frac{1}{2}\sum_{t=1}^T \left[\sum_{j=1}^J \E_{q_j}[\log \lambda_{jt}]+ \sum_{k=1}^K \E_{q_k}[\log \lambda_{kt}]\right] - \frac{1}{2} \sum_{t=1-B_l}^{T+B_r} \E_q[\lambda_t(y_t -\mu_t)^2] \\
    &\quad - \sum_{j=1}^J \text{KL}(q_j\:\lVert\: p_j) - \sum_{\ell=1}^L \text{KL}(q_\ell\:\lVert\: p_\ell) - \sum_{k=1}^K \text{KL}(q_k\:\lVert\: p_k) +C \\
    &= \frac{T + B_l + B_r}{2} \log \lambda_0 + \frac{1}{2}\sum_{t=1}^T \left[\sum_{j=1}^J (T+B_r-t+1)\overline{\pi}_{jt}[\psi(\overline{u}_{jt}) - \log\overline{v}_{jt}]+ \sum_{k=1}^K(T+B_r-t+1)\overline{\pi}_{kt}[\psi(\overline{u}_{kt}) - \log\overline{v}_{kt}]\right] \\
    &\quad- \frac{1}{2} \sum_{t=1-B_l}^{T+B_r} \overline{\lambda}_t(\Tilde{r}_t^2 + \delta_t) - \sum_{j=1}^J \text{KL}(q_j\:\lVert\: p_j) - \sum_{\ell=1}^L \text{KL}(q_\ell\:\lVert\: p_\ell) - \sum_{k=1}^K \text{KL}(q_k\:\lVert\: p_k) +C
\end{align*}
\normalsize
where $C$ is a constant that does not change between iterations, $\psi(x)$ is the digamma function, and $\Tilde{r}_t$ is defined as in (\ref{eq:mod-resid}) except with $\mu_0$ also subtracted off, $\overline{\lambda}_t$ is defined as in (\ref{eq:lambda-bar}) except multiplied by $\lambda_0$, and $\delta_t$ is defined as in (\ref{eq:delta}). Each of these quantities as well as the KL divergences are functions of the parameters: $$\{\mu_0,\lambda_0,\{\{\overline{b}_{jt}, \overline{\tau}_{jt}, \overline{u}_{jt}, \overline{v}_{jt}, \overline{\pi}_{jt}\}_{j=1}^J, \{\overline{b}_{\ell t}, \overline{\tau}_{\ell t}, \overline{\pi}_{\ell t}\}_{\ell=1}^L, \{\overline{u}_{kt}, \overline{v}_{kt}, \overline{\pi}_{kt}\}_{k=1}^K\}_{t=1}^T\}.$$ See Appendices \ref{app:prop1-proof} and \ref{app:prop2-proof} for details. Proposition \ref{prop:1} and the proof in Appendix \ref{app:empirical-bayes} guaranty the convergence of these parameters, so the ELBO is guaranteed to converge as well, i.e. the increase in the ELBO is converging to zero. In practice, we follow the example set by \cite{Wang20} and set $\varepsilon = 0.001$, which is the default stopping rule implemented in our \texttt{R} package. We can also simply monitor the convergence of these parameters after each iteration using some distance metric like the $\ell_1$- or $\ell_2$-norm, and terminate the procedure when the distance between the parameters after each iteration drops below $\varepsilon$. This stopping rule tends to be more stringent and is also implemented in our \texttt{R} package.