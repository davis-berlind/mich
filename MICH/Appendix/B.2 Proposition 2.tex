\subsection{Proof of Proposition \ref{prop:2}}
\label{app:prop2-proof}

\begin{proof}
By Proposition \ref{prop:vb}, we know that in each iteration of Algorithm \ref{alg:1}, the distributions $q_j$, $q_\ell$, and $q_k$ that respectively solve the maximization problems in (\ref{eq:elbo-j}), (\ref{eq:elbo-l}), and (\ref{eq:elbo-k}) are fully characterized by the sets of parameters:
\begin{align*}
    \overline{\theta}_j &:= \{\overline{b}_{jt}, \overline{\tau}_{jt}, \overline{u}_{jt}, \overline{v}_{jt}, \overline{\pi}_{jt}\}_{t=1}^T \\
    \overline{\theta}_\ell &:= \{\overline{b}_{\ell t}, \overline{\tau}_{\ell t}, \overline{\pi}_{\ell t}\}_{t=1}^T\\
    \overline{\theta}_k &:= \{\overline{u}_{kt}, \overline{v}_{kt}, \overline{\pi}_{kt}\}_{t=1}^T
\end{align*} 
that are outputted by the respective calls to the \texttt{SMSCP}, \texttt{SMCP}, and \texttt{SSCP} functions we specified in Propoisition \ref{prop:vb}. Suppose that for each $j$, $\ell$, and $k$, we can show that these sets of parameters are the unique solutions for the following maximization problems: 
\begin{gather*}
    \max_{\{\mathbf{b}_j, \boldsymbol{\tau}_j, \mathbf{u}_j, \mathbf{v}_j, \boldsymbol{\pi}_j\} \in \mathbb{R}^T\times\mathbb{R}_{++}^T\times\mathbb{R}_{++}^T\times\mathbb{R}_{++}^T \times\mathcal{S}^T} \; F(\{\mathbf{b}_j, \boldsymbol{\tau}_j, \mathbf{u}_j, \mathbf{v}_j, \boldsymbol{\pi}_j\}_{j=1}^J, \{\mathbf{b}_\ell, \boldsymbol{\tau}_\ell, \boldsymbol{\pi}_\ell\}_{\ell=1}^L, \{\mathbf{u}_k, \mathbf{v}_k, \boldsymbol{\pi}_k\}_{k=1}^K) \\
    \;\;\quad\quad\quad\max_{\{\mathbf{b}_\ell, \boldsymbol{\tau}_\ell, \boldsymbol{\pi}_\ell\} \in \mathbb{R}^T\times\mathbb{R}_{++}^T\mathcal{S}^T} \;\quad\quad\quad\;\; F(\{\mathbf{b}_j, \boldsymbol{\tau}_j, \mathbf{u}_j, \mathbf{v}_j, \boldsymbol{\pi}_j\}_{j=1}^J, \{\mathbf{b}_\ell, \boldsymbol{\tau}_\ell, \boldsymbol{\pi}_\ell\}_{\ell=1}^L, \{\mathbf{u}_k, \mathbf{v}_k, \boldsymbol{\pi}_k\}_{k=1}^K) \\
    \;\;\quad\quad\max_{\{\mathbf{u}_k, \mathbf{v}_k, \boldsymbol{\pi}_k\} \in \mathbb{R}_{++}^T\times\mathbb{R}_{++}^T \times\mathcal{S}^T} \;\quad\quad\;\; F(\{\mathbf{b}_j, \boldsymbol{\tau}_j, \mathbf{u}_j, \mathbf{v}_j, \boldsymbol{\pi}_j\}_{j=1}^J, \{\mathbf{b}_\ell, \boldsymbol{\tau}_\ell, \boldsymbol{\pi}_\ell\}_{\ell=1}^L, \{\mathbf{u}_k, \mathbf{v}_k, \boldsymbol{\pi}_k\}_{k=1}^K) 
\end{gather*}
where the objective function $F$ is  continuously differentiable in each parameter. Then by Proposition 2.7.1 of \cite{Bertsekas97}, the sequences of parameters $\overline{\theta}_j$, $\overline{\theta}_\ell$, and $\overline{\theta}_k$ generated by each iteration of Algorithm \ref{alg:1} will converge to a stationary point. Therefore, if the stopping rule for Algorithm \ref{alg:1} depends only on whether the iterates of these parameters converge (see Appendix \ref{app:convergence} to see that this is in fact the case), then convergence of the procedure as a whole is guaranteed. 

It now only remains to show that the parameters we specify for the SMSCP, SMCP, and SSCP distributions in Proposition \ref{prop:vb} are in fact the unique block-coordinate maximizers for some continuously differentiable $F$. To see this, we begin by noting that under the mean-field assumption in (\ref{eq:mean-field}), if $q \in \mathcal{Q}$ then the ELBO can be rewritten as:
\begin{align}
    \text{ELBO}(q\:;\mu_0,\lambda_0) &= \E_q[\log p(\mathbf{y}| \theta\:; \mu_0,\lambda_0)] - \sum_{j=1}^J \text{KL}(q_j\:\lVert\: p_j) - \sum_{\ell=1}^L \text{KL}(q_\ell\:\lVert\: p_\ell) - \sum_{k=1}^K \text{KL}(q_k\:\lVert\: p_k). \label{eq:elbo-full}
\end{align}
Suppose that we choose each $q_j$, $q_\ell$, and $q_k$ so that:
\begin{align*}
    \theta_j &\sim \text{SMSCP}(\mathbf{b}_j, \boldsymbol{\tau}_j, \mathbf{u}_j, \mathbf{v}_j, \boldsymbol{\pi}_j)\\
    \theta_\ell &\sim \text{SMCP}(\mathbf{b}_\ell, \boldsymbol{\tau}_\ell, \boldsymbol{\pi}_\ell)\\
    \theta_k &\sim \text{SSCP}(\mathbf{u}_k, \mathbf{v}_k, \boldsymbol{\pi}_k). 
\end{align*}
Then $\text{ELBO}(q\:;\mu_0,\lambda_0)$ will be a continuously differentiable function of the parameters of interest, so we use $\text{ELBO}(q\:;\mu_0,\lambda_0)$ as our objective function $F$ with this restriction on $q$.

\begin{enumerate}[label=\roman*.]

\item To avoid notational confusion in this section, we use $\tau_0$ and $\{\pi_{0t}\}_{t=1}^T$ to refer to the prior parameters for $\theta_\ell$ in the MICH model. We want to show $\overline{\theta}_\ell$ uniquely solves: 
\begin{align}
    \max_{\{\mathbf{b}_\ell, \boldsymbol{\tau}_\ell, \boldsymbol{\pi}_\ell\} \in \mathbb{R}^T\times\mathbb{R}_{++}^T\mathcal{S}^T} \;  \text{ELBO}(q\:;\mu_0,\lambda_0) \label{eq:elbo-l-param}
\end{align}
Since only the log-evidence and $\text{KL}(q_\ell\:\lVert\: p_\ell)$ depend on $\{\mathbf{b}_\ell, \boldsymbol{\tau}_\ell, \boldsymbol{\pi}_\ell\}$ in (\ref{eq:elbo-full}), the maximization problem in (\ref{eq:elbo-l-param}) is equivalent to solving: 
\begin{align*}
    \max_{\{\mathbf{b}_\ell, \boldsymbol{\tau}_\ell, \boldsymbol{\pi}_\ell\} \in \times\mathbb{R}_{++}^T\times\mathbb{R}_{++}^T \times\mathcal{S}^T} \;  \E_q[\log p(\mathbf{y}| \theta\:; \mu_0,\lambda_0)] - \text{KL}(q_\ell\:\lVert\: p_\ell).
\end{align*}
Since $q \in \mathcal{Q}$, we can rewrite the objective function above as:
\begin{align*}
    \E_{q_\ell}\{\E_{q_{-\ell}}[\log p(\mathbf{y}| \theta\:; \mu_0,\lambda_0)+\log p_\ell(\theta_\ell)]\} - \E_{q_\ell}[\log q_\ell(\theta_\ell)].
\end{align*}
In the proof of part ii of Proposition \ref{prop:vb} we showed that the term $\E_{q_{-\ell}}[\log p(\mathbf{y}| \theta\:; \mu_0,\lambda_0)+\log p_\ell(\theta_\ell)]$ is equal to:
\begin{align*}
     \sum_{t=1}^T \mathbbm{1}\{\gamma_\ell = t\}\left[\frac{1}{2} \log \overline{\tau}_{\ell t} - \frac{\overline{\tau}_{\ell t}(b_{\ell } - \overline{b}_{\ell t})^2}{2} + \log \overline{\pi}_{\ell t}\right] + C
\end{align*}
where $C$ is some constant that does not depend on $\theta_\ell$. Note that the assumption in Proposition \ref{prop:2} that $\pi_{0t} > 0 \sforall t$ guarantees that $\overline{\pi}_{\ell t} > 0$ and thus that $\log \overline{\pi}_{\ell t}$ is well-defined and finite. Next, our choice of $q_\ell$ implies the following identities:
\begin{align*}
    \E_{q_\ell}[\mathbbm{1}\{\gamma_\ell = t\}] &= \pi_{\ell t} \\
    \E_{q_\ell}[(b_\ell - b_{\ell t})^2\;|\;\gamma_\ell=t] &= \tau_{\ell t}^{-1}\\
    \E_{q_\ell}[(b_\ell - \overline{b}_{\ell t})^2\;|\;\gamma_\ell=t] &= \tau_{\ell t}^{-1} + (b_{\ell t}- \overline{b}_{\ell t})^2
\end{align*}
So we get that the term $\E_{q_\ell}\{\E_{q_{-\ell}}[\log p(\mathbf{y}| \theta\:; \mu_0,\lambda_0)+\log p_\ell(\theta_\ell)]\}$ is equal to:
\begin{align*}
    \sum_{t=1}^T \pi_{\ell t}\left[ \frac{1}{2}\log \overline{\tau}_{\ell t} - \frac{\overline{\tau}_{\ell t}}{2\tau_{\ell t}}- \frac{\overline{\tau}_{\ell t}(b_{\ell t}  - \overline{b}_{\ell t})^2}{2} + \log \overline{\pi}_{\ell t}\right] + C
\end{align*}
where again $C$ is some constant that does not depend on the parameters $\{\mathbf{b}_\ell, \boldsymbol{\tau}_\ell, \boldsymbol{\pi}_\ell\}$. By our choice of $q_\ell$, we also have that:
\begin{align*}
    \E_{q_\ell}[\log q_\ell(\theta_\ell)] &= \E_{q_\ell}\left[\sum_{t=1}^T \mathbbm{1}\{\gamma_\ell = t\}\left[\frac{1}{2}\log \tau_{\ell t} - \frac{\tau_{\ell t}(b_\ell - b_{\ell t})^2}{2} + \log \pi_{\ell t}\right]\right] - \frac{1}{2} \log 2\pi  \\
    &= \sum_{t=1}^T \E_{q_\ell}\left[\mathbbm{1}\{\gamma_\ell = t\}\left[\frac{1}{2}\log \tau_{\ell t} - \frac{\tau_{\ell t}\E_{q_\ell}\left[(b_\ell - b_{\ell t})^2\:|\: \gamma_{\ell} = t\right]}{2} + \log \pi_{\ell t}\right]\right] - \frac{1}{2} \log 2\pi  \\
    &= \sum_{t=1}^T \overline{\pi}_{\ell t}\left[\frac{1}{2}\log \tau_{\ell t} + \log \pi_{\ell t}\right] - \frac{1}{2} - \frac{1}{2} \log 2\pi 
\end{align*}
Therefore, the objective function in (\ref{eq:elbo-l-param}) is equal to:
\begin{align}
     \sum_{t=1}^T \pi_{\ell t}\left[ \frac{1}{2}\log \frac{\overline{\tau}_{\ell t}}{\tau_{\ell t}} - \frac{\overline{\tau}_{\ell t}}{2\tau_{\ell t}} - \frac{\overline{\tau}_{\ell t}(b_{\ell t}  - \overline{b}_{\ell t})^2}{2} + \log \frac{\overline{\pi}_{\ell t} }{\pi_{\ell t}}\right]+C.\label{eq:objective-l}
\end{align}
By the assumption in Proposition \ref{prop:2} that $\tau_{0} > 0$, we have $\overline{\tau}_{\ell t} > 0 \sforall t$, and thus for each $t$, the term:
\begin{align*}
    - \frac{\overline{\tau}_{\ell t}(b_{\ell t}  - \overline{b}_{\ell t})^2}{2}
\end{align*}
is a strictly concave as a function of $b_{\ell t}$ and is uniquely maximized by setting $b_{\ell t} = \overline{b}_{\ell t} \sforall t$. Plugging the optimal value of $b_{\ell t}$ into (\ref{eq:objective-l}), the objective function becomes: 
\begin{align*}
     \sum_{t=1}^T \pi_{\ell t}\left[\frac{1}{2} \log \frac{\overline{\tau}_{\ell t}}{\tau_{\ell t}} - \frac{\overline{\tau}_{\ell t}}{2\tau_{\ell t}} + \log \frac{\overline{\pi}_{\ell t} }{\pi_{\ell t}}\right]+C
\end{align*}
Note that the term in the square brackets only depends on $\tau_{\ell t}$ through:
\begin{align*}
    f_\tau(\tau_{\ell t}) :=  \frac{\overline{\tau}_{\ell t}}{2\tau_{\ell t}} - \log \tau_{\ell t}.
\end{align*}
We have:
\begin{align*}
    f'_\tau(\tau) = \frac{\overline{\tau}_{\ell t}}{2 \tau_{\ell t}} - \frac{1}{2\tau_{\ell t}}.
\end{align*}
Again, because $\overline{\tau}_{\ell t} > 0$, then for $\tau \in (0,\infty)$ we have:
\begin{align*}
    f'_\tau(\tau)
    \begin{cases}
        > 0, & \text{if } \tau < \overline{\tau}_{\ell t}, \\ 
        = 0, & \text{if } \tau = \overline{\tau}_{\ell t}, \\  
        < 0, & \text{if } \tau > \overline{\tau}_{\ell t}.
    \end{cases}
\end{align*}
In words, $f_\tau(\tau)$ is strictly increasing for $0 < \tau < \overline{\tau}_{\ell t}$, and strictly decreasing for $\tau > \overline{\tau}_{\ell t}$, telling us that $f_\tau(\tau)$ is uniquely maximized at $\tau = \overline{\tau}_{\ell t}$. Plugging this value for $\tau_{\ell t}$ and the optimal value for $b_{\ell t}$, the objective function in (\ref{eq:objective-l}) becomes:
\begin{align*}
    \sum_{t=1}^T \pi_{\ell t} \log \frac{\overline{\pi}_{\ell t}}{\pi_{\ell t}} +C.
\end{align*}
Defining:
\begin{align*}
    f_\pi(\pi_{\ell t}):=\pi_{\ell t} \log \frac{\overline{\pi}_{\ell t}}{\pi_{\ell t}}
\end{align*}
we have
\begin{align*}
    f''_\pi(\pi):= -\frac{1}{\pi} < 0 \sforall \pi \geq 0
\end{align*}
so $f_\pi(\pi)$ is strictly concave on $[0,1]$. Therefore, $\sum_{t=1}^T f_\pi(\pi_{\ell t})$ is strictly concave as a function of $\boldsymbol{\pi}_\ell$ on the closed, convex set $\mathcal{S}^T$, and will therefore have a unique maximizer. Setting up the Lagrangian, we get:
\begin{align*}
    \max_{\boldsymbol{\pi}_\ell} \; \sum_{t=1}^T \pi_{\ell t} \log \frac{\overline{\pi}_{\ell t}}{\pi_{\ell t}} - \alpha \left(1 - \sum_{t=1}^T \pi_{\ell t}\right) 
\end{align*}
where $\alpha$ is the Lagrange multiplier. We now have the following first order conditions:
\begin{align*}
    \log \frac{\overline{\pi}_{\ell t}}{\pi_{\ell t}}  -1 + \alpha &=0, \sforall t \in \{1, \ldots, T\} \\
    1 - \sum_{t=1}^T \pi_{\ell t} &= 0.
\end{align*}
The first condition implies $\pi_{\ell t} = e^{1- \lambda}\overline{\pi}_{\ell t}$ for each $t$. Plugging this optimal $\pi_{\ell t}$ into the second condition implies:
\begin{align*}
   1 = \sum_{t=1}^T \pi_{\ell t} = \sum_{t=1}^T \pi_{\ell t} e^{1- \lambda}\overline{\pi}_{\ell t} =  e^{1- \lambda}
\end{align*}
and thus $\lambda=1$, i.e. $\pi_{\ell t} = \overline{\pi}_{\ell t} \sforall t$ is the unique solution to the maximization problem. Note that above we showed that $\overline{b}_{\ell t}$ and $\overline{\tau}_{\ell t}$ are the unique maximizers of the term in the square brackets of (\ref{eq:objective-l}), but they will only be the unique maximizers of the actual objective function in (\ref{eq:objective-l}) if $\pi_{\ell t} > 0$, since if $\pi_{\ell t}=0$ we have:
\begin{align*}
     \pi_{\ell t}\log \frac{\overline{\pi}_{\ell t}}{\pi_{\ell t}} = \pi_{\ell t}\left[ \frac{1}{2}\log \frac{\overline{\tau}_{\ell t}}{\tau_{\ell t}} - \frac{\overline{\tau}_{\ell t}}{2\tau_{\ell t}} - \frac{\overline{\tau}_{\ell t}(b_{\ell t}  - \overline{b}_{\ell t})^2}{2} + \log \frac{\overline{\pi}_{\ell t} }{\pi_{\ell t}}\right]
\end{align*}
for any choice of $b_{\ell t}, \tau_{\ell t} >0$. To demonstrate the joint uniqueness of the parameters, assume that there is some other parameter set $\{b'_{\ell t}, \tau'_{\ell t}, \pi'_{\ell t}\}_{t=1}^T$ that solves the maximization problem in (\ref{eq:elbo-l-param}). Suppose that there is some $t'$ such that $b'_{\ell t'} \neq \overline{b}_{\ell t'}$ or $\tau'_{\ell t'}\neq \overline{\tau}_{\ell t'}$, then by the uniqueness results on $\overline{b}_{\ell t'}$ and $\overline{\tau}_{\ell t'}$ above, we must have $\pi'_{\ell t'} = 0$. But by the assumption in Proposition \ref{prop:2} that $\pi_{0t} >0 \sforall t$, we have $\overline{\pi}_{\ell t} > 0 \sforall t$, and thus $\pi'_{\ell t'} \neq \overline{\pi}_{\ell t}$. Using the fact that $\pi'_{\ell t} > 0 \implies \{b'_{\ell t}, \tau'_{\ell t}\} = \{\overline{b}_{\ell t}, \overline{\tau}_{\ell t}\}$, then plugging $\{b'_{\ell t}, \tau'_{\ell t}, \pi'_{\ell t}\}_{t=1}^T$ into the objective function we get:
\begin{align*}
    \sum_{t=1}^T \pi'_{\ell t} \log \frac{\overline{\pi}_{\ell t}}{\pi'_{\ell t}} < \sum_{t=1}^T \overline{\pi}_{\ell t} \log \frac{\overline{\pi}_{\ell t}}{\overline{\pi}_{\ell t}} 
\end{align*}
where the strict inequality follows from the fact that $\pi'_{\ell t'} \neq \overline{\pi}_{\ell t}$, and $\overline{\boldsymbol{\pi}}_\ell$ is the unique maximizer of $\sum_{t=1}^T f_\pi(\pi_{\ell t})$. So we must have $ \{b'_{\ell t}, \tau'_{\ell t}\} = \{\overline{b}_{\ell t}, \overline{\tau}_{\ell t}\} \sforall t$. But we just showed that when this is the case, (\ref{eq:elbo-l-param}) is uniquely maximized by setting $\pi_{\ell t} = \overline{\pi}_{\ell t} \sforall t$, which establishes $\overline{\theta}_\ell$ as the unique solution to (\ref{eq:elbo-l-param}), as desired.

\item To avoid notational confusion in this section, we use $u_0$, $v_0$, and $\{\pi_{0t}\}_{t=1}^T$ to refer to the prior parameters for $\theta_k$ in the MICH model. We want to show $\overline{\theta}_k$ uniquely solves: 
\begin{align}
    \max_{\{\mathbf{u}_k, \mathbf{v}_k, \boldsymbol{\pi}_k\} \in \mathbb{R}_{++}^T\times\mathbb{R}_{++}^T \times\mathcal{S}^T} \;  \text{ELBO}(q\:;\mu_0,\lambda_0) \label{eq:elbo-k-param}
\end{align}
Again, only the log-evidence and $\text{KL}(q_k\:\lVert\: p_k)$ depend on $\{\mathbf{u}_k, \mathbf{v}_k, \boldsymbol{\pi}_k\}$ in (\ref{eq:elbo-full}), the maximization problem in (\ref{eq:elbo-k-param}) is equivalent to solving: 
\begin{align*}
    \max_{\{\mathbf{u}_k, \mathbf{v}_k, \boldsymbol{\pi}_k\} \in \mathbb{R}_{++}^T\times\mathbb{R}_{++}^T \times\mathcal{S}^T} \;  \E_q[\log p(\mathbf{y}| \theta\:; \mu_0,\lambda_0)] - \text{KL}(q_k\:\lVert\: p_k).
\end{align*}
Since $q \in \mathcal{Q}$, we can rewrite the objective function above as:
\begin{align*}
    \E_{q_k}\{\E_{q_{-k}}[\log p(\mathbf{y}| \theta\:; \mu_0,\lambda_0)+\log p_k(\theta_k)]\} - \E_{q_k}[\log q_k(\theta_k)].
\end{align*}
In the proof of part iii of Proposition \ref{prop:vb} we showed that the term $\E_{q_{-k}}[\log p(\mathbf{y}| \theta\:; \mu_0,\lambda_0)+\log p_k(\theta_k)]$ is equal to:
\begin{align*}
     \sum_{t=1}^T \mathbbm{1}\{\gamma_k = t\}\left[ \overline{u}_{kt} \log \overline{v}_{kt} -  \log\Gamma(\overline{u}_{kt}) + \left(\overline{u}_{kt} -1\right)\log s_{k}  - \overline{v}_{kt} s_{k} + \log \overline{\pi}_{kt}\right] + C
\end{align*}
where $C$ is some constant that does not depend on $\theta_k$. Once again, the assumption in Proposition \ref{prop:2} that $\pi_{0t} > 0 \sforall t$ ensures that $\overline{\pi}_{kt} > 0$ and that $\log \overline{\pi}_{kt}$ is well-defined and finite. Next, our choice of $q_k$ implies the following identities:
\begin{align*}
    \E_{q_k}[\mathbbm{1}\{\gamma_k = t\}] &= \pi_{kt} \\
    \E_{q_k}[s_{k}\;|\;\gamma_k=t] &= \frac{u_{kt}}{v_{kt}} \\
    \E_{q_k}[\log s_{k}\;|\;\gamma_k=t] &= \psi(u_{kt}) - \log v_{kt}
\end{align*}
where $\psi(x)$ is the digamma function. So we get that the term $\E_{q_k}\{\E_{q_{-k}}[\log p(\mathbf{y}| \theta\:; \mu_0,\lambda_0)+\log p_k(\theta_k)]\}$ is equal to:
\begin{align*}
    \sum_{t=1}^T \pi_{kt}\left[ \overline{u}_{kt} \log \overline{v}_{kt} -  \log\Gamma(\overline{u}_{kt}) + \left(\overline{u}_{kt} -1\right)(\psi(u_{kt}) - \log v_{kt}) - \frac{\overline{v}_{kt} u_{kt}}{v_{kt}} + \log \overline{\pi}_{kt}\right] + C
\end{align*}
where again $C$ is some constant that does not depend on the parameters $\{\mathbf{u}_k, \mathbf{v}_k, \boldsymbol{\pi}_k\}$. By our choice of $q_k$, we also have that:
\begin{align*}
    \E_{q_k}[\log q_k(\theta_k)] = \sum_{t=1}^T \pi_{kt}\left[ u_{kt} \log v_{kt} -  \log\Gamma(u_{kt}) + \left(u_{kt} -1\right)(\psi(u_{kt}) - \log v_{kt}) - u_{kt} + \log \pi_{kt}\right].
\end{align*}
Therefore, the objective function in (\ref{eq:elbo-k-param}) is equal to:
\small
\begin{align}
    \sum_{t=1}^T \pi_{kt}\left[ \overline{u}_{kt} \log \overline{v}_{kt} - \log\Gamma(\overline{u}_{kt}) + \log\Gamma(u_{kt}) + \left(\overline{u}_{kt} -u_{kt}\right)\psi(u_{kt}) - \overline{u}_{kt} \log v_{kt} + u_{kt}\left(1 - \frac{\overline{v}_{kt} }{v_{kt}}\right) + \log \frac{\overline{\pi}_{kt}}{\pi_{kt}}\right] +C.\label{eq:objective-k}
\end{align}
\normalsize
Note that the terms in square brackets in (\ref{eq:objective-k}) only depend on $v_{kt}$ through the term: 
\begin{align*}
     f_v(v_{kt}) := u_{kt}\left(1- \frac{\overline{v}_{kt} }{v_{kt}}\right) - \overline{u}_{kt} \log v_{kt}.
\end{align*}
We have:
\begin{align*}
    f'_v(v) = \frac{\overline{v}_{kt} u_{kt}}{v^2} -\frac{\overline{u}_{kt}}{v}.
\end{align*}
Because $u_0, v_0 > 0$ by the assumptions of Proposition \ref{prop:2}, then $\overline{u}_{kt},\overline{v}_{kt} > 0$, so for fixed $u_{kt} >0$, for $v\in(0,\infty)$ we get:
\begin{align*}f'_v(v)
    \begin{cases}
        > 0, & \text{if } v < \frac{\overline{v}_{kt} u_{kt}}{\overline{u}_{kt}}, \\ 
        = 0, & \text{if } v = \frac{\overline{v}_{kt} u_{kt}}{\overline{u}_{kt}}, \\  
        < 0, & \text{if } v > \frac{\overline{v}_{kt} u_{kt}}{\overline{u}_{kt}}.
    \end{cases}
\end{align*}
In words, $f_v(v)$ is strictly for $0<v < \frac{\overline{v}_{kt} u_{kt}}{\overline{u}_{kt}}$, and strictly decreasing for $v > \frac{\overline{v}_{kt} u_{kt}}{\overline{u}_{kt}}$, telling us that $f_v(v)$ is uniquely maximized at $v = \frac{\overline{v}_{kt} u_{kt}}{\overline{u}_{kt}}$. Plugging this value for $v_{kt}$ into the term in square brackets in (\ref{eq:objective-k}) we get:
\begin{align*}
    -  \log\Gamma(\overline{u}_{kt}) + \log\Gamma(u_{kt}) + \left(\overline{u}_{kt} -u_{kt}\right)\psi(u_{kt}) - \overline{u}_{kt} \log u_{kt} + \overline{u}_{kt} \log\overline{u}_{kt}  + \left(u_{kt}- \overline{u}_{kt}\right) + \log \frac{\overline{\pi}_{kt}}{\pi_{kt}}
\end{align*}
which depends on $u_{kt}$ through the term:
\begin{align*}
    f_u(u_{kt}) := \log\Gamma(u_{kt}) + \left(\overline{u}_{kt} -u_{kt}\right)\psi(u_{kt}) - \overline{u}_{kt} \log u_{kt} + u_{kt}.
\end{align*}
We have:
\begin{align*}
    f'_u(u) = \left(\overline{u}_{kt} -u\right)\psi'(u) - \frac{\overline{u}_{kt}}{u} + 1
\end{align*}
We can use the following bounds for the trigamma function when $u>0$:
\begin{align*}
    \frac{1}{u} + \frac{1}{2u^2} \leq \psi'(u) \leq \frac{1}{u} + \frac{1}{u^2}
\end{align*}
to get:
\begin{align*}
    \frac{\overline{u}_{kt} -u}{2u^2} \leq f'_u(u) \leq \frac{\overline{u}_{kt} -u}{u^2}.
\end{align*}
Therefore, we have:
\begin{align*}
f'_u(u)
    \begin{cases}
        > 0, & \text{if } u < \overline{u}_{kt}, \\ 
        = 0, & \text{if } u = \overline{u}_{kt}, \\  
        < 0, & \text{if } u > \overline{u}_{kt}.
    \end{cases}
\end{align*}
So $f_u(u)$ is uniquely maximized at $u = \overline{u}_{kt}$ on $(0,\infty)$. Plugging in this value for $u_{kt}$ along with the optimal value for $v_{kt}$ into the term in square brackets in (\ref{eq:objective-k}), the objective function in (\ref{eq:objective-k}) becomes:
\begin{align*}
    \sum_{t=1}^T \pi_{kt} \log \frac{\overline{\pi}_{kt}}{\pi_{kt}} +C.
\end{align*}
As was the case in part i above, $\pi_{kt} = \overline{\pi}_{kt} \sforall t$ is the unique maximizer of the above expression. Again we note that the uniqueness of $\overline{u}_{kt}$ and $\overline{v}_{kt}$ as the maximizers of (\ref{eq:objective-k}) was only guaranteed when $\pi_{kt} > 0$. But once more by the assumption in Proposition \ref{prop:2} that $\pi_{0t} >0 \sforall t$, we have $\overline{\pi}_{kt} > 0 \sforall t$, so we can use the same argument from part i to show that $\overline{\theta}_k$ is the unique solution to (\ref{eq:elbo-k-param}), as desired.

\item As we did in parts i and ii above, we use $\tau_0$, $u_0$, $v_0$, and $\{\pi_{0t}\}_{t=1}^T$ to refer to the prior parameters for $\theta_j$ in the MICH model. We want to show $\overline{\theta}_j$ uniquely solves: 
\begin{align}
     \max_{\{\mathbf{b}_j, \boldsymbol{\tau}_j, \mathbf{u}_j, \mathbf{v}_j, \boldsymbol{\pi}_j\} \in \mathbb{R}^T\times\mathbb{R}_{++}^T\times\mathbb{R}_{++}^T\times\mathbb{R}_{++}^T \times\mathcal{S}^T} \;  \text{ELBO}(q\:;\mu_0,\lambda_0) \label{eq:elbo-j-param}
\end{align}
Again, only the log-evidence and $\text{KL}(q_j\:\lVert\: p_j)$ depend on $\{\mathbf{b}_j, \boldsymbol{\tau}_j, \mathbf{u}_j, \mathbf{v}_j, \boldsymbol{\pi}_j\}$ in (\ref{eq:elbo-full}), so the maximization problem in (\ref{eq:elbo-j-param}) is equivalent to solving: 
\begin{align*}
     \max_{\{\mathbf{b}_j, \boldsymbol{\tau}_j, \mathbf{u}_j, \mathbf{v}_j, \boldsymbol{\pi}_j\} \in \mathbb{R}^T\times\mathbb{R}_{++}^T\times\mathbb{R}_{++}^T\times\mathbb{R}_{++}^T \times\mathcal{S}^T} \;  \E_q[\log p(\mathbf{y}| \theta\:; \mu_0,\lambda_0)] - \text{KL}(q_j\:\lVert\: p_j).
\end{align*}
Since $q \in \mathcal{Q}$, we can rewrite the objective function above as:
\begin{align*}
    \E_{q_j}\{\E_{q_{-j}}[\log p(\mathbf{y}| \theta\:; \mu_0,\lambda_0)+\log p_j(\theta_j)]\} - \E_{q_j}[\log q_j(\theta_k)].
\end{align*}
In the proof of part iii of Proposition \ref{prop:vb} we showed that the term $\E_{q_{-j}}[\log p(\mathbf{y}| \theta\:; \mu_0,\lambda_0)+\log p_j(\theta_j)]$ is equal to:
\small
\begin{align*}
     \sum_{t=1}^T \mathbbm{1}\{\gamma_j = t\}\left[ \frac{1}{2} \log \overline{\tau}_{j t} - \frac{\overline{\tau}_{\ell t}s_j(b_j - \overline{b}_{jt})^2}{2} + \overline{u}_{jt} \log \overline{v}_{jt} -  \log\Gamma(\overline{u}_{jt}) + \left(\overline{u}_{jt} -\frac{1}{2}\right)\log s_j  - \overline{v}_{jt} s_j + \log \overline{\pi}_{jt}\right] + C
\end{align*}
\normalsize
where $C$ is some constant that does not depend on $\theta_j$. Note that the assumption in Proposition \ref{prop:2} that $\pi_{0t} > 0 \sforall t$ guarantees that $\overline{\pi}_{jt} > 0$ and thus that $\log \overline{\pi}_{jt}$ is well-defined and finite. Next, our choice of $q_j$ implies the following identities:
\begin{align*}
    \E_{q_j}[\mathbbm{1}\{\gamma_j = t\}] &= \pi_{jt} \\
    \E_{q_j}[s_j\;|\;\gamma_j=t] &= \frac{u_{jt}}{v_{jt}} \\
    \E_{q_j}[\log s_j\;|\;\gamma_j=t] &= \psi(u_{jt}) - \log v_{jt} \\
    \E_{q_j}[b_j\;|\;s_j,\gamma_j=t] &= b_{jt} \\
    \E_{q_j}[(b_j - b_{jt})^2 \;|\;s_j,\gamma_j=t] &= (s_j \tau_{jt})^{-1} \\
    \E_{q_j}[(b_j - \overline{b}_{jt})^2 \;|\;s_j,\gamma_j=t] &= (s_j \tau_{jt})^{-1} + (b_{jt} - \overline{b}_{jt})^2
\end{align*}
So we get that the term $\E_{q_j}\{\E_{q_{-j}}[\log p(\mathbf{y}| \theta\:; \mu_0,\lambda_0)+\log p_j(\theta_j)]\}$ is equal to:
\scriptsize
\begin{align*}
    \sum_{t=1}^T \pi_{jt}\left[\frac{1}{2}\log \overline{\tau}_{j t} - \frac{\overline{\tau}_{j t}}{2\tau_{j t}}- \frac{\overline{\tau}_{j t}u_{jt}(b_{j t}  - \overline{b}_{j t})^2}{2v_{jt}} + \overline{u}_{jt} \log \overline{v}_{jt} -  \log\Gamma(\overline{u}_{jt}) + \left(\overline{u}_{jt} -\frac{1}{2}\right)(\psi(u_{jt}) - \log v_{jt}) - \frac{\overline{v}_{jt} u_{jt}}{v_{jt}} + \log \overline{\pi}_{jt}\right] + C
\end{align*}
\normalsize
where again $C$ is some constant that does not depend on the parameters $\{\mathbf{b}_j, \boldsymbol{\tau}_j, \mathbf{u}_j, \mathbf{v}_j, \boldsymbol{\pi}_j\}$. By our choice of $q_j$, we also have that:
\small
\begin{align*}
    \E_{q_j}[\log q_j(\theta_j)] = \sum_{t=1}^T \pi_{jt}\left[ \frac{1}{2}\log \tau_{jt} + u_{jt} \log v_{jt} -  \log\Gamma(u_{jt}) + \left(u_{jt} -\frac{1}{2}\right)(\psi(u_{jt}) - \log v_{jt}) - u_{kt} + \log \pi_{jt}\right] + C.
\end{align*}
\normalsize
Therefore, the objective function in (\ref{eq:elbo-j-param}) is equal to:
\small
\begin{align}
    &\quad\sum_{t=1}^T \pi_{jt}\left[\frac{1}{2}\log \frac{\overline{\tau}_{j t}}{\tau_{jt}} - \frac{\overline{\tau}_{j t}}{2\tau_{j t}}- \frac{\overline{\tau}_{j t}u_{jt}(b_{j t}  - \overline{b}_{j t})^2}{2v_{jt}}\right] \notag\\
    &+ \sum_{t=1}^T \pi_{jt}\left[\overline{u}_{jt} \log \overline{v}_{jt} -  \log\Gamma(\overline{u}_{jt}) + \log\Gamma(u_{jt}) + \left(\overline{u}_{jt} -u_{jt}\right)\psi(u_{jt}) - \overline{u}_{jt}\log v_{jt} + u_{jt}\left(1- \frac{\overline{v}_{jt} }{v_{jt}}\right)\right] \notag \\ \label{eq:objective-j}
    &+ \sum_{t=1}^T \pi_{jt} \log \overline{\pi}_{jt}+ C. 
\end{align}
\normalsize
Focusing on the term:
\begin{align*}
    - \frac{\overline{\tau}_{j t}u_{jt}(b_{j t}  - \overline{b}_{j t})^2}{2v_{jt}}
\end{align*}
we again have that $\tau_0 > 0$ by the assumption of Proposition \ref{prop:2}, we have $\overline{\tau}_{j t}$, so for any $u_{jt}, v_{jt} > 0$, this term is strictly concave as a function of $b_{j t}$ and is uniquely maximized by setting $b_{j t} = \overline{b}_{j t}$. Plugging this optimal value into (\ref{eq:objective-j}), the first summation term reduces to:
\begin{align*}
    \sum_{t=1}^T \pi_{jt}\left[\frac{1}{2}\log \frac{\overline{\tau}_{j t}}{\tau_{jt}}  - \frac{\overline{\tau}_{j t}}{2\tau_{j t}}\right] = \sum_{t=1}^T f_{\tau}(\tau_{jt})
\end{align*}
and in the proof of part i above, we showed that $f_{\tau}(\tau_{jt})$ is uniquely maximized by setting $\tau_{jt} = \overline{\tau}$. If we plug in this value for $\tau_{jt}$ as well as the optimal value for $b_{jt}$ into (\ref{eq:objective-j}), then the objective function becomes:
\small
\begin{align*}
    &\quad\sum_{t=1}^T \pi_{jt}\left[\overline{u}_{jt} \log \overline{v}_{jt} -  \log\Gamma(\overline{u}_{jt}) + \log\Gamma(u_{jt}) + \left(\overline{u}_{jt} -u_{jt}\right)\psi(u_{jt}) - \overline{u}_{jt}\log v_{jt} + u_{jt}\left(1- \frac{\overline{v}_{jt} }{v_{jt}}\right) + \log \overline{\pi}_{jt}\right] + C.
\end{align*}
\normalsize
But this is identical to the objective function in (\ref{eq:objective-k}), so by the same argument we used in part ii above (as well as the fact that the that $u_0,v_0,\pi_{0t} >0$ by the assumptions of Proposition \ref{prop:2}, implying that $\overline{u}_{jt}, \overline{v}_{jt}, \overline{\pi}_{jt} > 0$), this term is uniquely maximized by setting $\{u_{jt}, v_{jt}, \pi_{jt}\} = \{\overline{u}_{jt}, \overline{v}_{jt}, \overline{\pi}_{jt}\}$ for all $t$. Using the same global uniqueness argument we used in the proof of part i above, we have that $\overline{\theta}_j$ is the unique solution to (\ref{eq:elbo-j-param}), as desired.

\end{enumerate}
\end{proof}