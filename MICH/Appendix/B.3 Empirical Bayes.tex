\subsection{Estimating \texorpdfstring{$\mu_0$}{mu0} and \texorpdfstring{$\lambda_0$}{lambda0} with Empirical Bayes}
\label{app:empirical-bayes}

In most cases, the intercept $\mu_0$ and initial scale $\lambda_0$ terms in (\ref{eq:mu_t}) and (\ref{eq:lambda_t}) are unknown nuisance parameters that must be estimated. One approach is to just set $\mu_0 = 0$ and $\lambda_0 = 1$ in Algorithm \ref{alg:mich}. Then, if $\mu_0 \neq 0$ or $\lambda_0 \neq 1$, Algorithm \ref{alg:mich} will attempt to detect a change-point at $t=1$ and center $\mu_{t}$ and $\lambda_t$ around $\mu_0$ and $\lambda_0$ respectively for all $t$ up until the first real change-point. The problem with fitting $\mu_0$ and $\lambda_0$ this way is that it requires MICH to search for a change-point whose location is known. Searching for an additional change-point requires MICH to estimate  $\mathcal{O}(T)$ additional parameters, when we only actually needed to fit two parameters $\mu_0$ and $\lambda_0$.\footnote{E.g. in the case of $\mu_0 \neq 0$ and $\lambda_0 \neq 1$, we end up having to estimate $5T$ additional parameters $\{\overline{b}_t, \overline{\tau}_t, \overline{u}_t, \overline{v}_t, \overline{\pi}_t\}_{t=1}^T$.} Another problem with this approach is that we risk missing a real change-point, e.g. if the are $J^*>0$ true changes and we set $J=J^*$ to fit MICH, we will end up using one component of $J$ to fit $\mu_0$ and $\lambda_0$ leaving only $J^*-1$ components to fit the remaining $J^*$ true change-points. 

To circumvent these challenges, we adopt an empirical Bayes (EB) approach for estimating $\mu_0$ and $\lambda_0$. We seek the values $\mu_0$ and $\lambda_0$ that maximize the evidence $\log p(\mathbf{y}_{1:T};\mu_0,\lambda_0)$. Note that in the expression (\ref{eq:elbo}), $\text{ELBO}(q;\mu_0,\lambda_0)$ is equivalent to $\log p(\mathbf{y}_{1:T};\mu_0,\lambda_0)$ plus a term that does not depend on $\mu_0$ or $\lambda_0$. Therefore, for fixed $q$, the $\mu_0$ and $\lambda_0$ that maximize the evidence are equivalent to: 
\begin{align}
    \argmax{\{\mu_0,\lambda_0\}\in\mathbb{R}\times\mathbb{R}_{++}} \; \text{ELBO}(q\:;\mu_0,\lambda_0),
\end{align}
which in turn is equivalent to:
\begin{align}\label{eq:EB-max}
    \argmax{\{\mu_0,\lambda_0\}\in\mathbb{R}\times\mathbb{R}_{++}} \; \E_{q} \left[\log p\left(\mathbf{y}_{1:T} \:|\: \boldsymbol{\Theta};\mu_0,\lambda_0\right)\right]. 
\end{align}
\cite{Wang20} use this same approach, and as they note, optimizing the ELBO over $\mu_0$ and $\lambda_0$ constitutes the M-step of an EM algorithm where the E-step is approximate (\citealp{Dempster77, Heskes03, Neal98}) do to the use of $q$. Define $\tilde{r}_t$, $\overline{\lambda}_t$ and $\delta_t$ as in (\ref{eq:mod-resid})-(\ref{eq:delta}) along with: 
\begin{align}
    \tilde{r}_{-0t} &:= \tilde{r}_t + \mu_0 \\
    \overline{\lambda}_{-0t} &:= \lambda_0^{-1}\overline{\lambda}_t
\end{align}
then (\ref{eq:EB-max}) can be rewritten as:
\begin{align}\label{eq:EB-max-simple}
    \argmax{\{\mu_0,\lambda_0\}\in\mathbb{R}\times\mathbb{R}_{++}} \; \frac{T }{2}\log\lambda_0 - \frac{\lambda_0}{2}\sum_{t=1}^{T} \overline{\lambda}_{-0t}\left[\left(\tilde{r}_{-0t}-\mu_0\right)^2 + \delta_t\right].
\end{align}
The solutions to (\ref{eq:EB-max-simple}) is given by:
\begin{align}
    \hat{\mu}_0 &= \frac{\sum_{t=1}^{T} \overline{\lambda}_{-0t}\tilde{r}_{-0t}}{\sum_{t=1}^{T} \overline{\lambda}_{-0t}} \label{eq:EB-max-solution-mu} \\
    \hat{\lambda}_0 &= \left[\frac{\sum_{t=1}^{T} \overline{\lambda}_{-0t}[\left(\tilde{r}_{-0t}-\hat{\mu}_0\right)^2 + \delta_t]}{T}\right]^{-1}. \label{eq:EB-max-solution-lambda}
\end{align}
We incorporate the estimation step for $\mu_0$ and $\lambda_0$ into Algorithm \ref{alg:mich-eb}.

\input{MICH/Figures/EB MICH Algorithm}

Note that Algorithm \ref{alg:mich-eb} embeds Algorithm \ref{alg:mich} with the additional step of maximizing the ELBO with respect to $\mu_0$ and $\lambda_0$ while holding the approximate posterior $q$ fixed. Therefore, Algorithm \ref{alg:mich-eb} also defines a coordinate ascent procedure. The objective function in (\ref{eq:EB-max-simple}) is continuously differentiable with respect to $\mu_0$ and $\lambda_0$, so if we can show that $\hat{\mu}_0$ and $\hat{\lambda}_0$ are the unique maximizers of (\ref{eq:EB-max-simple}), then the conditions of Proposition 2.7.1 of \cite{Bertsekas97} will still hold even with the added coordinate ascent step of maximizing the ELBO with respect to $\mu_0$ and $\lambda_0$. 

To prove the uniqueness of $\hat{\mu}_0$ and $\hat{\lambda}_0$, first note that the term:
\begin{align*}
    -\frac{1}{2}\sum_{t=1}^{T} \overline{\lambda}_{-0t}\left(\tilde{r}_{-0t}-\mu_0\right)^2
\end{align*}
is strictly concave as a function of $\mu_0$ and is maximized at $\hat{\mu}_0$. Therefore, for any $\mu_0\in\mathbb{R}\setminus\{\hat{\mu}_0\}$ we have:
\begin{align*}
    -\frac{1}{2}\sum_{t=1}^{T} \overline{\lambda}_{-0t}\left(\tilde{r}_{-0t}-\hat{\mu}_0\right)^2 > -\frac{1}{2}\sum_{t=1}^{T} \overline{\lambda}_{-0t}\left(\tilde{r}_{-0t}-\mu_0\right)^2
\end{align*}
and thus for any $\lambda_0>0$,
\begin{align*}
    \frac{T }{2}\log\lambda_0-\frac{\lambda_0}{2}\sum_{t=1}^{T} \overline{\lambda}_{-0t}\left(\tilde{r}_{-0t}-\hat{\mu}_0\right)^2 > \frac{T }{2}\log\lambda_0-\frac{\lambda_0}{2}\sum_{t=1}^{T} \overline{\lambda}_{-0t}\left(\tilde{r}_{-0t}-\mu_0\right)^2.
\end{align*}
Since the $\log$ is strictly concave, the left-hand side of the above inequality is uniquely maximized by $\hat{\lambda}_0$. So for any $\lambda_0 > 0$, we have 
\begin{align*}
    \frac{T }{2}\log\hat{\lambda}_0-\frac{\hat{\lambda}_0}{2}\sum_{t=1}^{T} \overline{\lambda}_{-0t}\left(\tilde{r}_{-0t}-\hat{\mu}_0\right)^2 &> \frac{T }{2}\log\lambda_0-\frac{\lambda_0}{2}\sum_{t=1}^{T} \overline{\lambda}_{-0t}\left(\tilde{r}_{-0t}-\hat{\mu}_0\right)^2 \\
    &> \frac{T }{2}\log\lambda_0-\frac{\lambda_0}{2}\sum_{t=1}^{T} \overline{\lambda}_{-0t}\left(\tilde{r}_{-0t}-\mu_0\right)^2.
\end{align*}
Proving that $\hat{\mu}_0$ and $\hat{\lambda}_0$ are the global maximizers of (\ref{eq:EB-max-simple}). Therefore, Proposition \ref{prop:coord-ascent} still holds with the included maximization step for $\mu_0$ and $\lambda_0$ and Algorithm \ref{alg:mich-eb} will also converge to a stationary point. 

%\input{MICH/Figures/Algorithm 2}