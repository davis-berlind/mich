\subsection{Estimating \texorpdfstring{$\mu_0$}{mu0} and \texorpdfstring{$\lambda_0$}{lambda0} with Empirical Bayes}
\label{app:empirical-bayes}

The intercept $\mu_0$ and initial scale $\lambda_0$ are taken as known constants in Algorithm \ref{alg:1}; however, in most cases $\mu_0$ and $\lambda_0$ are unknown parameters and must be estimated. One approach to estimating $\mu_0$ and $\lambda_0$ is to just set $\mu_0 = 0$ and $\lambda_0 = 1$ in Algorithm \ref{alg:1} and fit MICH with $B_l = 0$. If we use this approach and $\mu_0 \neq 0$ or $\lambda_0 \neq 1$, then MICH will attempt to detect a change-point at $t=1$ and center $\mu_{t}$ and $\lambda_t$ around $\mu_0$ and $\lambda_0$ respectively for all $t$ up until the first real change-point.

The problem with fitting $\mu_0$ and $\lambda_0$ this way is that it requires MICH to search for a change-point whose location we already know. Searching for an additional change-point requires MICH to estimate additional parameters on the order of $\mathcal{O}(T)$, when we only actually needed to fit two nuisance parameters $\mu_0$ and $\lambda_0$.\footnote{E.g. in the case of $\mu_0 \neq 0$ and $\lambda_0 \neq 1$, we end up having to estimate $5T$ additional parameters $\{\overline{b}_t, \overline{\tau}_t, \overline{u}_t, \overline{v}_t, \overline{\pi}_t\}_{t=1}^T$.} Another problem with this approach is that we risk missing a real change-point, e.g. if the are $J^*$ true changes and we set $J=J^*$ to fit MICH, we will end up using one component of $J$ to fit $\mu_0$ and $\lambda_0$ leaving only $J^*-1$ components to fit the remaining $J^*$ true change-points. 

To circumvent these difficulties, we adopt an Empirical Bayes (EB) approach for estimating $\mu_0$ and $\lambda_0$. For fixed $q$ and $p$, we can maximize the ELBO with respect to $\mu_0$ and $\lambda_0$. \cite{Wang20} use the same approach to estimate nuisance parameters in the SuSiE model. As these authors they note, optimizing over $\mu_0$ and $\lambda_0$ in the the ELBO constitutes the M-step of an EM algorithm where the E-step is approximate (\citealp{Dempster77, Heskes03, Neal98}). Note that in the expression for the ELBO, only the marginal log-likelihood depends on $\mu_0$ and $\lambda_0$. So for a fixed approximate posterior $q$, solving:
\begin{align}
    \argmax{\{\mu_0,\lambda_0\}\in\mathbb{R}\times\mathbb{R}_{++}} \; \text{ELBO}(q\:;\mu_0,\lambda_0)
\end{align}
is equivalent to solving:
\begin{align}\label{eq:EB-max}
    \argmax{\{\mu_0,\lambda_0\}\in\mathbb{R}\times\mathbb{R}_{++}} \; \E_{q} \left[\log p\left(\mathbf{y} \:|\: \theta;\mu_0,\lambda_0\right)\right].
\end{align}
If we define:
\begin{align}
    r_{-0t} &:= y_t - \sum_{j=1}^J \mu_{jt} - \sum_{\ell=1}^L \mu_{\ell t} \\
    \lambda_{-0t} &:= \prod_{j=1}^J \lambda_{jt} \prod_{\ell=1}^L \lambda_{\ell t}
\end{align}
and correspondingly,
\begin{align}
    \tilde{r}_{-0t} &:= \tilde{r}_t + \mu_0 \\
    \overline{\lambda}_{-0t} &:= \lambda_0^{-1}\overline{\lambda}_t
\end{align}
then (\ref{eq:EB-max}) can be rewritten as:
\small
\begin{align}\label{eq:EB-max-simple}
    \argmax{\{\mu_0,\lambda_0\}\in\mathbb{R}\times\mathbb{R}_{++}} \; \frac{T+B_r +B_l}{2}\log\lambda_0 - \frac{\lambda_0}{2}\sum_{t=1-B_l}^{T+B_r} \overline{\lambda}_{-0t}\left[\left(\tilde{r}_{-0t}-\mu_0\right)^2 + \delta_t\right].
\end{align}
\normalsize
The solutions to (\ref{eq:EB-max-simple}) is given by:
\begin{align}
    \hat{\mu}_0 &= \frac{\sum_{t=1-B_l}^{T+B_r} \overline{\lambda}_{-0t}\tilde{r}_{-0t}}{\sum_{t=1-B_l}^{T+B_r} \overline{\lambda}_{-0t}} \label{eq:EB-max-solution-mu} \\
    \hat{\lambda}_0 &= \left[\frac{\sum_{t=1-B_l}^{T+B_r} \overline{\lambda}_{-0t}[\left(\tilde{r}_{-0t}-\hat{\mu}_0\right)^2 + \delta_t]}{T+B_l+B_r}\right]^{-1}. \label{eq:EB-max-solution-lambda}
\end{align}
We incorporate the estimation step for $\mu_0$ and $\lambda_0$ into Algorithm \ref{alg:2}. Note that Algorithm \ref{alg:2} embeds Algorithm \ref{alg:1} with the additional step of maximizing the ELBO with respect to $\mu_0$ and $\lambda_0$ while holding the approximate posterior $q$ fixed. Therefore, Algorithm \ref{alg:2} also defines a coordinate ascent procedure. The objective function in (\ref{eq:EB-max-simple}) is continuously differentiable in $\mu_0$ and $\lambda_0$, so if we can show that $\hat{\mu}_0$ and $\hat{\lambda}_0$ are the unique maximizers of (\ref{eq:EB-max-simple}), then the conditions of Proposition 2.7.1 of \cite{Bertsekas97} will still hold even with the added coordinate ascent step of maximizing the ELBO with respect to $\{\mu_0,\lambda_0\}$. 

To prove the uniqueness of $\hat{\mu}_0$ and $\hat{\lambda}_0$, first note that the term:
\begin{align*}
    -\frac{1}{2}\sum_{t=1-B_l}^{T+B_r} \overline{\lambda}_{-0t}\left(\tilde{r}_{-0t}-\mu_0\right)^2
\end{align*}
is strictly concave as a function of $\mu_0$ and is maximized at $\hat{\mu}_0$. Therefore, for any $\mu_0\in\mathbb{R}\setminus\{\hat{\mu}_0\}$ we have:
\begin{align*}
    -\frac{1}{2}\sum_{t=1-B_l}^{T+B_r} \overline{\lambda}_{-0t}\left(\tilde{r}_{-0t}-\hat{\mu}_0\right)^2 > -\frac{1}{2}\sum_{t=1-B_l}^{T+B_r} \overline{\lambda}_{-0t}\left(\tilde{r}_{-0t}-\mu_0\right)^2
\end{align*}
and thus for any $\lambda_0>0$,
\begin{align*}
    \frac{T+B_r +B_l}{2}\log\lambda_0-\frac{\lambda_0}{2}\sum_{t=1-B_l}^{T+B_r} \overline{\lambda}_{-0t}\left(\tilde{r}_{-0t}-\hat{\mu}_0\right)^2 > \frac{T+B_r +B_l}{2}\log\lambda_0-\frac{\lambda_0}{2}\sum_{t=1-B_l}^{T+B_r} \overline{\lambda}_{-0t}\left(\tilde{r}_{-0t}-\mu_0\right)^2.
\end{align*}
Since the $\log$ is strictly concave, the left-hand side of the above inequality is uniquely maximized by $\hat{\lambda}_0$. So for any $\lambda_0 > 0$, we have 
\begin{align*}
    \frac{T+B_r +B_l}{2}\log\hat{\lambda}_0-\frac{\hat{\lambda}_0}{2}\sum_{t=1-B_l}^{T+B_r} \overline{\lambda}_{-0t}\left(\tilde{r}_{-0t}-\hat{\mu}_0\right)^2 &> \frac{T+B_r +B_l}{2}\log\lambda_0-\frac{\lambda_0}{2}\sum_{t=1-B_l}^{T+B_r} \overline{\lambda}_{-0t}\left(\tilde{r}_{-0t}-\hat{\mu}_0\right)^2 \\
    &> \frac{T+B_r +B_l}{2}\log\lambda_0-\frac{\lambda_0}{2}\sum_{t=1-B_l}^{T+B_r} \overline{\lambda}_{-0t}\left(\tilde{r}_{-0t}-\mu_0\right)^2.
\end{align*}
Proving that $\hat{\mu}_0$ and $\hat{\lambda}_0$ are the global maximizers of (\ref{eq:EB-max-simple}). Therefore, Proposition \ref{prop:2} still holds with the included maximization step for $\mu_0$ and $\lambda_0$ and Algorithm \ref{alg:2} will also converge to a stationary point. 

\input{MICH/Figures/Algorithm 2}