\subsection{MICH for Multivariate Mean Changes}
\label{app:multi-mich}

In order to simplify the exposition of the MICH model in Section \ref{sec:mich}, we solely focused on detecting multiple mean and variance change-points for univariate data. However, when $\mathbf{y}_{t} \overset{\text{ind}.}{\sim}\mathcal{N}_d(\boldsymbol{\mu}_t,\boldsymbol{\Lambda}_t)$ for some $d>1$, it is still possible for MICH to identify multiple breaks in $\boldsymbol{\mu}_{1:T}$, assuming that: 
\begin{align}
    \boldsymbol{\mu}_{t} :=  \boldsymbol{\mu}_0+ \sum_{\ell = 1}^L \boldsymbol{\mu}_{\ell t}:= \boldsymbol{\mu}_0+ \sum_{\ell = 1}^L \mathbf{b}_\ell\mathbbm{1}\{t \geq\tau_\ell\}, \label{eq:mu_t-multi}
\end{align}
where $\mathbf{b}_0 \in \mathbb{R}^d$ is an intercept, each $\tau_\ell$ is independently distributed according to (\ref{eq:tau-cat}) for some $\boldsymbol{\pi}_\ell \in \mathcal{S}^T$, and each $\mathbf{b}_\ell$ is independently distributed according to (\ref{eq:smcp-end}) for some $\omega_\ell > 0 $. Algorithm \ref{alg:mich-multi} shows how to modify \ref{alg:mich-eb} to handle multivariate mean changes of the form (\ref{eq:mu_t-multi}).

\input{MICH/Figures/Multi MICH Algorithm}

As with Algorithms \ref{alg:mich} and \ref{alg:mich-eb}, Algorithm \ref{alg:mich-multi} returns a variational approximation to the true posterior.  

\begin{proposition} 
\label{prop:vb-multi}
Given the distributions $\{q_\ell\}_{\ell=1}^L$, define $q:=\prod_{\ell=1}^Lq_\ell$. Define the residual $\overline{\mathbf{r}}_t := \mathbf{y}_t - \sum_{\ell=1}^L \E_{q_\ell}[\boldsymbol{\mu}_{\ell t}]$ and the partial residual $\overline{\mathbf{r}}_{-\ell t} := \overline{\mathbf{r}}_t + \E_{q_\ell}[\boldsymbol{\mu}_{\ell t}]$. Then $\text{\normalfont{arg}}\max_{q_\ell} \text{\normalfont ELBO}\left(q;\boldsymbol{\mu}_0,\boldsymbol{\Lambda}_{1:T}\right)$ is equivalent to the mean-scp posterior in (\ref{eq:gamma-post-cat1}) and (\ref{eq:b-smcp}) with parameters $\overline{\boldsymbol{\theta}}_\ell = \normalfont{\texttt{mean-scp}}(\overline{\mathbf{r}}_{-\ell,1:T} \:;\: \boldsymbol{\Lambda}_{1:T}, \omega_{\ell}, \boldsymbol{\pi}_{\ell,1:T}).$ 
\end{proposition}

The proof of Proposition \ref{prop:vb-multi} follows an identical argument to the proof of Proposition \ref{prop:vb} (i). We simply replace the univariate variables $b_{\ell}$ and $\lambda_t$ with their multivariate analogues $\mathbf{b}_{\ell}$ and $\boldsymbol{\Lambda}_t$. As argued in Appendix \ref{app:empirical-bayes}, the empirical Bayes step for updating $\boldsymbol{\mu}_0$ in Algorithm \ref{alg:mich-multi} also constitutes a coordinate ascent step for maximizing the ELBO with respect to $\boldsymbol{\mu}_0$ seeing as:
\begin{align*}
     \E_{q} \left[\log p\left(\mathbf{y}_{1:T} \:|\: \boldsymbol{\Theta};\boldsymbol{\mu}_0,\boldsymbol{\Lambda}_{1:T}\right)\right] &\underset{\boldsymbol{\mu}_0}{\propto} -\frac{1}{2} \left\lVert\left( \sum_{t=1}^T \boldsymbol{\Lambda}_t\right)^{\frac{1}{2}}\boldsymbol{\mu}_0\right\rVert_2^2 +\left\langle\boldsymbol{\mu}_0, \sum_{t=1}^T\boldsymbol{\Lambda}_t\overline{\mathbf{r}}_t \right\rangle,
\end{align*}
which is convex as a function of $\boldsymbol{\mu}_0$ and uniquely maximized on $\mathbb{R}^d$ at:
\begin{align*}
    \hat{\boldsymbol{\mu}}_0 := \left(\sum_{t=1}^T\boldsymbol{\Lambda}_t\right)^{-1}\sum_{t=1}^T\boldsymbol{\Lambda}_t\overline{\mathbf{r}}_t
\end{align*}
So once again, Algorithm \ref{alg:mich-multi} constitutes a coordinate ascent procedure.
\begin{corollary}
\label{cor:coord-ascent-multi}
Algorithm \ref{alg:mich} is a coordinate ascent procedure for maximizing $\text{\normalfont ELBO}\left(q;\boldsymbol{\mu}_0,\boldsymbol{\Lambda}_{1:T}\right)$ with respect to $q$ on $\mathcal{Q}_{\text{MF}}$ and $\boldsymbol{\mu}_0$ on $\mathbb{R}^d$.
\end{corollary}
\vspace{-10pt}

Due to Corollary \ref{cor:coord-ascent-multi}, the sequence of parameters $\{\overline{\boldsymbol{\Theta}}_{n}\}_{n \geq 1}$ returned by Algorithm \ref{alg:mich-multi} converge to some limit point. Furthermore, because these parameters fully characterize the distributions that comprise $q_{\text{mich}}$, the sequence of distributions returned by Algorithm \ref{alg:mich-multi} must converge to a stationary point of $\text{\normalfont ELBO}(q\:;\boldsymbol{\mu}_0, \boldsymbol{\Lambda}_{1:T})$. 

\begin{proposition}
\label{prop:stationary-point-multi}
Assume that the prior distributions for each block of $\boldsymbol{\Theta}$ are non-degenerate. Then the sequence of distributions $\{q^{(n)}\}_{n\geq 1}$ generated by Algorithm \ref{alg:mich-multi} converge to a stationary point of $\text{\normalfont ELBO}(q\:;\boldsymbol{\mu}_0, \boldsymbol{\Lambda}_{1:T})$.
\end{proposition}
\vspace{-10pt}

It is possible that the sequence of precision matrices is unknown. In this case, we can replace $\boldsymbol{\Lambda}_{1:T}$ with a consistent estimator $\hat{\boldsymbol{\Lambda}}_{1:T}$ in Algorithm \ref{alg:mich-multi}. For instance, if $\boldsymbol{\Lambda}_t = \boldsymbol{\Lambda} \sforall t \in [T]$, then $\Var(y_t - y_{t-1}) = 2\boldsymbol{\Lambda}^{-1}$ for each $t > 1$. We can define $\Tilde{y}_t := y_t - y_{t-1}$, then:
\begin{align*}
    \hat{\boldsymbol{\Lambda}}_T := \left[\frac{\sum_{t=1}^{T-1} \left(\Tilde{y}_t - \frac{\sum_{t=1}^{T-1}\Tilde{y}_t}{T-1}\right)\left(\Tilde{y}_t - \frac{\sum_{t=1}^{T-1}\Tilde{y}_t}{T-1}\right)^T}{2(T-2)}\right]^{-1}
\end{align*}
will be a consistent estimator of $\boldsymbol{\Lambda}$ that we can plug directly into Algorithm \ref{alg:mich-multi}. 