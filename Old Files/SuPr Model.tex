\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{fullpage}
\setlength{\parindent}{0cm}
\setlength{\parskip}{1em}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{enumitem}
\usepackage{bbm}
\usepackage{hyperref}
\usepackage{qtree}

\newcommand{\sforall}{\;\forall\;}
\newcommand{\E}{\mathbb{E}}
\newcommand{\Var}{\text{Var}}
\newcommand{\Cov}{\text{Cov}}
\newcommand{\argmin}[1]{\underset{#1}{\text{arg min}}}
\newcommand{\argmax}[1]{\underset{#1}{\text{arg max}}}

\title{Bayesian Joint Location-Scale Change Point Detection}
\author{Davis Berlind, Oscar Hernan Madrid Padilla, Lorenzo Cappello}
\date{\today}

\begin{document}

\maketitle

\section{Sum and Product of Single Effects}

\subsection{Model}

Suppose we have $T+1$ observations from a univariate time series $\mathbf{y} = \{y_t\}_{t=0}^T$ with a piece-wise constant structure. Let $\mathbf{X} \in \{0,1\}^{T \times T}$ be a lower-triangular matrix where the $t^\text{th}$ row of $\mathbf{X}$ is equal to $\mathbf{x}'_t = \{\mathbbm{1}\{t \geq s\}\}_{s=1}^T$. Then $\mathbf{y}$ is generated according to the following model
\begin{align*}
    \mathbf{y} &= \sum_{\ell=1}^L \mathbf{X} \beta_\ell + \Lambda^{-\frac{1}{2}} \epsilon \\
    \epsilon &\sim \mathcal{N}(\mathbf{0}, \sigma^2 \mathbf{I}_T).
\end{align*}
where the baseline noise $\sigma^2$ is assumed to be known. The mean component of the model has the same specification as Wang et al. (2020),
\begin{align*}
    \beta_\ell &= \gamma_\ell b_\ell \\
    \gamma_\ell &\overset{\text{ind.}}{\sim} \text{Categorical}(\pi) \\
    b_\ell &\overset{\text{ind.}}{\sim} \mathcal{N}(0,\sigma_0^2) 
\end{align*}
and the variance component of the model has the same specification as Capello and Hernan Madrid Padilla (2022), 
\begin{align*}    
    \Lambda &= \prod_{k=1}^K \Lambda_k\\
    \Lambda_{k} &= \text{diag}(\{\lambda^2_{k,t}\}_{t=1}^T) \\
    \lambda_{k,t} \;|\; t_k, s_k &= 
    \begin{cases}
        1, & \text{if } 1 \leq t < t_k, \\
        s_k,  & \text{if } t_k \leq t \leq T. \\
    \end{cases} \\
    t_k &= \argmax{t} \;\alpha_{k,t} \\
    \alpha_k &\overset{\text{ind.}}{\sim} \text{Categorical}(\omega) \\
    s_k^2 &\overset{\text{ind.}}{\sim} \text{Gamma}(u_0, v_0).
\end{align*} 
Note that since a change point in the scale component of the model could in principle occur at $t = 1$, it is without loss of generality to assume that $\sigma^2 = 1$ going forward.

\subsection{VB Solution}

Let $q\left(\{b_\ell,\gamma_\ell\}_{\ell=1}^L,\{s^2_k,\alpha_k\}_{k=1}^K\right)$ be a Variational Bayes (VB) approximation to the true posterior distribution $$p\left(\{b_\ell,\gamma_\ell\}_{\ell=1}^L,\{s^2_k,\alpha_k\}_{k=1}^K\;|\; \mathbf{y}, \mathbf{X}\right).$$ In order to solve for $q$, we make the following Mean-Field (MF) assumption, 
\begin{equation}\label{eq:mean-field}
    q\left(\{b_\ell,\gamma_\ell\}_{\ell=1}^L,\{s^2_k,\alpha_k\}_{k=1}^K\right) = \left(\prod_{\ell=1}^Lq(b_\ell,\gamma_\ell)\right)\left(\prod_{k=1}^K q(s^2_k,\alpha_k)\right).
\end{equation}

\begin{itemize}

\item Solving for $q(b_i,\gamma_i)$.

Treating the distributions $\{q(b_\ell,\gamma_\ell)\}_{\ell \neq i}$ and $\{q(s^2_k,\alpha_k)\}_{k=1}^K$ as known, if we let $\E_{-\beta_i}[\:\cdot\:]$ denote the expectation taken with respect to $q(\{b_\ell,\gamma_\ell\}_{\ell \neq i}, \{s^2_k,\alpha_k\}_{k=1}^K)$, then we have 
\begin{align*}
    \log q(b_i,\gamma_i) &\underset{\beta_i}{\propto} \E_{-\beta_i} \left[\log p\left(\mathbf{y} \;|\; \mathbf{X}, \sigma^2, \{b_\ell,\gamma_\ell\}_{\ell=1}^L, \{s^2_k,\alpha_k\}_{k=1}^K\right)\right] + \log p(b_i,\gamma_i) \\
    &\underset{\beta_i}{\propto} \E_{-\beta_i} \left[-\frac{1}{2\sigma^2}\sum_{t=1}^T\prod_{k=1}^K\lambda_{k,t}^2\left(y_t - \sum_{\ell=1}^L \mathbf{x}'_t\beta_\ell\right)^2\right] + \log p(b_i,\gamma_i) \\ 
    &\underset{\beta_i}{\propto} -\frac{1}{2\sigma^2}\sum_{t=1}^T \left(\prod_{k=1}^K\E_{\Lambda_k}[\lambda_{k,t}^2]\right)\E_{-\beta_i} \left[\left(y_t - \sum_{\ell=1}^L \mathbf{x}'_t\beta_\ell\right)^2\right] + \log p(b_i,\gamma_i). \tag{by (\ref{eq:mean-field})}
\end{align*}
Let $\overline{\lambda}^2_{k,t} = \E_{\Lambda_k}[\lambda_{k,t}^{2}]$, then treating this as a known quantity, we can define an expected precision matrix
\begin{align*}
    \Bar{\Lambda} = \text{diag}\left(\left\{\prod_{k=1}^K\overline{\lambda}^2_{k,t}\right\}_{t=1}^T\right).
\end{align*}
We now have the following simplification,
\begin{align*}
     \log q(b_i,\gamma_i) &\underset{\beta_i}{\propto} \E_{-\beta_i} \left[-\frac{1}{2\sigma^2}\left\lVert  \Bar{\Lambda}^{\frac{1}{2}} \left(\mathbf{y} - \sum_{\ell=1}^L \mathbf{X} \beta_\ell\right) \right\rVert^2\right] + \log p(b_i,\gamma_i).
\end{align*}
As in Wang et al. (2020), if we let $ \overline{\beta}_\ell = \E_{\beta_\ell}\left[\beta_\ell\right]$, then we define the expected partial location residual
\begin{align*}
    \overline{\mathbf{r}}_i &= \E_{-\beta_i}[\mathbf{r}_i] \\
    &= \E_{-\beta_i}\left[\mathbf{y} - \sum_{\ell \neq i} \mathbf{X} \beta_\ell\right] \\
    &= \mathbf{y} - \sum_{\ell \neq i} \mathbf{X} \E_{\beta_\ell}\left[\beta_\ell\right] \\
    &= \mathbf{y} - \sum_{\ell \neq i} \mathbf{X} \overline{\beta}_\ell.
\end{align*}
We can also introduce weighted versions of the residuals and design matrix
\begin{align*}
    \widetilde{\mathbf{r}}_i &= \Bar{\Lambda}^{\frac{1}{2}}\overline{\mathbf{r}}_i \\
    \widetilde{\mathbf{X}} &= \Bar{\Lambda}^{\frac{1}{2}} \mathbf{X}.
\end{align*}
Then we have 
\begin{align*}
    \left\lVert \widetilde{\mathbf{r}}_i - \widetilde{\mathbf{X}}\beta_i \right\rVert^2 &\underset{\beta_i}{\propto} -2\widetilde{\mathbf{r}}'_i\widetilde{\mathbf{X}}\beta_i + \beta'_i\widetilde{\mathbf{X}}'\widetilde{\mathbf{X}}\beta_i \\ 
    &= -2 \overline{\mathbf{r}}_i \Bar{\Lambda} \mathbf{X}\beta_i + \beta'_i \mathbf{X}'\Bar{\Lambda} \mathbf{X}\beta_i \\
    &= -2 \mathbf{y}' \Bar{\Lambda} \mathbf{X}\beta_i + 2 \sum_{\ell \neq i} \overline{\beta}_\ell \mathbf{X}'\Bar{\Lambda}\mathbf{X}\beta_i + \beta'_i \mathbf{X}'\Bar{\Lambda} \mathbf{X}\beta_i \\
    &= \E_{-\beta_i}\left[-2 \mathbf{y}' \Bar{\Lambda} \mathbf{X}\beta_i + 2 \sum_{\ell \neq i} \beta_\ell \mathbf{X}'\Bar{\Lambda}\mathbf{X}\beta_i + \beta'_i \mathbf{X}'\Bar{\Lambda} \mathbf{X}\beta_i\right] \\
    &\underset{\beta_i}{\propto} \E_{-\beta_i}\left[(\Bar{\Lambda}^{\frac{1}{2}}\mathbf{y})' (\Bar{\Lambda}^{\frac{1}{2}}\mathbf{y})  -2 (\Bar{\Lambda}^{\frac{1}{2}}\mathbf{y})'  \sum_{\ell = 1}^L \Bar{\Lambda}^{\frac{1}{2}}\mathbf{X}\beta_\ell  + \sum_{\ell =1}^L\sum_{\ell'=1}^L \beta_\ell (\mathbf{X}\Bar{\Lambda}^{\frac{1}{2}})'(\Bar{\Lambda}^{\frac{1}{2}}\mathbf{X})\beta_{\ell'} \right] \\
    &=  \E_{-\beta_i} \left[\left\lVert \Bar{\Lambda}^{\frac{1}{2}} \left(\mathbf{y} - \sum_{\ell=1}^L \mathbf{X} \beta_\ell\right) \right\rVert^2\right].
\end{align*}
Therefore 
\begin{align*}
    \log q(b_i,\gamma_i) &\underset{\beta_i}{\propto} -\frac{1}{2\sigma^2}\left\lVert \widetilde{\mathbf{r}}_i - \widetilde{\mathbf{X}}\beta_i \right\rVert^2 + \log p(b_i,\gamma_i) \\
    &\underset{\beta_i}{\propto} \log p\left(\widetilde{\mathbf{r}}_i \;|\; \widetilde{\mathbf{X}}, \sigma^2, b_i,\gamma_i\right) + \log p(b_i,\gamma_i),
\end{align*}
which shows that the VB solution for the multiple effect model reduces to solving a single effect model with the weighted residuals and design. Suppose that $\gamma_i = \mathbf{e}_j$, where $\mathbf{e}_j$ is the $j^{\text{th}}$ standard basis vector of $\mathbb{R}^T$. 

If we define
\begin{align*}
    \overline{\mu}_{ij} &= \frac{\overline{\sigma}_{ij}^2}{\sigma^2}\left(\widetilde{\mathbf{r}}'_i\widetilde{\mathbf{X}}\mathbf{e}_j\right) \\
    \overline{\sigma}_{ij}^2 &= \left(\frac{\mathbf{e}'_j\widetilde{\mathbf{X}}'\widetilde{\mathbf{X}}\mathbf{e}_j}{\sigma^2} + \frac{1}{\sigma_0^{2}}\right)^{-1}
\end{align*}
then, we have 
\begin{align*}
    \log q(b_i \;|\; \gamma_i = \mathbf{e}_j) &\underset{b_i}{\propto} -\frac{1}{2\overline{\sigma}_{ij}^2}\left(b_i - \overline{\mu}_{ij}\right)^2 
\end{align*}
which we recognize as the kernel of a Gaussian distribution with mean $\overline{\mu}_{ij}$ and variance $\overline{\sigma}_{ij}^2$. We can simplify further by noting that
\begin{align*}
    \widetilde{\mathbf{X}}\mathbf{e}_j &=  
    \begin{bmatrix}
        \mathbbm{1}\{1 \geq j\}\left(\prod_{k=1}^K\overline{\lambda}^2_{k,1}\right)^{1/2} \\ 
        \vdots \\
        \mathbbm{1}\{t \geq j\}\left(\prod_{k=1}^K\overline{\lambda}^2_{k,t}\right)^{1/2} \\ 
        \vdots \\
        \mathbbm{1}\{T \geq j\}\left(\prod_{k=1}^K\overline{\lambda}^2_{k,T}\right)^{1/2} \\ 
    \end{bmatrix}
\end{align*}
so
\begin{align*}
    \overline{\mu}_{ij} &= \frac{\overline{\sigma}_{ij}^2}{\sigma^2}\left(\sum_{j \leq t \leq T}\overline{r}_{i,t}\prod_{k=1}^K\overline{\lambda}^2_{k,t}\right) \\
    \overline{\sigma}_{ij}^2 &= \left(\frac{1}{\sigma^2}\sum_{j \leq t \leq T} \prod_{k=1}^K\overline{\lambda}^2_{k,t}  + \frac{1}{\sigma_0^{2}}\right)^{-1}.
\end{align*}

To solve for $q(\gamma_i)$, we begin by writing 
\begin{align*}
    q(\gamma_i = \mathbf{e}_j) &= \int_{-\infty}^\infty q(b_i, \gamma_i = \mathbf{e}_j)\;db_i \\
    &= \int_{-\infty}^\infty \exp[\log q(b_i, \gamma_i = \mathbf{e}_j)]\;db_i. 
\end{align*}
Letting $\phi(x; \mu,\sigma^2)$ be the density of a Gaussian random variable with mean $\mu$ and variance $\sigma^2$, we have
\begin{align*}
    \log q(b_i, \gamma_i = \mathbf{e}_j) &= \log \phi(b_i ; \overline{\mu}_{ij}, \overline{\sigma}_{ij}^2) + \log \bar{\sigma}_{ij} + \frac{\overline{\mu}_{ij}^2}{2\overline{\sigma}_{ij}^2} + \log \pi_j + \log C
\end{align*}
where $C$ is a constant that does not depend on $b_i$ or $\gamma_i$. We can now write
\begin{align*}
     q(\gamma_i = \mathbf{e}_j) &= \int_{-\infty}^\infty \exp[\log q(b_i, \gamma_i = \mathbf{e}_j)]\;db_i \\
    &= C \pi_j \bar{\sigma}_{ij} \exp\left[\frac{\overline{\mu}_{ij}^2}{2\overline{\sigma}_{ij}^2}\right] \int_{-\infty}^\infty \phi(b_i ; \overline{\mu}_{ij}, \overline{\sigma}_{ij}^2) \;db_i \\
    &= C \pi_j \bar{\sigma}_{ij} \exp\left[\frac{\overline{\mu}_{ij}^2}{2\overline{\sigma}_{ij}^2}\right]
\end{align*}
and since $$\sum_{t=1}^T q(\gamma_\ell = \mathbf{e}_t) = 1,$$ we have $$C^{-1} = \sum_{t=1}^T \pi_t \bar{\sigma}_{it} \exp\left[\frac{\bar{\mu}_{it}^2}{2\bar{\sigma}^2_{it}}\right].$$ So the solution for $q(\gamma_i)$ is given by $$ q(\gamma_i = \mathbf{e}_j) = \frac{\pi_j \bar{\sigma}_{ij} \exp\left[\frac{\overline{\mu}_{ij}^2}{2\overline{\sigma}_{ij}^2}\right]}{\sum_{t=1}^T \pi_t \bar{\sigma}_{it} \exp\left[\frac{\bar{\mu}_{it}^2}{2\bar{\sigma}^2_{it}}\right]}.$$

\item Solving for $q(s_i^2,\alpha_i)$.

Treating the distributions $\{q(b_\ell,\gamma_\ell)\}_{\ell=1}^L$ and $\{q(s^2_k,\alpha_k)\}_{k \neq i}$ as known, we have 
\begin{align*}
    \log q(s_i^2,\alpha_i) &\underset{\Lambda_i}{\propto} \E_{-\Lambda_i} \left[\log p\left(\mathbf{y} \;|\; \mathbf{X}, \{b_\ell,\gamma_\ell\}_{\ell=1}^L, \{s^2_k,\alpha_k\}_{k=1}^K\right)\right] + \log p(s_i^2,\alpha_i) \\
    &\underset{\Lambda_i}{\propto} \frac{1}{2}\sum_{t=1}^T \log \lambda^2_{i,t} - \frac{1}{2\sigma^2}\sum_{t=1}^T\E_{-\Lambda_i}\left[\prod_{k=1}^K \lambda^2_{k,t}\left(y_t - \sum_{\ell=1}^L \mathbf{x}'_t\beta_\ell\right)^2\right] + \log p(s_i^2,\alpha_i) \\
    &= \frac{1}{2}\sum_{t=1}^T \log \lambda^2_{i,t} - \frac{1}{2\sigma^2} \sum_{t=1}^T\lambda_{i,t}^2\prod_{k\neq i} \E_{\Lambda_k}[\lambda^2_{k,t}]\E_\beta\left[\left(y_t - \sum_{\ell=1}^L \mathbf{x}'_t\beta_\ell\right)^2\right]  + \log p(s_i^2,\alpha_i) \tag{by (\ref{eq:mean-field})} \\
    &= \frac{1}{2}\sum_{t=1}^T \log \lambda^2_{i,t} - \frac{1}{2\sigma^2} \sum_{t=1}^T\lambda_{i,t}^2\prod_{k\neq i} \overline{\lambda}_{k,t}^2\E_\beta\left[\left(y_t - \sum_{\ell=1}^L \mathbf{x}'_t\beta_\ell\right)^2\right]  + \log p(s_i^2,\alpha_i) 
\end{align*}

With a slight abuse of notation, let
\begin{align*}
    \bar{\Lambda}_k &= \text{diag}\left(\left\{\overline{\lambda}^2_{k,t}\right\}_{t=1}^T\right) \\
    \Bar{\Lambda}_{-i} &= \prod_{k \neq i} \bar{\Lambda}_k.
\end{align*}
Capello and Hernan Madrid Padilla (2022) define a the notion of an expected partial scale residual 
\begin{align*}
    \overline{\mathbf{s}}_i = \Bar{\Lambda}^{\frac{1}{2}}_{-i} \mathbf{y}
\end{align*}
which is analogous to the previously defined location residual $\overline{\mathbf{r}}_i$, except now we are scaling $\mathbf{y}$ by the known expected precisions rather than subtracting off the known expected means. Capello and Hernan Madrid Padilla (2022) show that finding the VB solution for $q(s_i^2,\alpha_i)$ in the multiple scale effects model is equivalent to solving for $q(s_i^2,\alpha_i)$ in a single scale effect model where the response $\mathbf{y}$ is replaced with the residual $\overline{\mathbf{s}}_i$. In our joint location-scale model, we no longer have $\E[y_t] = 0$, but if we define 
\begin{align*}
    \widetilde{y}_t &= \left(\E_\beta\left[\left(y_t - \sum_{\ell=1}^L \mathbf{x}'_t\beta_\ell\right)^2\right]\right)^{\frac{1}{2}} \\
    \widetilde{\mathbf{z}}_i &= \Bar{\Lambda}^{\frac{1}{2}}_{-i} \widetilde{\mathbf{y}}
\end{align*}
then 
\begin{align*}
    \log q(s_i^2,\alpha_i) &\underset{\Lambda_i}{\propto} \frac{1}{2} \log |\Lambda_i|
    - \frac{1}{2\sigma^2}\left\lVert\Lambda_i^{\frac{1}{2}}\widetilde{\mathbf{z}}_i\right\rVert^2  + \log p(s_i^2,\alpha_j)\\
    &\underset{\Lambda_i}{\propto} \log p\left(\widetilde{\mathbf{z}}_i \;|\; \sigma^2, s^2_i,\alpha_i\right) + \log p(s_i^2,\alpha_i).
\end{align*}
So again we see that the VB solution for $q(s_i^2,\alpha_i)$ reduces to solving the solution for single variance change point with a scaled and de-noised residual. 

Suppose now that $\alpha_i = \mathbf{e}_j$, then the first $j-1$ diagonal elements of $\Lambda_i$ are constant with respect to $s_i^2$, so we have
\begin{align*}
    \log q(s_i^2\;|\;\alpha_i = \mathbf{e}_j) &\underset{s_i^2}{\propto} \frac{T - j + 1}{2}  \log s_i^2
    - \frac{s_i^2}{2\sigma^2} \sum_{t=j}^T \widetilde{z}_{i,t}^2 + (u_0 - 1) \log s_i^2 - v_0 s_i^2 \\
    &= (\bar{u}_{ij} - 1) \log s_i^2 - \var{v}_{ij} s_i^2
\end{align*}
where 
\begin{align*}
    \bar{u}_{ij} &= u_0 + \frac{T - j + 1}{2} \\
    \bar{v}_{ij} &= v_0 + \frac{1}{2\sigma^2} \sum_{t=j}^T \widetilde{z}_{i,t}^2.
\end{align*}
We recognize this as the kernel of a Gamma distribution with shape and rate parameters $\bar{u}_{ij}$ and $\bar{v}_{ij}$ respectively. We can simplify further by noting that
\begin{align*}
    \widetilde{z}_{i,t}^2 &= \prod_{k\neq i} \overline{\lambda}^2_{k,t}\E_\beta\left[\left(y_t - \sum_{\ell=1}^L \mathbf{x}'_t\beta_\ell\right)^2\right] \\
    &= \prod_{k\neq i} \overline{\lambda}^2_{k,t}\left(y_t^2 - 2 y_t \sum_{\ell=1}^L \mathbf{x}'_t \overline{\beta}_\ell + \sum_{\ell = 1}^L\sum_{\ell'=1}^L \overline{\beta}'_{\ell'} \mathbf{x}_t\mathbf{x}'_t \overline{\beta}_\ell - \sum_{\ell=1}^L \overline{\beta}'_{\ell} \mathbf{x}_t\mathbf{x}'_t \overline{\beta}_\ell + \sum_{\ell=1}^L\E_{\beta_\ell}\left[\beta'_{\ell} \mathbf{x}_t\mathbf{x}'_t\beta_\ell\right]\right) \\
    &= \prod_{k\neq i} \overline{\lambda}^2_{k,t}\left[\left(y_t - \sum_{\ell=1}^L \mathbf{x}'_t\overline{\beta}_\ell\right)^2 - \sum_{\ell=1}^L \overline{\beta}'_{\ell} \mathbf{x}_t\mathbf{x}'_t \overline{\beta}_\ell + \sum_{\ell=1}^L\sum_{t'=1}^t (\overline{\mu}_{\ell t'}^2 + \overline{\sigma}_{\ell t'}^2) q(\gamma_\ell = \mathbf{e}_{t'})\right]
\end{align*}
where the last line follows from 
\begin{align*}
    \E_{\beta_\ell}\left[\beta'_{\ell} \mathbf{x}_t\mathbf{x}'_t\beta_\ell\right] &= \E_{\gamma_\ell}\left[\E_{b_\ell|\gamma_\ell}\left[ \beta'_\ell \mathbf{x}_t\mathbf{x}'_t\beta_\ell\right]\right] \\
    &= \sum_{i=1}^T \E_{b_\ell|\gamma_\ell = \mathbf{e}_i}[b^2]q(\gamma_\ell = \mathbf{e}_i)\mathbf{e}'_i\mathbf{x}_t\mathbf{x}'_t\mathbf{e}_i \\
    &= \sum_{i=1}^T \E_{b_\ell|\gamma_\ell = \mathbf{e}_i}[b^2]q(\gamma_\ell = \mathbf{e}_i)\mathbbm{1}\{i \leq t\}.
\end{align*}

To solve for $q(\alpha_i)$, we begin by writing
\begin{align*}
    q(\alpha_i = \mathbf{e}_j) &= \int_{-\infty}^\infty q(s_i^2, \alpha_i = \mathbf{e}_j)\;ds_i^2 \\
    &= \int_{-\infty}^\infty \exp[\log q(s_i^2, \alpha_i = \mathbf{e}_j)]\;ds_i^2. 
\end{align*}
Let $f(x; u, v)$ be the density of a Gamma random variable with shape $u$ and rate $v$, and define $\widetilde{z}^2_{i,0} = 0$. Then we have
\begin{align*}
    \log q(s_i^2, \alpha_i = \mathbf{e}_j) &= \log  f(s_i^2 ; \bar{u}_{ij}, \bar{v}_{ij}) + \log  \Gamma(\bar{u}_{ij}) - \bar{u}_{ij} \log \bar{v}_{ij} - \frac{1}{2\sigma^2}\sum_{t=0}^{j-1}\widetilde{z}_{i,t}^2 + \log \omega_j + \log C
\end{align*}
where $C$ is a constant that does not depend on $s_i^2$ or $\alpha_i$. We can now write
\begin{align*}
     q(\alpha_i= \mathbf{e}_j) &= \int_{-\infty}^\infty \exp[\log q(s_i^2, \alpha_i = \mathbf{e}_j)]\;ds_i^2 \\ 
     &=  C \omega_j\Gamma(\bar{u}_{ij})\bar{v}_{ij}^{-\bar{u}_{ij}} \exp\left[ - \frac{1}{2\sigma^2}\sum_{t=1}^{j-1}\widetilde{z}_{i,t}^2\right]\int_{-\infty}^\infty f(s_i^2 ; \bar{u}_{ij}, \bar{v}_{ij})\;ds_i^2 \\
     &= C \omega_j\Gamma(\bar{u}_{ij})\bar{v}_{ij}^{-\bar{u}_{ij}}\exp\left[ - \frac{1}{2\sigma^2}\sum_{t=1}^{j-1}\widetilde{z}_{i,t}^2\right]
\end{align*}
and thus 
\begin{align*}
    q(\alpha_i= \mathbf{e}_j) &= \frac{\omega_j\Gamma(\bar{u}_{ij})\bar{v}_{ij}^{-\bar{u}_{ij}}\exp\left[ - \frac{1}{2\sigma^2}\sum_{t=0}^{j-1}\widetilde{z}_{i,t}^2\right]}{\sum_{t=1}^T\omega_j\Gamma(\bar{u}_{it})\bar{v}_{it}^{-\bar{u}_{it}}\exp\left[ - \frac{1}{2\sigma^2}\sum_{t'=0}^{t-1}\widetilde{z}_{i,t'}^2\right]}
\end{align*}

\end{itemize}

\end{document}