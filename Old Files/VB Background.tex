We begin by considering the generic model:
\begin{align}
    \mathbf{y} \:|\: \theta &\sim f(\mathbf{y}\:|\: \theta;\eta) \\
    \theta &\sim g(\theta)
\end{align}
where the nuisance parameter $\eta$ in the likelihood of $\mathbf{y}$ is taken as known. The goal of VB inference is to find a distribution $q^*(\theta)$ that is the closest approximation to the true posterior $p(\theta\:|\: \mathbf{y};\eta)$ as measured by the Kullbackâ€“Leibler divergence (\citealp{Kullback51}):
\begin{align}\label{eq:unrestriced-kl}
    q^* := \argmin{q}  \; \text{KL}( q(\theta) \:\lVert\: p(\theta\:|\: \mathbf{y};\eta)).
\end{align}
If we knew the exact form of $p(\theta\:|\: \mathbf{y};\eta)$, then we could solve (\ref{eq:unrestriced-kl}) by simply setting $q^*(\theta)\equiv p(\theta\:|\: \mathbf{y};\eta)$, in which case the KL-divergence would achieve its zero lower-bound. However, in cases such as ours where there is no closed form available for the posterior, we must resort to computational methods to find $q^*$. Because the minimization in (\ref{eq:unrestriced-kl}) is taken with respect to all possible distributions over $\theta$, finding a solution computationally is generally an impossible task. Therefore, we restrict $q^*$ to a known family of distributions $\mathcal{Q}$ that we hope will simplify the problem:

In many applications, we can naturally partition $\theta$ into a finite number of blocks $\theta = \{\theta_1, \ldots, \theta_M\}$.\footnote{Examples include product partition models (\citealp{Hartigan90, Barry92}), stochastic block models (\citealp{Holland83}), and mixture models (\citealp{Corduneanu01}).} In such cases, one intuitive way to structure $\mathcal{Q}$ and $g$ is:
\begin{align} 
    \mathcal{Q} &= \left\{q \::\: q(\theta) = \prod_{i=1}^M q_i(\theta_i)\right\}, \label{eq:mfvb} \\
    g(\theta) &= \prod_{i=1}^M g_i(\theta_i) \label{eq:ind-prior}
\end{align}
i.e. we restrict the approximate posterior $q$ and the prior $g$ to families of distributions that render the $M$ blocks of $\theta$ independent. Restrictions of the form (\ref{eq:mfvb}) are called mean-field assumptions and are very common within the VB literature due to the fact that they often lead to analytic solutions for (\ref{eq:restriced-kl}) (\citealp{Wainwright08}). In particular, for MICH we have:
\begin{align}
    \theta &:= \left\{\{\theta_j\}_{j=1}^J, \{\theta_\ell\}_{\ell=1}^L,\{\theta_k\}_{k=1}^K\right\} \\
    \eta &:= \{\mu_0,\lambda_0\}
\end{align}
and we make the mean-field assumption:

Note that the distribution we defined in (\ref{eq:q-j})-(\ref{eq:indep-blocks}) belongs to $\mathcal{Q}$, meaning that we already know the distribution returned by Algorithm \ref{alg:1} is at least a candidate solution to (\ref{eq:restriced-kl}). In addition to restricting the solution to $\mathcal{Q}$, the minimization problem in (\ref{eq:restriced-kl}) can often be simplified even further by using the following equivalent characterization of the KL-divergence: 
\begin{align}
     \text{KL}( q(\theta) \:\lVert\: p(\theta\:|\: \mathbf{y};\eta)) &= \log \int f(\mathbf{y}\:|\:\theta;\eta)g(\theta)\;d\theta - \int q(\theta) \log \frac{ f(\mathbf{y}\:|\: \theta;\eta) g(\theta)}{q(\theta)} \; d\theta. \label{eq:kl-decomp} 
\end{align}
Because the KL-divergence is non-negative for all choices of $q$, by (\ref{eq:kl-decomp}) we have:

The log-marginal likelihood that appears in the upper bound of (\ref{eq:elbo}) is frequently referred to as the log-evidence, and thus the term on the left-hand side of the inequality is called the evidence lower bound (ELBO). Notably, the log-evidence does not depend on $q$, so by (\ref{eq:kl-decomp}) solving (\ref{eq:restriced-kl}) is equivalent to solving: 
\begin{align}
    q^* := \argmax{q\in\mathcal{Q}}  \; \text{ELBO}(q;\eta).
\end{align}
We can now show that the distribution returned by Algorithm \ref{alg:1} is an approximation to the precise $q^*$ we seek in (\ref{eq:restriced-elbo}). In the statement of this result, we use the following non-standard notation for the distribution $q$ with the $i^{\text{th}}$ parameter block marginalized out:
\begin{align}
    q_{-i}(\theta_{-i}) &:= \int q(\theta) \; d\theta_i. \label{eq:q-minus-i}
\end{align}
When $q \in \mathcal{Q}$, we simply have $q_{-i}(\theta_{-i}) =q_{i}(\theta_{i})^{-1}q(\theta)$. We are now ready to state our main result.